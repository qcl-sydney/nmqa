\section{Physical Setting}\label{sec:physicalsetting}

\begin{enumerate}
	\item We consider a spatial arrangement of qubits coupled to a common classical, static dephasing noise field that is continuously varying in space. The spatial arrangement of qubits relative to the grid is assumed kknown, and in our notation, we reserve the index, $j_t$, to label the location (Euclidean position coordinates)  where a physical measurement on a single qubit is performed at time $t$.
	\item The value of the dephasing field at any qubit location induces a relative phase between the ground and excited states of the single qubit superposition state, thereby affecting the  quantum mechanical probability of observing a  `0' or `1' in a Ramsey measurement. 
	\item We make a sequence of sensing measurements  at different qubits in the arrangement. Each measurement is a single qubit operation yielding a $0$ or a $1$ outcome. The resulting record of $T$ measurements, in the simplest case, are a set of time-stamped and space-indexed binary measurements.
	\item Over a sequence of measurements, the changing availability of the sensing qubit defines a control trajectory on the grid, given as $\{u_{t}\}_{1:t:T}$. This trajectory is not a continuous path, but resembles hopping on the grid. This trajectory assumes that only one qubit can involved in a sensing measurement at one time. 
	\item The manuscript assumes that the dephasing field is time invariant, however, temporal noise jitter can be modelled as an uncertainty in our knowledge of qubit locations relative to the dephasing field. 
	\item The quantum mechanical probability of observing a $0$ or a $1$ measurement on a single qubit is given by Born's rule. In the limit of measuring every qubit on the grid an infinite number of times (letting $T \to \infty$ for time-invariant noise fields), we know that the histogram of physical measurements on each qubit will yield a sample probability that converges to the true Born probability for any pair $(t, j_t)$.
	\item Let $Y_t^{(j_t)}$ denote the `0' or `1' outcome of a single qubit measurement at the location $j_t$ and time $t$. Using Born's rule, the measurement model is
		\begin{align}
		Y_t^{(j_t)} := \mathcal{Q}(\frac{1}{2} \cos(F_t^{(j_t)}) + V_t + \frac{1}{2}) \label{main:qslam:measurementmodel}
		\end{align} where $F_t^{(j_t)}$ is the value of the dephasing field at location $j_t$ and $v_t$ models our uncertainty in  the knowledge of the true dephasing field. In particular, we assume $V_t$ are zero mean truncated Gaussian distributed errors, truncated appropriately to ensure that the resulting argument of $\mathcal{Q}$ is  a random probability measure. Here, $\mathcal{Q}$ represents a Bernoulli trial parameterised by a probability that depends on the phase,  $F_t^{(j_t)}$.
	\item We can re-write the density of the noise model as a so-called `likelihood function' for the measurement equation \cref{main:qslam:measurementmodel} above, given by:
	\begin{align}
	g_1(t, Y_t^{(j_t)}) & :=  \frac{\rho_0}{2} + \frac{\rho_0  \cos(F_t^{(j_t)}) }{2} \left( \delta(Y_t^{(j_t)} - 1) - \delta(Y_t^{(j_t)}) \right) \label{main:qslam:likelihood:g1:func} \\
	\rho_0 &: =  \erf(\frac{2b}{\sqrt{2\Sigma_R}}) + \frac{\sqrt{2\Sigma_R}}{2b} \frac{e^{-(\frac{2b}{\sqrt{2\Sigma_R}})^2}}{\sqrt{\pi}}  - \frac{1}{2b}\frac{\sqrt{2\Sigma_R}}{\sqrt{\pi}},\\
	b &:= 1/2 \label{main:qslam:likelihood:g1:rho0}
	\end{align} The purpose of $g_1(t, Y_t^{(j_t)})$ is to score state estimates for $F_t^{(j_t)}$ as being probable or not, given measurement data. For zero mean noise, the quantities $\Sigma_R$ and $b$  represents the variance of the Gaussian noise, and the truncation point $b$ for the Gaussian distribution. $\rho_0$ is simply a factor that depends on $\Sigma_R$ and $b$, and the expression for   $g_1(t, Y_t^{(j_t)})$   reduces to what one would expect for a noiseless coin toss experiment if $\Sigma_R \to 0$. The derivation of $\rho_0$ is based on probability theory for classical quantised sensor information in \cite{riddhinotes} and references therein.  
	\item  The above equations refer to single qubit measurements and likelihood functions. One could define the state vector $F_t$ to be the set of random variables over each of the qubit locations, taking in values between $[0, \pi]$, such that $F_t^{(j_t)}$ is the $j_t$ element of $F_t$. The set of phases for all qubits in the system is the random vector $F_t$ and is referred to as the `true map' or equivalently, as the set of `map values'. 
	\item We assume that the prior  distribution for the dephasing field at $t=0$ is uniform over the outcomes  $[0, \pi]$ and that $g_1(t, Y_t^{(j_t)})$ holds for all qubits. We further state  that $F_t$ represents a Markov process with respect to time $t$. For time invariant dephasing fields, this means that the transition probabilities for $F_t$ reflect identity dynamics from one time step to the next.
	\item Under these conditions, we have an unhelpful filtering problem: our model requires us to infer individual qubit phase by measuring each qubit independently, as both the measurement model (\cref{main:qslam:likelihood:g1:func}) and the likelihood function  (\cref{main:qslam:likelihood:g1:rho0})  are  indexed by the location of the physically measured qubit $(j_t)$. In the noiseless case, the procedure stated above would simply be replaced by tallying `0's and `1's on each qubit to obtain an empirical Born probability and inverting the Born rule to obtain $F_t^{(j_t)}$ at each location $j_t$.
	\item Hence, we seek to develop a mechanism of sharing information between qubits. The development of this mechanism is the core proposal of the `qslam' framework, and this is the subject of the next section.
\end{enumerate}

\section{qslam Framework}

\begin{enumerate}
	\item The qslam framework draws heavily from classical literature on simultaneous mapping and localisation problems.	At the high level, the quantity $F_t$ represents a true map of the environment that must be inferred by an autonomous sensor (robot), here, represented by a qubit sensor. 
	\item In our setting, the `robot' does not move continuously along an autonomously chosen path in space, but hops on pre-specified spatial grid, corresponding to the autonomous choice of which qubit to measure next on our hardware. The idea of a `robot' scanning the environment is instead replaced by the assumption that physical noise sources are continuously varying, and therefore, the value of the phase, $F_t^{(j_t)}$ at point $j_t$  is close to values of the unknown phases in a small region around $j_t$.  
	\item With these modifications, the core SLAM problem retains its essential character: does a joint probabilistic model over the motion of the sensor relative to an unknown environment enable us to infer an environmental map better than measuring the environment directly through repeated measurements?
	\item To accommodate these modifications, the technical details below replace the original state variable, $F_t$, with the extended state vector $(F_t, R_t)$, where $R_t$ is a set of length-scales for all the qubit locations. These length-scales parameterise a set of overlapping Gaussian functions (neighbourhoods) centered at each qubit location $j_t$. That $R_t$ is a state vector reflects the fact that qslam must infer the extent to which state information can be shared reliably within some neighbourhood. In particular, the posterior estimate of the length-scale at $j_t$, $R_t^{(j_t)}$, sets the radius of the neighbourhood for which data relating to posterior estimate of $F_t^{(j_t)}$ is shared at the $t+1$ time step. 
	\item Specifically, let $R_t^{(j_t)}$ be the length-scale at the $j_t$, a random variable that takes the value between $[R_{min}, \infty)$.The length-scale parameterises a Gaussian function with amplitude $F_t^{(j_t)}$  that spreads the point-value of the phase $F_t^{(j_t)}$, at $j_t$, into a small neighbourhood. The argument of the Gaussian, $\nu_{(j_t,q)}$, is the separation distance between the qubit at location $j_t$ and some other point on the 2D grid, $q \in \mathbb{R}^2$, written as a ($L_2$) norm. 
	\begin{align}
	\bar{F}(\nu_{(j_t,q)}) &: = F_t^{(j_t)} \exp\left( \frac{- \nu_{(j_t,q)}^2}{(R_t^{(j_t)})^2}\right) \label{main:qslam:defn:bar_F}
	\end{align} The quantity $\bar{F}$ is the value of the phase $F_t^{(j_t)}$ smeared about $j_t$. \item If the true spatial dephasing field is rapidly changing in space, then one expects qslam to pick point value of the phase by setting $R_t^{(j_t)} \to 0$. However, the spatial resolution of the predefined qubit grid sets the smallest spatial resolution at $R_{min} > 0 $. Likewise, if the dephasing field is uniform, then one expects qslam to quickly infer that large values of $R_t^{(j_t)} \to \infty$ work well for all qubit locations. Here, the size of the pre-defined grid naturally sets  a physically sensible bound, $R_t^{(j_t)} < R_{max}$ in practice.  
	\item We also use $R_t^{(j_t)}$ to define the neighourhoods as follows. Let  $q_t$ denote the location labels for all qubits excluding the measured qubit, $ q_t \in \{1, 2, \hdots d \} \setminus \{j_t\} $. Then the neighbourhood, $Q_t$ of the measured qubit at $j_t$ with length-scale $R_t^{(j_t)}$ is the set:
	\begin{align}
	Q_t &:= \{ q_t | \nu_{(j_t, q_t)} \leq c_0 R_t^{(j_t)}, \forall q_t \in \{1, 2, \hdots d \} \setminus \{j_t\} \} \label{main:qslam:defn:Q_t} \\
	 c_0 &\in [1, 3]. 
	\end{align} That is, neighbours are the set of qubits whose separations distances fall within a radius of $c_0 R_t^{(j_t)}$ about the location $j_t$. Here, $c_0$ is an arbitrary constant that truncates the neighbourhood when smeared phase values at the boundary of the neighborhood are dissimilar to those at the center. 
	\item If there does exist a set of  true random variables $R_t$ that parameterise some approximation to $F_t$, then it is physically appropriate to expand the joint distribution of $X_t$ by  conditioning $R_t$ on a true map $F_t$; rather than the reverse situation, using the product rule for probabilities. In doing so, it makes physical sense to think of the noise density for the data given $X_t$ to be associated with a noise density of $F_t$ (given model) multiplied by a separate noise density of $R_t$ given $F_t$ and the model. 
	\item Hence, assume that $F_t$ is given  at some step of the inference procedure. Then, we associate with the state variables $R_t$ a measurement model and noise density function: 
	\begin{align}
		F_t^{(q_t)} &= \mathcal{X}_{q_t} + W_t, \quad \forall q_t \in Q_t  \label{main:qslam:defn:msmtmodel:Rt}\\
		\mathcal{X}_{q_t} &:= (1 - \lambda_0^{\tau_{q_t}})F_t^{(q_t)} + \lambda_0^{\tau_{q_t}} \bar{F}(\nu_{(j_t,q_t)}) \label{main:qslam:defn:msmtmodel:Rt:Xqt} \\
		W_t & \sim \mathcal{N}(\mu_F, \Sigma_F) \\
		g_2(t, Q_t) & := \prod_{q_t \in Q_t} \frac{1}{\sqrt{2\pi \Sigma_F}} \exp \left( -\frac{( F_t^{(q_t)} - \mathcal{X}_{q_t} - \mu_F )^2}{2 \Sigma_F }\right) \label{main:qslam:defn:likelihood:Rt}\\
		\lambda_0 &\in [0,1] \\
		\tau_{q_t} &\in \mathbb{N}, \tau_{q_0}:= 0
	\end{align} The expression in \cref{main:qslam:defn:msmtmodel:Rt} compares the estimate $\mathcal{X}_{q_t} $ to the best available phase information, $F_t^{(q_t)}$, for the qubit at $q_t$. The quantity $\mathcal{X}_{q_t}$ is based on a weighted average of both current map estimate on the qubit at $q_t$ and smeared state estimates of physically measured qubit at $j_t$, mediated by a factor $\lambda_0^{\tau_{q_t}}$.  The noise parameters $ \mu_F, \Sigma_F$ represent the true error in approximating a continuously varying spatial field with overlapping Gaussian functions. The constant $ \tau_{q_t} \leq T$  is the tally of the total number of times $q_t$ has been physically measured, whereas $\lambda_0$ is a hyper-parameter of the qslam model. Collectively, these two parameters regulate the extent to which shared information is ignored by the inference procedure.  
	\item However, we note that while \cref{main:qslam:defn:likelihood:Rt} may be used to discover the ideal neighborhood at $j_t$ using a posterior length-scale estimate $R_t^{(j)}$ at time $t$,  we have not yet influenced the inference procedure for the map, $F_t$ by the development of this mechanism. 
	\item To feed information back into the global mapping procedure, based on what we've learned about a local neighbourhood, we introduce quasi-measurements analogously to physical-measurements. In the language of this manuscript, we say that a physical measurement of a qubit at $j_t$ at time step $t$ induces a set of quasi-measurements in the posterior neighbourhood $Q_t$, denoted $\hat{Y}_{t+1}^{(q_{t+1})} \in \{0,1\}$ using a Bernoulli trial:
	 	\begin{align}
	 	\hat{Y}_{t+1}^{(q_{t+1})}  & := \mathcal{Q}(\frac{1}{2} \cos(\mathcal{X}_{q_t}) + \frac{1}{2}) \label{main:qslam:defn:quasimsmts:1} \\
	 	\forall \quad q_{t+1}& \in Q_{t+1} \equiv q_t \in Q_t (\mathrm{posterior})
	 	\end{align} That is, the random variable $\hat{Y}_{t+1}^{(q_{t+1})} \in \{0,1\}$ generates a bit of information using the posterior map estimate on the neighbouring qubit and posterior state information shared by physically measured qubit at $j_t$. In particular, the quantity $\mathcal{X}_{q_t}$ is to be interpreted as the posterior version of \cref{main:qslam:defn:msmtmodel:Rt:Xqt}, where $ Q_{t+1}$ (above) equals the $Q_t$ set by posterior state estimate $R_t^{(j)}$ from the previous time step, $t$, and the posterior $F_t$ is used for all calculations. 
	\item With these definitions, we can formally define the qslam filtering problem as:
\begin{defn} \label{qslam:filteringprob}
	Let $X = \{X_t, t = 1, 2, \hdots \}$ be a stochastic process defined on the composite probability space $(\samplespace{X}, \family{X}, \measure{{}}{\cdot})$. Define the state vector at $t$  by $X_t = (F_t, R_t) \in \mathbb{S}_{X} := \mathbb{S}_{F} \times \mathbb{S}_{R}$. Let the filtration generated by the process $X$ as $\mathcal{F}_{t} := \sigma(\{X_s, s\in[0, t]\})$ and the Borel \ofield{}s generated  are of increasing size  $\mathcal{F}_{t-1} \subset \mathcal{F}_{t} \subseteq \mathcal{F}_{X}$. Denote  $X_{0:t} := (X_0, \hdots, X_t)$ as the set of state variables until $t$. \\
	\\
	Let $Z_t = (Y_t^{(j_t)}, \{\hat{Y}_t^{(q_t)}\}_{q_t \in Q_t}) \in $ be the observational vector at time $t$ due to a single physical measurement at $j_t$ and a set of quasi-measurements $\{\hat{Y}_t^{(q_t)}\}_{q_t \in Q_t}$ in a neighbourhood $Q_t$ about $j_t$. Define the observational vector $Z := \{ Z_t, t = 1, 2, \hdots \}$ and denote  $Z_{0:t} := (Z_0, \hdots, Z_t)$ as the measurement record until $t$.\\
	\\
	Then, the filtering problem is to compute a random measure $\pi_t$ that is the conditional probability of  $X$ given the \ofield{} generated by the observation process $Z_{0:T}$:
	\begin{align}
	\pi_t &:= \measure{{}}{X_t \in A | \ofieldgen{Z_{0:T}}}, \quad \forall A \in \mathcal{S}_X \\
	\pi_t f &= \ex{f(X_t) |  \ofieldgen{Z_{0:T}}} \quad \forall f \in B(\mathbb{S}_X), A \in \mathcal{S}_X \\
	\pi_0 & \sim \mathcal{U}(\mathbb{S}_Y)
	\end{align} Here, $A$ refers to the event in the Borel \ofield generated by $X$, and $B(\mathbb{S}_X)$ refers to any bounded Borel-measurable function on the state space for $X$. The measure, $\pi_0$, is often called the prior in Bayesian analysis and it is specified by the probability distribution functions assigned to $X$ at $t=0$. \\
	For each instance of data, $Z_t=z_t$,  global `likelihood' function written as a product of noise densities for $W_t, V_t$, in \cref{main:qslam:defn:msmtmodel:Rt,main:qslam:measurementmodel}, is:
	\begin{align}
	g_t^{z_t} &: = g_1(t, Y_t^{(j_t)}) g_2(t, Q_t) \label{main:qslam:defn:globallikelihood}
	\end{align}
	Additionally, we specify $X$ is Markov with transition probabilities, $K_{t}$,  representing identity dynamics, and that the probability distribution function for $X_0$ is given by uniformly distributed outcomes for $R_0$ over $[R_{min}, \infty)^d$ and $F_0$ over  $[0, \pi)^d$, where $d$ is the total number of qubit locations in the spatial grid. 
	Collectively, the discrete time non-linear filtering problem is that the random probability measures $\pi_t^{Z_{0:t}} \equiv \pi_t$ satisfy the recursion relation:
	\begin{align}
	\pi_t &= g_t^{z_t} * K_{t-1} \pi_{t-1} 
	\end{align} The recursion relation is defined in terms of a projective product (`$*$') and it is effectively a re-statement of the familiar Bayes rule. 
\end{defn}
\item Footnote: To see the projective product as Bayes rule, let $(\mathbb{S}_x, \ofieldgen{\mathbb{S}_x}, \mathbb{P})$ be a probability space and $\mu$ be a random probability measure for random variable $x$ on the space of probability measures for $\mathbb{S}_x$. If $g$ be a non-negative function such that $\mu(g) := \int_{\mathbb{S}_x} g d\mu(x)  > 0 $, then the projective product $g * \mu(A) :=\frac{\int_{A \in \ofieldgen{\mathbb{S}_x}} g d\mu(x) }{\mu(g)}$. Here the denominator is the integral over all events for $x$, corressponding to the normalisation in Bayes Rule, while the numerator is the probability that the event $x=A$ occurred.  
\item The qslam framework, so described, is complete in the sense that the posterior recursion over the space of all maps and lengthscales is defined but formidable. Indeed, this is often a roadblock in classical SLAM problems [refs]. We now follow \cite{thrun2001probabilistic} in performing a numerical approximation to solve the qslam inference problem. This numerical approximation relies on maximising the quantity $g_t^{z_t}$ iteratively at each time step $t$, and using a filter called a `particle filter', to represent the probability distribution functions $\pi_{t}, \pi_{0}$ as a discrete collection of weighted particles.
\item 	Schematically, \cref{fig_intro_highlevel_ver_2_3} summarises the full qslam framework implemented using a iterative maximum likelihood algorithm. The figure depicts phase map and neighborhoods discovered by the particle filter based on both physical and quasi-measurements. This process is mediated by a controller which autonomously decides what qubit to physically measure next. The details of this numerical procedure are given in the next section. 
\end{enumerate}
\begin{widetext}
	\begin{figure}[]
%		\includegraphics[scale=0.8]{fig_intro_highlevel_ver_2_3}. 
		\caption{\label{fig_intro_highlevel_ver_2_3} TODO: Update notation.}    	
	\end{figure}
\end{widetext}

\section{Iterative Maximum Likelihood Algorithm}

\begin{enumerate}
	\item The iterative maximum likelihood (ML) approach is a powerful numerical approximation to full Bayesian inference problem over the space of all possible global maps and local sensor states. The iterative ML approach assumes that there is some `data association' mechanism available to us which allows us to update the environment map at $t$ using data under the strong assumption that the state variables at $t-1$ are known \cite{thrun2001probabilistic}. Irrespective of the form of the data association mechanism, the procedure chosen in \cite{thrun2001probabilistic} is to ensure that the data association mechanism maximises the likelihood, or equivalently,  $g_t^{z_t}$, at each time step.
	\item A key observation from the procedures in \cite{thrun2001probabilistic} is that the iterative maximum likelihood update happens quite naturally if a particle filtering algorithm is chosen. A particle filtering algorithm presents the Bayesian posterior $\pi_t$ as a distribution of a particles. Each particle has two properties - a position and a weight - where the position of the particle carries information about state variable and the weight specifies the importance of the particle. If the weights are normalised over the entire particle set, then the normalised weights and positions is the discrete approximation to the random measure $\pi_t$. For the particle filter considered in this manuscript, the weights are simply given by $g_t^{z_t}$. At designated intervals (in our case, each time-step $t$), the particles are `re-sampled' i.e. the original set of particles at $t$ are replaced by an off-spring set of particles, where the probability that a parent is chosen to represent itself again (with replacement) is directly proportional to its weight. Over many iterations, only the highest weighted particles survive. In probability theory, the replacement of parent particles by an off-spring distribution can be described as a branching random process, where the properties of this branching process determine essential properties of the particle filter, for example, algorithmic convergence.  A standard textbook for particle filtering and a review of pitfalls and advantages of branching mechanisms can be found here [refs]. 
	\item qslam naturally lends itself to a two-step iterative likelihood maximisation. The two-step approach arises naturally from $X_t$ being the extended state vector of $F_t, R_t$. In particular, we assume that at each time step $t$, $K_{t-1}X_{t-1}$ is known and we update $F_t$ using some data association mechanism, symbolically a function of the incoming data $Z_t$, and the a qubit location, $j$, $h_1(j, Z_t)$. In the second step, we assume that the state $F_t$ is now known, and we update $R_t$ given some other data association function, $h_2(j, F_t)$. We now seek to maximise the global likelihood for both updates - implying that two different \textit{types} of particles are required by the particle filter - one for ranking updates for $F_t$ and a second type for ranking updates to $R_t$ given $F_t$.
	\item These considerations lead to the development of a two-tiered resampling mechanism for the qslam particle filter. Below, we describe what this branching process looks like for the qslam particle filter. In Appendix A, we show that the branching mechanism for qslam satisfies a set of critical conditions from which the convergence properties of any particle filter follow. 
\item We will denote  the first set of particles as $\alpha$-particles indexed by the set of numbers $\{1, 2, \hdots, n_\alpha\}$. For each $\alpha$ we associated a layer of $\beta^{(\alpha)}$-particles labeled from $\{1, 2, \hdots, n_\beta \}$. Each  $\alpha$ particle carries the state $X_t$ and it is weighted by $ g_1(t, Y_t^{(j_t)})$. A single $\beta^{(\alpha)}$-particle inherits the state $X_t \setminus \{R_t^{(j_t)}\} $ from its $\alpha$-parent; and additionally, acquires a single uniformly distributed sample for $R_t^{(j_t)}$ from the length-scale prior distribution. These properties make the $\beta$-layer uniquely a particle layer to discover local neighbourhood length-scales at $j_t$, and as such are scored by  $g_2(t, Q_t)$. Then the total likelihood $g_t^{z_t}$ over the entire particle set is given by the product of the $\alpha$ and $\beta$ particle weights. Lastly, we manipulate the various particle distributions so that the particle filter has $n_\alpha$ number of particles at the start and end of a two-step iteration process i.e. total number of resampled particles are constant for all $t$.\\
\\
Box: \textit{Branching steps for qslam.} \\
=== === === === === === === === \\
Given time, $t$, and an incoming data stream, $Z_t$:
\begin{thmprop}
	\item Assume $\bar{X}_{t} = K_{t-1}X_{t-1}$ is known. \label{main:prop:qslam:branching:1}
	\begin{enumerate}
		\item Update $F_t^{(j_t)}$ for each $\alpha$-particle using $h_1(j_t, Z_t)$. 
		\item Weight each $\alpha$-particle according to $ g_1(t, Y_t^{(j_t)})$.
	\end{enumerate}
	\item Assume $F_t$ is known. \label{main:prop:qslam:branching:2}
	\begin{enumerate}
		\item For each $\alpha$-particle, generate $n_\beta$ number of particles of a second type,  called $\beta^{(\alpha)}$-particles.
		\item Weight each $\beta^{(\alpha)}$-particle according $g_2(t, Q_t)$.
	\end{enumerate}
	\item Maximise global likelihood $ g_t^{z_t}$:  obtain $n_\alpha$ particles by resampling the full set of $n_\alpha n_\beta$ particles according to the product of their weights \label{main:prop:qslam:branching:3}
	\item Update $R_t^{(j_t)}$ for each $\alpha$-particle by applying $h_2(F_t)$ to its $\beta^{(\alpha)}$ layer. \label{main:prop:qslam:branching:4}
	\item Marginalise over $\beta^{(\alpha)}$ layer yielding $n_\alpha$ number of $\alpha$-particles with uniform weights.  \label{main:prop:qslam:branching:5}
\end{thmprop}
=== === === === === === === === \\
\item Let $h_1(j, Z_t)$ be deterministic data association mechanism which updates the state variable $F_t^{(j)}$  given $X_{t-1}$ as:
\begin{align}
F_t^{(j)} &:= h_1(j, Z_t) := \cos^{-1} (2P_t^{(j)} - 1) \\
P_t^{(j)} &:= \kappa_{t}^{(j)} + \gamma_{t}^{(j)} \\
\kappa_{t}^{(j)} &:= \frac{\tau_{t-1}^{(j)}}{\tau_{t}^{(j)}}  \kappa_{t-1}^{(j)} + \frac{1}{\tau_t^{(j)}} Y_t^{(j_t)}I_{(j_t = j)} \\
\gamma_{t}^{(j)} &:= \frac{\beta_{t-1}^{(j)} }{\beta_{t}^{(j)}}  \lambda_1^{I_{(q_t= j, q_t \in Q_t)}} \gamma_{t-1}^{(j)} + \frac{\lambda_1^{\tau_t^{(j)}}}{\beta_t^{(j)}} \hat{Y}_t^{(q_t)}I_{(q_t= j, q_t \in Q_t)} \\
\tau_t^{(j)} &:= \tau_{t-1}^{(j)} + I_{(j_t = j)}, \quad \tau_0^{(j)} =0 \\
\beta_t^{(j)} &:= \beta_{t-1}^{(j)} + I_{(q_t= j, q_t \in Q_t)}, \quad \beta_0^{(j)} =0 \\
I_{X} &:= \begin{cases}
1, \quad X \\
0, \quad \neg X 
\end{cases} \\
\lambda_1 &\in [0,1]
\end{align} These set of recursive equations reduce down to the idea that $P_t^{(j)}$ is just an empirical Born probability estimate obtained by averaging the binary measurement data obtained at a single qubit. This data consists of averaging over $\tau_t^{(j)}$ physical measurements at qubit $j$,  and $\beta_t^{(j)}$ quasi-measurements induced by measuring the neighboring qubits of $j$. The role of quasi-measurements in data association is reduced as the number of physical measurements increase, at a rate governed by $\lambda_1$, which along with $\lambda_0$, are the two hyper-parameters of the qslam framework. 
\item Let $h_2$ be deterministic data association mechanism which locally updates the state variable $R_t^{(j_t)}$ as the expectation over resampled $\beta$-particles given the state at $F_t$:
\begin{align}
R_t^{(j_t)} := h_2(F_t) = \ex{\{R_t^{(j_t), n}\}_{n=1}^{n_\beta} | F_t}
\end{align} The dependence on $F_t$ is implicit, as $\beta$-particles are weighted via the $g_2(t, Q_t)$ likelihood function for which it is assumed $F_t$ is known.
\item Lastly, qslam autonomously chooses where to measure next on the qubit grid. In absence of external reference or control trajectories, the simplest controllers can be designed by tracking and using information from the state estimation procedure itself [REFS]. We link the controller to the level of uncertainty in the distribution of the posterior particles of  $\beta$ layer for $R_t^{(j_t)}$ i.e  $\mathbb{V}\{\{R_t^{(j_t), n}\}_{n=1}^{n_\beta} | F_t\}$. Sites with the highest variance over length-scales are chosen as the potential locations for the next physical measurement.  For multiple qubit locations with comparably high variance, we sample uniformly randomly among the locations; and of course, one can inform the controller if any particular qubits are unavailable for a noise-sensing measurement at any $t$. 
\end{enumerate}



























\iffalse
\section{Results} \label{sec:risk}
\begin{enumerate}
	\item FIG: 
\end{enumerate}

\textit{Placeholder section: not written up properly} \\

Each risk metric is the expectation of an error metric taken over many repetitions of simulated datasets. We use two error metrics:  an MSE error metric and a SSIM error metric. For engineered true dephasing maps, the MSE error metric can be expressed in the notation on an L2 norm:
\begin{align}
	MSE = \frac{L_{2}(f - \hat{f})}{L_{2}(f)}
\end{align}
The SSIM error metric compares the structural similarity between images and is used to obtain the universal index for comparing the quality of any two images in recent machine learning literature \cite{wang2004image}. To follow the notation of the original paper, let $x$ and $y$ be two images (or maps) in vectorised form. Then for a comparison of the elements of $x$ and $y$ in a small global region (or multiple local regions) yields:
\begin{align}
	SSIM(y,x) = \frac{(2 \mu_x \mu_y + C_1)(2 \sigma_{xy} + C_2)}{(\mu_x^2  + \mu_y^2 + C_1)(\sigma_{x}^2 + \sigma_{y}^2 + C_2)}
\end{align} In the notation above $\mu_{x,y}$ refers to the mean of an input signal, $\sigma_{x,y}$ is the unbiased sample standard deviation of an input signal, and $\sigma_{xy}$ represents the unbiased sample cross correlation between the two signals. We set $C_1 = C_2 = 0.01$ to stablise the index for data with means close to zero, and the choice of these values make the SSIM score comparable to the universal quality factors, as discussed in detail in \cite{wang2004image}. For our application with small qubit grids, we use the SSIM score globally on the posterior dephasing field, $\hat{f}_t$, (a column vector) in a dataset, setting $x := f_t $ and $y := \hat{f}_t$, with the notation emphasising the order of inputs into the SSIM score as the metric is not a symmetric distance type measure. \\


\section{Numerical Results} \label{sec:results}

\iffalse
[clarify approach]\\
\\
We can consider the performance of qslam for arbitrary spatial arrangements in 2D. A key aspect of performance is whether the physical setting and measurement procedure enables a resolution that allows the qslam algorithm to perform meaningful inference. To investigate spatial resolution, we fix the geometry of the spatial arrangement of qubits and we assume true fields are time-invariant. We vary spatial domains of field values over discrete (suddenly discontinuous) domains. We then explore the performance of the qslam algorithm as a function of the height of the field's discontinuous change (phase resolution); size of field domains relative to total measurement information  (spatial resolution); quality of local single qubit measurements, and the quasi-measurement forgetting factors $\lambda_1, \lambda_2$ i.e. where a value of unity for both factors bring the qslam algorithm to essentially mimic brute force measurements. [ADD TO THEORY, CHECK]. \\
\\
In \cref{figs_zsl_2}, we consider a simple geometry of a linear array of 25 equi-distant qubits, where the true field values are depicted to have two discontinuously split regions. In columns [i]-[iii], the true field is split into a small region (covering 20\% of qubits) of low dephasing (blue) and a large region (covering 80\% of qubits) of high dephasing (green). The low and high dephasing regions are flipped in the last column [iv]. We compare naive approaches with qslam via the Bayes Risk metric discussed in the previous section, where the top row of data panels represents a RMS error loss function and the bottom row represents the SSIM. The orange data in (a)-(d) and the red data in (e)-(h) is generated via a naive brute force approach of measuring qubit phase via single shot Ramsey experiments. The blue data and the indigo data in (a)-(d) and (e)-(h) respectively is generated via the qslam algorithm. The $y$ axis represents the Bayes Risk normalised such that a value of zero represents perfect learning, and unity represents maximal possible error in field map reconstructions. The $x$ axis presents a parameter regime where zero represents inference virtually ignores physical measurements (quasi-measurements dominate) and unity represents  that qslam maximally mimics the brute force measurement procedure (quasi-measurements are virtually ignored). The details of the parameter regimes are provided in Appendix [CODE CHECK] and we note that risk values for the naive approach should be independent of the $x$ axis in all cases (flat-lines).  \\
\\
We increase the number of iterations from column [i] to [iv] in \cref{figs_zsl_2}. In this case, each iteration correspond to one single qubit measurement per iteration. The total number of measurement budget is chosen so that every qubit is measured twice in the naive approach in column [ii] and 10 times in column [iii] and [iv]. In qslam, the location of the single qubit measurement at each iteration is given by the controller.  The RMSE based risk metric in (a) - (d) confirms the general trend that as the number of measurements increase, both algorithms show a reduced error score but that performance of qslam does not depend on the choice of the regimes for $\lambda_1, \lambda_2$. In contrast, the SSIM based risk metric shows as the total amount measurement information increases, qslam performance shows a clear dependence on the choice of the regimes for $\lambda_1, \lambda_2$. Further, for the parameter regimes chosen, the SSIM risk is minimised at a value that is not unity. This dependence of performance on $\lambda_{1,2}$ does not depend on the values of the true field if field domain structures are preserved, for example, by flipping the field from column [iii] to [iv]. For each of the shaded configurations, we show a single map reconstruction from naive and qlam on the top of each data column.\\
\\
FIG: remove true field  / number of measurement arrows in \cref{figs_zsl_2}. 
\begin{figure}[H]
	\includegraphics[scale=0.8]{figs_zsl_2}. 
	\caption{\label{figs_zsl_2} Caption here}    	
\end{figure}
Corresponding to the shaded configuration of $\lambda_{1,2}$ in the previous results, we now fix the forgetting factors and the true field configuration, and we change the quality of local measurements from [i] to [iv] in \cref{figs_zsl_1}. Here, each single qubit measurement is repeated at the same location a number of times before the next iteration is commenced. The total measurement budget for qslam and naive approach is conserved for each experiment and simply increases the overall number of measurements in the brute force approach. However for qslam, the increase in the number of qubit measurements per qubit corresponds to increased quality of the local qubit phase estimate before the local state estimate is shared with neighbouring qubits via a quasi-measurement calculation. In panels (a)-(d), the RMS risk metric for both algorithms is plotted against the number of iterations, where measurements per qubit per iteration increase from 1, 2, 8, 50 from left to right. qslam incurs lower RMS risk than naive for  small and medium values of total iterations, and converges in performance with naive approach in the high information regimes.The performance for the naive approach rapidly improves at iteration = 25, where each location in the linear array has been measured at least once, and this drop in the RMS risk metric becomes more pronounced as the quality of local information improves.  The SSIM risk metric mirror these general trends, but depicts a larger performance gap between the the two algorithms.\\
\\
In panels (i)- (l), we plot the amount of information required by qslam to achieve the same SSIM score under a brute force naive approach. Here,  the SSIM scores of (e) - (h) are plotted along the $x$-axis and the $y$-axis quantifies information given to qslam as a \% of the total measurement budget for the naive approach plotted on a log-scale. In all panels, for almost all values of acceptable SSIM scores between $0-0.5$, the information required by qslam is less than naive ($< 100\%$). To achieve SSIM risk arbitrarily close to zero, all information curves rise  corresponding to the high information regime (right extremal data) in panels (e)-(h). The relative information gain is minimised for an SSIM score of $0.1-0.2$ for the parameter regimes show, and rises again for higher scores corresponding to fewer iterations of the qslam algorithm in the low information regime (left extremal data) in panels (e)-(h).\\
\\
In \cref{figs_zsl_1} (m), we test whether the SSIM risk scores for qslam vary if the correlation lengths of the true field change from columns [v] - [viii], where a true field, and a single map reconstruction from qslam and naive are shown for 125 iterations, with a single measurement per iteration. The field split ratios determine the ratio of a low dephasing true field region over the linear array, and this is plotted on the $x$-axis of the two data panels. The $y$-axis depicts the SSIM risk metric on a log-scale, and each data line corresponds to a different number of total iterations. The left (right) plot shows data from qslam (naive ) in different shades of blue (orange). For all algorithms, all lines are relatively flat showing that the variation in SSIM risk metric is low even as the true field changes from left to right. All data lines drop vertically as the number of iterations (information) increase, confirming the intuition that performance improves as more data is made available for inference. However, data markers for qslam are  are substantially lower than the corresponding markers for the naive approach and this shows qslam outperforms naive for all parameter regimes depicted in panel (m).\\
\\
FIG/CODE CHECK: remove anything about linearisation!. 
CODE CHECK: msmt budget is compared properly between qslam and naive as msmts / qubit are increased.
FIG/CODE CHECK: \cref{figs_zsl_1} (m) - make colours same in both plots. or find another way to show this information. \\
\\
\begin{figure}
	\includegraphics[scale=0.8]{figs_zsl_1}. 
	\caption{\label{figs_zsl_1} Caption here}    	
\end{figure}
Preserving the geometry of the physical setting, we introduce measurement error in \cref{fig_nsl_v1}. We explore two different noise classes: salt and pepper noise and dark noise. Physically, salt and pepper noise corresponds to introducing classification error, where randomly chosen measurements are set to extremal values zero or one with probability half. Dark noise represents randomly chosen measurements set to value zero. The noise strength parameter represents the probability that measurements are corrupted by noise - where zero represents the noiseless case (reported previously)  and one represents maximal noise. For example, under dark noise with strength parameter unity, all measurements fed to an algorithm are zero. In \cref{fig_nsl_v1}, we increase the number of iterations from [i]-[iv] fixing one measurement per qubit per iteration. The $y$-axis depicts risk metrics for all data plots and the $x$-axis varies the noise strength parameter for S\&P noise in (a)-(h) and Dark noise in (j)-(q). In virtually all cases, qslam outperforms naive approaches in low and moderate noise regimes, and both algorithms converge the maximal risk value for extremely high noise regimes. The SSIM risk metric breaks down in (q) for Dark noise at unity strength parameter since the SSIM score is not stable when the means of the data are close to zero (in our case, data will be exactly zero). We depict single map reconstructions from the shaded regimes corresponding to XX\% noise level below each data panel, where the naive approach essentially summarises the measurement outcome without any measurement noise filtering. The weight factor for qslam is set to the value of, as before, in \cref{figs_zsl_2}. No noise filtering parameters were tuned in qslam, and the smoothening effect is due to the quasi-measurement mechanism alone. In panels (s)-(v), we consider S\&P noise with improved local quality of measurements by using eight measurements per qubit before sharing globally. With improved local in formation the performance gap between naive and qslam closes rapidly as the number of iterations increase.\\
\\
FIG : delete lengthscale distributions 
NB: can't add a noise corrupt input image unless you take the same positions on the map and wipe them out for 1 mst per qubit.\\
\\
\begin{figure}
	\includegraphics[scale=0.8]{fig_nsl_v1}. 
	\caption{\label{fig_nsl_v1} Caption here}    	
\end{figure}
We conclude the discussion of the linear 1D array of qubits by looking that many runs of the same experiment and plotting the posterior length scale and posterior map value at a single qubit at the boundary between two domains in \cref{fig_correlation_v0_summary}. Each row corressponds to an increase in the number of total iterations, where one measurement occurs per qubit per iteration. A single qslam run is repeated 150 times and the posterior correlation length and map value at qubit 11 (as shown) is a single data point in the left and right columns respectively. As the number of iterations increase, the distribution of posterior map value over many independent runs converges to the true map value (dotted line). In contrast, the distribution of posterior correlation lengths broadens  and appears localised only in the small information regime.\\
\\
FIG: add the position of qubit 11. See if there is a similar effect for increase in noise levels and plot that too. \\
\\
\begin{figure}
	\includegraphics[scale=0.8]{fig_correlation_v0_summary}. 
	\caption{\label{fig_correlation_v0_summary} Caption here}    	
\end{figure}
We extend the qslam analysis to consider a grid arrangement in 2D, with a true field that varies discontinuously between three domains as shown in \cref{figs_zqg_3}. We increase the number of iterations from columns [i]-[iii], and in high information regimes, the SSIM-risk metric is minimised for a weighting ratio that is less than unity. Shaded configurations of the weighting ratios in (a)-(h) correspond to the single run maps plotted in panel (i). In the bottom half of the figure, we increase the field difference at the boundaries of the true field from columns [v]-[vii], where the corresponding risk metrics plotted as a function of the weighting ratio. Column [vii] repeats the results presented in the first half of the figure to link the comparison to earlier results. For an extreme difference between the low and high field domains, as in [viii] qslam breaks at the brute force algorithm outperforms qslam for any choice of $\lambda$ weighting ratio. \\
\\
FIG : remove labels and arrows. Don't plot single run maps from keeping the forgetting factor constant. Pick panel (h) and plot maps for different weighting ratios in panel (h) in (i). OR only retain [v]-[viii] and cover the rest in the next figure. 
QUESTION: for moderate information regimes, if the risk metric is minimised at unity, shouldn't the value of the SSIM-risk be the same as naive approach? namely, explain (f) and (g). In what ways is the forgetting factor at unity not the same for naive approach and qslam? \\
\\
\begin{figure}
	\includegraphics[scale=0.8]{figs_zqg_3}. 
	\caption{\label{figs_zqg_3} Caption here}    	
\end{figure} 
As in the linear regime, we depict performance as total information for both approaches is increased - first, by increasing the number of iterations and second, by improving the local quality of measurement in \cref{figs_zqg_1b}. In (a)-(d), we depict increasing quality of local measurements from left to right. The risk metrics are plotted as a function of the total number of iterations. In high information regimes, both algorithms converge in performance. In low to moderate information regimes, qslam outperforms the naive approach by a considerable margin according to both RMS and SSIM based risk metrics. We take a selection of these parameter regimes (shaded green) and visually inspect single map reconstructions from qslam and naive in (i). From columns [i]-[ii], we increase the total number of iterations, which increases total information and spatial sampling of the 2D grid. From [ii]-[iv], we increase the total information budget by increasing the number of measurements per qubit per iteration before local state estimates are shared globally in qslam. 
\begin{figure}
	\includegraphics[scale=0.8]{figs_zqg_1b}. 
	\caption{\label{figs_zqg_1b} Caption here}    	
\end{figure}

\clearpage
NB: There is no need for \cref{figs_zqg_2}
\begin{figure}
	\includegraphics[scale=0.8]{figs_zqg_2}. 
	\caption{\label{figs_zqg_2} Caption here}    	
\end{figure}




\fi
\section{Discussion} \label{sec:discussion}
\section{Conclusion} \label{sec:conclusion}
%The recursive calculation of the posterior, as well as the normalisation process, is given in the pseudo-code of a typical particle filtering algorithm (see, for example, \cite{candy2016bayesian}). In a typical recursive calculation, each particle represents a hypothesis about the true state, and is denoted as $x_t^{(\alpha)}$. These particles are propagated using a known or chosen  $\prob{}{x_{t}| x_{t-1}, u_{t-1} }$. Given propagated states, one computes the weights of particles. 







\iffalse
	\begin{algorithmic}[0]
		
		\Procedure{Euclid}{$a,b$}\Comment{The g.c.d. of a and b}
		\State $r\gets a\bmod b$
		\While{$r\not=0$}\Comment{We have the answer if r is 0}
		\State $a\gets b$
		\State $b\gets r$
		\State $r\gets a\bmod b$
		\EndWhile\label{euclidendwhile}
		\State \textbf{return} $b$\Comment{The gcd is b}
		\EndProcedure
		
	\end{algorithmic}
	
\fi
\fi