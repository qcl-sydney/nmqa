\section{Physical Setting}\label{sec:physicalsetting}

We consider a spatial arrangement of qubits coupled to a common environment. For simplicity, we model the environment as a randomly fluctuating dephasing noise field exhibiting correlations in both space and time. The value of the dephasing field at any node induces a relative phase between the ground and excited states of the single qubit superposition state, thereby affecting the  quantum mechanical probability of observing a  `0' or `1' in a Ramsey measurement.  We make a sequence of sensing measurements  at different qubits in the arrangement. Each measurement is a single qubit operation yielding a $0$ or a $1$ outcome. This outcome is labeled by the $\pmsmt{j_t}{t}$, where $j_t \in [0, N]$ marks the sensing qubit (control input) at the $t^{th}$ measurement,  for $t = 1,2, \hdots T$. The resulting record of $T$ measurements, in the simplest case, are a set of time-stamped and space-indexed binary measurements. \\
\\ 
In our arbitrary spatial arrangement of qubits, the location of a single qubit is called a `node', and the arrangement is called a `grid', with all nodes specified as coordinates in $\mathbb{R}^2$. Consider a qubit grid with $N$ qubits at each node. All qubits on this grid are coupled to a common, classical, spatio-temporal dephasing field, and we denote this field as a vectorised set of values of the continuous field over a discrete, finite set of nodes, $\map{t}$. In the material to follow, we approximate the true  continuously varying dephasing field using a collection of sigmoids (in this manuscript, specifically, using Gaussian functions). Namely, we spread or smear the information from each physical measurement to a small set of neighbouring qubits, and the size of these neighbourhoods centered at a node $k$ is given by a spatial length-scale parameter, $\rval{k}{t}$. Using a sigmoid approximation, we use $\rval{k}{t}$ to evaluate to reconstruct the dephasing field at each node. \\
\\
Collectively, the position of the sensing qubit at $t$, $s_t$, the true dephasing field over all nodes, $ \map{t}:=\{\mval{k}{t} \}$, and the neighbourhood length scales $\rstate{t}:= \{\rval{k}{t}\}$, for the entire qubit grid $ k= 1, 2, \hdots N $ can be written as an extended state vector, $x_t$, for our inference problem:

% acts is a proxy for a classical mobile robot; and the smearing of a projective measurement information onto its neighbours is a proxy for the classical equivalent of taking a scan of the environment. However, our proxy for the classical mobile robot hops discontinuously rather than smoothly according to some \textit{ a priori} dynamical model. Further, our proxy environment scan itself depends on the state vector - namely, we wish to infer the relevant length scales $\rstate{t}:= \{\rval{j}{t}\}, \quad  j= 1, 2, \hdots N $ of our system.  In overcoming challenges related to the application of SLAM to a qubit control application, we are lead to naturally define an extended state vector and a new likelihood function for any a SLAM-based inference procedure. \\
\begin{align}
x_t & := \begin{bmatrix}
\pose{t} & 
\rval{1}{t} &
\hdots &
\rval{N}{t} &
\mval{1}{t} &
\hdots &
\mval{N}{t} 
\end{bmatrix}^T
\end{align}


\subsection{Sensing Qubit Control ($\pose{t}$)}
Over a sequence of measurements, the changing availability of the sensing qubit defines a control trajectory on the grid, given as $\{u_{t}\}_{1:t:T}$. This trajectory is not a continuous path, but resembles hopping on the grid. This trajectory arises from two assumptions: (a) that only one qubit can involved in a sensing measurement at one time, and (b) the availability of a qubit for sensing changes with time i.e. a qubit is released and absorbed into computations. Imperfections in the control are seen as uncertainty in the classical noise field relative to the physical grid and are modelled as jitter in the knowledge of a qubit position. Physically, this arises from weak time domain jitter in the noise field and /or imprecision in technical specifications of a hardware device. We model this as white, Gaussian zero mean input `process' noise, $w_t$. The resulting dynamical model for the position of the sensing qubit, $\pose{t}$ is:
\begin{align}
\pose{t} = u_{t-1} + w_t, \quad w_t \sim \mathcal{N}(\mu_w, \sigma_w^2 \mathbb{I}) \quad \forall t \label{eqn:physetting:1}
\end{align} In our notation, we reserve the index, $j_t$, to label the node where a physical measurement is performed, with the Euclidean position coordinates of the node $j_t$ are given by $\pose{t}$.

\subsection{Spatio-Temporal Dephasing ($\map{t}$)} \label{sec:subsec:dephasing}

As observed in literature, the probability of observing a $0$ or a $1$ measurement on a single qubit is given by Born's rule. In the limit of measuring every qubit on the grid an infinite number of times (either by running parallel experiments or letting $T \to \infty$ for time-invariant noise fields), we know that the histogram of physical measurements on each qubit will yield a sample probability that converges to the true Born probability for any pair $(j_t,t)$. Let $\mval{j_t}{t}$ denote the effect of the true dephasing field in units of a relative phase between qubit states at the node $j_t$, and let $\pmsmt{j_t}{t}$ be a physical measurement outcome of a Ramsey experiment on a qubit at the node $j_t$. We write the true dephasing field over all nodes is the state vector $ \map{t}:=\{\mval{k}{t} \}, \quad \forall k \in [1,N]$. Then the sample probability for seeing the qubit in the  $\pmsmt{j_t}{t}=1$ state is obtained in the large data limit for any $(j_t, t)$ pair as: \\
\begin{align}
\hat{P}_\infty(\pmsmt{j_t}{t}=1|I)^{(j_t)} & = \cos^2(\frac{\mval{j_t}{t}}{2}), \quad \forall j_t \in [0, N], t \in [0, T] \label{eqn:physetting:born_singlequbit}
\end{align}
\cref{eqn:physetting:born_singlequbit} provides a link between experimental observations and the state variable of interest, namely, the dephasing field. However, unlike a classical mapping problem, we cannot observe the dephasing field using a `measurement scan' - namely, that a single measurement procedure reveals information about the dephasing field only at the node $j_t$, and is not comparable, for example, to a collection of data values received from a classical environmental scanning devices (e.g. using a visual laser scan or sonar). \\
\\
In the next section, we develop techniques to allow us to use the measurement information obtained at $j_t$ to infer some information about the dephasing field experienced by the neighbouring qubits around $j_t$.  


\section{ Field Reconstructions using Quasi-Measurements }  \label{sec:fieldreconstruct}

In classical applications,  an environmental map is a collection of specific features (e.g. boundaries, edges, objects, spikes). These map features extracted from environmental scanning data (observation) taken from one location, and unambiguously re-observed from another location on the map. The correct re-observation of existing map features from different locations helps to reduce errors in both map reconstruction, and the accuracy in which one's own current position can be placed on the map. The rule or procedure by which one updates the environmental map based on raw data is known in classical literature as the `data association' step: classically, this step can require an additional inference procedure to identify map features in a stream of raw data generated by a scanning device for each `measurement' indexed by $t$.\\
\\
The uniqueness of our application lies in that firstly, there is no quantum equivalent of a environment scanning device which can scan the dephasing environment without measuring (hence destroying) information stored on qubits. A single measurement on a sensing qubit allows us to only to infer something about the dephasing field at a single location. Secondly, we have an \textit{apriori}  deterministic function to update the dephasing noise field map if the state vectors $\pose{t}, \rstate{t}$ are given: namely, the data association step in our application is entirely given by applying Born's rule for a single qubit projective measurement. \\
\\
With the observations mentioned above, we proxy the classical environmental scanning procedures by exploiting the fact that classical sources of noise generate fields which are continuously varying everywhere for our application. In particular,  we will smear the measurement information obtained from the sensing qubit at $j_t$ to other neighbouring qubits in a small region around $j_t$. The details for the data association step and the smearing mechanism are the subjects of the next two sub sections respectively. 

\subsection{Data Association via Born's Rule}


Since the Born rule must be satisfied irrespective of the inference procedure, we assert that the data association step is simply the inversion of the Born rule in \cref{eqn:physetting:born_singlequbit} using an empirical estimate of Born probability. This empirical estimate for measuring the `1' qubit state is defined shortly (in \cref{eqn:physetting:samplebornprob}). However, at present, we define the inverse estimator for the dephasing field as $\hat{M}_t^{(k)}$ based on the empirical estimate of the Born probability:
\begin{align}
\hat{M}_t^{(k)} &:= \cos^{-1}\left( 2\probest{}{\{1\} | \pose{t}, \rstate{t}, x_{t-1}, u_{t-1}, \mathcal{D}_{t-1}} - 1 \right) \label{eqn:physetting:mapdatassoc:2} \\
& \forall \quad  k \in \{j_t, \qset{q}{j_t q, t } \}, \nonumber 
\end{align} Here, the index $j_t$  is fixed by the location of the physical measurement at the sensing qubit  given by $\pose{t}$. In particular, the location index $j_t$  will be the key output of a controller introduced in \cref{sec:control}. The function  $\hat{M}_t^{(k)}$ theoretically prescribes how a noise map should be updated at $t$ if $\pose{t}, \rstate{t}, z_t$ are all given, and $\map{t-1}$ is available from the previous time step. The term inside the cosine, $\probest{}{\{1\} | \pose{t}, \rstate{t}, x_{t-1}, u_{t-1}, \mathcal{D}_{t-1}}$ can be approximated by \cref{eqn:physetting:samplebornprob} when the noise field is $\map{t}$ slowly drifting with respect to $t$, and the approximation is exact for the invariance condition $\map{t} = \map{t-1}$. (At present, we defer the explanation of the $\qset{q}{j_t q, t } $ term, as this involves the definition of quasi-measurements and neighborhoods, as explained below.) Hence, \cref{eqn:physetting:mapdatassoc:2} enables us to update a single value of the dephasing map based on measurement data via the empirical estimate, $\probest{}{\cdot}$. \\
\\
We now introduce the concept of `quasi-measurements' to define a small neighbourhood around the sensing qubit over which noise field information can be `smeared'. Unlike physical measurements taken on a qubit, quasi-measurements are simulated measurements, where the measurement outcome depends on the smearing of the map estimate at node $j_t$ over its neighbourhood. A single physical measurement and its set of associated quasi-measurements are all treated an equal footing - this set of data collectively defines the classical equivalent of `scanning' the environment at $t$. The details of how to generate a quasi-measurement are reserved in the next sub-section. At present, we define  the empirical estimate, $\probest{}{\cdot}$, which estimates Born probabilities using both physical measurements ($\pmsmt{j_t}{t}$) and quasi-measurements ($\qmsmt{j_t}{t}$) at a node, $j_t$. The sample probability function estimates the Born probability for measuring an `up' ($1$) qubit state at the node $j_t$ is:
\begin{align}
\probest{}{&\{1\} | \pose{t}, \rstate{t}, x_{t-1}, u_{t-1}, \mathcal{D}_{t-1}} \nonumber \\ 
&=  \frac{1}{|\tau^{(j_t)}_t|}\sum_{i \in \tau^{(j_t)}_t} \pmsmt{j_t}{i} + \frac{\lambda_1^{|\tau^{(j_t)}_t|}}{|\beta^{(j_t)}_t|}\sum_{i \in \beta^{(j_t)}_t } \qmsmt{j_t}{i}  \label{eqn:physetting:samplebornprob} \\ 
\probest{}{&\{0\} | \pose{t}, \rstate{t}, x_{t-1}, u_{t-1}, \mathcal{D}_{t-1}} \nonumber \\
&= 1 - \probest{}{\{1\} | \pose{t}, \rstate{t}, x_{t-1}, u_{t-1}, \mathcal{D}_{t-1}} \\
\tau^{(j_t)}_t , \beta^{(j_t)}_t & \subseteq [1, t] \label{eqn:physetting:msmtcountset:1}  \\
\lambda_1 & \in (0,1) \label{eqn:physetting:forgettingfactor} 
\end{align} We recall that the index $j_t$  on the right hand side of \cref{eqn:physetting:samplebornprob} denotes the location of the physical measurement at the sensing qubit given by $\pose{t}$. For the sample probability function at the node $j_t$, we let $\tau^{(j_t)}_t $ be the set of $t$-indices for physical measurements at $j_t$, and $ \beta^{(j_t)}_t $ be the set of $t$-indices for quasi-measurements at $j_t$, over the entire record of $[1, t]$ sensing measurements. In \cref{eqn:physetting:samplebornprob}, the first term is the sample probability  implied by a total of $|\tau^{(j_t)}_t|$ physical measurements performed on the qubit at node $j_t$; whereas the second term is the sample probability  implied by a total of $|\beta^{(j_t)}_t|$ quasi-measurements. The utility of quasi-measurements at $j_t$ is negligible if the qubit at $j_t$ is physically measured many times. We set the first forgetting factor, $\lambda_1$, such that the contribution to sample probability function due to a quasi-measurement at $q$ for high $|\tau^{(j_t)}_t|$ regimes can be tuned via the choice of $\lambda_1$.  \\
\\
\cref{eqn:physetting:msmtcountset:1} states that the counting sets, $\tau^{(j_t)}_t, \beta^{(j_t)}_t$ are subsets of the $t$-indices over $[1,t]$. We state two important properties of these subsets below:
\begin{align}
\tau^{(j_t)}_t \cap \beta^{(j_t)}_t & \equiv \emptyset , \quad\forall j_t \in N, \forall t \in T \label{eqn:physetting:msmtcountset:2} \\
Nt & \geq \sum_{j_t=1}^{N} |\tau^{(j_t)}_t| + |\beta^{(j_t)}_t| , \quad \forall t \in T \label{eqn:physetting:msmtcount} 
\end{align} \cref{eqn:physetting:msmtcountset:2}  specifies that one cannot associate a physical and a quasi-measurement for a node at $j_t$ at the same $t$, namely,  $\tau^{(j_t)}_t, \beta^{(j_t)}_t$ do not overlap. The total number of physical and quasi-measurements generated over the entire grid and over an entire sensing procedure until $t$ is therefore $\leq Nt$. If all neighborhoods encompass all nodes on grid over the entire sensing procedure, then equality holds in \cref{eqn:physetting:msmtcount}. \\

Each quasi-measurement in the second term of \cref{eqn:physetting:samplebornprob} is generated whenever $j_t$ is found to be in the neighborhood of another sensing qubit at any point in sensing procedure until $t$. In the next sub-section, we specify how to generate quasi-measurements by smearing information from a single physical measurements over its neighbourhood. 

\subsection{Quasi-Measurements ($\qmsmt{q}{t}$) and Neighbourhood Lengthscales ($\rstate{t}$)}

We mimic classical environmental scanning procedures by exploiting the fact that noise fields are continuously varying everywhere. The continuously varying noise fields allows us posit that  measurement information obtained from the sensing qubit at $j_t$ can be blurred locally over other neighbouring qubits in a small region around $j_t$. For the blurring process to be meaningful, we need a mechanism to discover the approximate length-scales over which the true noise field exhibits variation. The design of this smearing mechanism involves the generation of `quasi-measurements', as defined below.\\
\\
We denote $\qset{j_t}{t}$ as the neighbourhood of the qubit at $j_t$ and $\qset{j_t}{t}$ consists of a list of neighbouring qubits. These quasi-measurements are taken over neighbouring qubits, and yield a $0$ or $1$ outcome, and are denoted $\qmsmt{q}{t}$. In our notation, we reserve the usage of $q$ to label nodes in the neighbourhood of $j_t$ where a quasi-measurement is generated. Here, the neighbours are labeled as $q = 1, 2, \hdots |\qset{j_t}{t}|$, where $|\cdot|$ represents cardinality of a set, in this case, the number of neighbours stored in $\qset{j_t}{t}$. The list of neighbours changes with $t$ - namely the neighbourhoods can expand or contract as real physical data is collected. We parameterise the size of this neighbourhood as an additional physical state variable, a length-scale, $\rstate{t}:= \{\rval{k}{t}\}, \quad  k= 1, 2, \hdots N $. If we take a physical measurement on the qubit at node $j_t$, then the true $\rval{j_t}{t}$ specifies the degree to which the physical measurement at $j_t$ contributes meaningful information about its neighbours. \\
\\
For now, we denote the smearing action by an arbitrary function, $\kernel(v_{j_tq},\rest{j_t}{t}, \mest{j_t}{t})$, which we will simply call a \textit{kernel}. This kernel depends on the Euclidean distance between a node $j_t$ and a neighboring node $q$, denoted $v_{j_tq}$; the length scale estimate $\rest{j_t}{t}$ and map estimate, $\mest{j_t}{t}$ at the node $j_t$. Suppose we have the best possible estimates, $\rest{j_t}{t}$ and $\mest{j_t}{t}$.  Then, we apply $\kernel(v_{j_tq},\rest{j_t}{t}, \mest{j_t}{t})$ to smear $\mest{j_t}{t}$ over all neighbors listed in $\qset{j_t}{t}$. Next, we estimate what a projective measurement, $\qmsmt{q}{t}$, on every neighbor $q$ would look like, using Born's rule, giving us a set of binary outcomes i.e. `quasi-measurements'. Hence, each physical measurement generates quasi-measurements over its neighbors, and the total data generated at any $t$ is effectively, $z_t := \{\pmsmt{j_t}{t}, \{ \qmsmt{q}{t} \}_{1:q:|\qset{j_t}{t}|}\}$.\\
\\
Each quasi-measurement at node $q$, due to the $t^{th}$ sensing measurement performed at node $j_t$, is generated via quantisation procedure introduced in \cite{gupta2018machine} and justified in \cite{riddhinotes}. This quantisation procedure is captured in notation as the $\mathcal{Q}(\cdot)$, and represents a binomial coin toss experiment for a single coin and one trial with a probability of success given by the argument of $\mathcal{Q}(\cdot)$. In our case:
\\
\begin{align}
	\qmsmt{q}{t} & :=\mathcal{Q}\left( \cos^2(\frac{\qbornm{q}{(j_tq)}{t}}{2}) \label{eqn:quantiser:2} \right) \\
	\qbornm{q}{(j_tq)}{t} & :=  (1 - \lambda_2^{|\tau^{(q)}_{t-1}|})\mest{q}{t-1} + \lambda_2^{|\tau^{(q)}_{t-1}|}\kernel(v_{j_tq},\rest{j_t}{t}, \mest{j_t}{t}) \label{eqn:quantiser:3} \\
	\lambda_2 &\in (0,1) \label{eqn:forgetting_factor_2}
\end{align} \cref{eqn:quantiser:2} is a re-expression of the Born rule to generate quasi-measurements. The smearing of a physical measurement at $j_t$ to yield map information at node $q$ is captured in \cref{eqn:quantiser:3}. The first term in \cref{eqn:quantiser:3} represents the contribution of the best estimate of the map at $q$ until $t-1$. The $t$ measurement at $j_t$ contributes the second term in \cref{eqn:quantiser:3} where the smearing happens according to a known, a priori $\kernel(v_{j_tq},\rest{j_t}{t}, \mest{j_t}{t})$. In particular,  the kernel can be any sigmoid function that enables one to invoke the continuity assumption in the behaviour of the noise field. A second forgetting factor, defined by \cref{eqn:forgetting_factor_2}, appears as a tunable parameter for governing the influence of quasi-measurements on the inference procedure. From carefully choosing $(\lambda_1, \lambda_2)$ parameter regimes, one can tune the inference procedure from heavily relying on quasi-measurements  to entirely ignore quasi-measurements. The latter case effectively reduces to a naive averaging of physical measurements.\\
\\
 We will fully specify \cref{eqn:quantiser:3} by using  a Gaussian function but other choice of sigmoids remain unexplored (see, for details, \cite{ito1992approximation}):
\begin{align}
	\kernel(v_{qj_t},\rest{j_t}{t}, \mest{j_t}{t}) &:= \mest{j_t}{t}\exp\left(-v_{qj_t}^2 / (\rest{j_t}{t})^2 \right)  \label{eqn:kernel}
\end{align} \\
\\ The general form of \cref{eqn:quantiser:3} and the \cref{eqn:kernel} are both somewhat arbitrary. One could attempt to side-step the idea of quasi-measurements by setting the map update for $\mval{q}{t}$ as $\qbornm{q}{(j_tq)}{t}$. However, such an equation would define an update to the state variable that does not to take account of physical measurement data at node $q$ (see for example, the first term of \cref{eqn:physetting:samplebornprob}).  \cref{eqn:quantiser:2,eqn:quantiser:3,eqn:kernel} enable a full calculation of the quasi-measurements in the second term of \cref{eqn:physetting:samplebornprob}. \\
\\
This concludes the conceptual modifications for a typical SLAM framework for a specific quantum context, in particular, we substituted the Born rule as a data association step in SLAM and created a proxy environment map scanning device by generating quasi-measurement data in local neighborhoods surrounding each qubit node. \\
\\
We now turn to the problem of articulating our physical application under the SLAM framework. In the next few sections, we will begin by stating the full Bayesian inference problem statement in the present context. Next, we will make simplifications to the full Bayesian problem such that QSLAM can be solved by an iterative, numerical approach using a modified particle filter. 
\iffalse
The performance of any inference framework using both physical and quasi-measurements depends on two critical factors: (a) estimating the relevant length scales for all $j_t$ neighbourhoods, in the vector $\rstate{t}$, as data is made available and (b) that the design of the kernel function $\kernel$ improves state estimation without introducing a systematic bias to the inference procedure. We use substantial numerical simulations to characterise and check quasi-measurement framework in \cref{sec:results}.
\fi

\section{Full Bayesian SLAM for Physical and Quasi-Measurements}  \label{sec:qslam}
 
We will now outline the recursive Bayesian inference problem for our application and we show that this inference problem can be resolved by iterative likelihood maximization using a particle filter with two sets of particles. The first set of particles represent a set of hypotheses for the true state $x_t$; while the second set of particles cluster around each node, $k$, and represent the distribution of physically relevant length scales, $\rval{k}{t}$. Information from both sets of particles are recombined to give the Bayesian posterior distribution at any $t$. The stratified approach arises as we incorporate the data association step via Born's rule; and exploit the continuously varying property of dephasing fields via quasi-measurements. \\
\\
\textit{TO DO: Haven't tried this: For time-invariant dephasing fields, it is possible to adopt a simple iterative maximum likelihood function potentially without two tiers of particle filter (refer end of this section)} \\
\\
Let $x_t$ be the true state vector and $j_t$ be the node at which a sensing measurement is taken. We will treat physical and quasi-measurements on an equal footing so that the total set of input environmental sensor data is: $z_t := \{\pmsmt{j_t}{t}, \{ \qmsmt{q}{t} \}_{1:q:|\qset{j_t}{t}|}\}$. The next sensing qubit is given by the control $u_t$ and $\mathcal{D}_t$ be the set of controls and datasets over the entire procedure, i.e. $\mathcal{D}_t := \{z_1, u_1, \hdots, z_t \}$.  We will use the subscript ${}_{1:t}$ to denote the entire set of variables until $t$. Under the Markov assumption, the Bayesian prior can be written in terms of transition probability distribution that only depend on the state at the previous time step:
\begin{align}
\prob{}{x_{0:t} | u_{0:t}} & := \prob{}{x_{0}} \prod_{t'=1}^{t} \prob{}{x_{t'}| x_{t'-1}, u_{t'-1}  } \label{eqn:slam:transition:1}
\end{align} We extend the Markov assumption such that the likelihood depends only on the current state and can be expanded over the entire dataset as:
\begin{align}
\prob{}{z_{0:t} | x_{0:t}, u_{0:t}} & := \prod_{t'=0}^{t} \prob{}{z_{t'} | x_{t'}, u_{t'-1} } \label{eqn:slam:likelihood:1}
\end{align}  Using Bayes Rule,
\begin{align}
\prob{}{x_{0:t} | z_{0:t} , u_{0:t}} &:= \frac{\prob{}{x_{0:t} , z_{0:t} | u_{0:t}}}{\int \prob{}{x_{0:t} , z_{0:t} | u_{0:t}} dx_{0:t}}, \label{eqn:slam:bayes:1}
\end{align} we will state both the prediction and update equations for the inference procedure for the true state, and the approximations under importance sampling techniques that allow numerical solutions  in particle filtering. \\
\\
With the definitions stated above, one can obtain the recursive form of Bayes rule at $t$ as:
\begin{align}
&\prob{}{x_{0:t} | z_{0:t}, u_{0:t}} \nonumber \\ 
& = \frac{\prob{}{x_{t}| x_{t-1}, u_{t-1} } \prob{}{z_{t}| x_{t}, u_{t-1} }}{\prob{}{y_{0:t} | y_{0:t-1}}}\prob{}{x_{0:t-1} | z_{1:t-1}, u_{0:t-1}} \label{eqn:slam:bayes:2}
\end{align} Integrating both sides with respect to $\int dx_{0:t-1}$ will yield the one step head prediction and update equations typically seen in Bayesian inference. This integration reduces a multi-dimensional Gaussian over the entire measurement record and state variables to the simpler one step ahead prediction and recursive Bayes update equation.
% \begin{align}
% &\prob{}{x_t | \mathcal{D}_t}  \nonumber \\
% & =\eta_t\prob{}{z_t | x_t, u_{t-1}} \int \prob{}{x_t| x_{t-1}, u_{t-1}}  \prob{}{ x_{t-1}| \mathcal{D}_{t-1}} dx_{t-1} \label{eqn:recusrivebayesfilter:2}
% \end{align} 
The SLAM problem is introduced explicitly, as in \cite{thrun2001probabilistic}, by substituting the sensing qubit coordinates, noise map, and length scales for the true state $x_t$.	We expand the resulting terms  using the product rule and simplify based on physical arguments, to obtain: 
 \begin{widetext}
  	\begin{align}
  	&\prob{}{s_t, \map{t}, r_t | \mathcal{D}_t} \nonumber \\
  	& = \eta_t\prob{}{z_t | s_t, r_t, \map{t}, u_{t-1}} \int \int \int \prob{}{ s_t| s_{t-1}, u_{t-1}}  \prob{}{ \map{t}|  \map{t-1}} \prob{}{ r_t|  r_{t-1}, \map{t-1}} \prob{}{s_{t-1}, r_{t-1}, \map{t-1}| \mathcal{D}_{t-1}}  ds_{t-1} d\map{t-1} dr_{t-1}   \label{eqn:slam:bayes:3} 
  	\end{align} 
  \end{widetext} In the derivation of \cref{eqn:slam:bayes:3}, $\eta_t$ is a unknown normalisation constant and we observe that the effect of sensing qubits does not depend on the map, or the length-scales.  We obtain two Markov transition probabilities, or dynamic models:  $\prob{}{ s_t| s_{t-1}, u_{t-1}}$ for the choice of qubits for sensing, and $\prob{}{ r_t, \map{t}|  r_{t-1}, \map{t-1}}$ for time evolution of a noise field and the implied evolution of associated length-scales for the system. We separate $\prob{}{ r_t, \map{t}|  r_{t-1}, \map{t-1}}$ using the product rule and this separates the natural evolution of the environment with any dynamical update we design for the choice of length-scales. This separation yields a natural evolution, $\prob{}{  \map{t}|  r_t, r_{t-1}, \map{t-1}} $,  independent of lengthscales for the system, where the Markov transition probability $\prob{}{ \map{t}|  \map{t-1}}$ is assumed known. The second term, $\prob{}{ r_t|  r_{t-1}, \map{t-1}}$ represents our choice of model for dynamically updating length-scales as the environment evolves. The full derivation from \cref{eqn:slam:bayes:1} to \cref{eqn:slam:bayes:3} is given in \cref{sec:appendix1}.\\
  \\
  In the case that $\map{}$ represents a time invariant map in the time domain, we can assume that both $\map{t} = \map{} \forall t$, This logically suggests that $r$ used for the approximation of $\map{}$ should also be time invariant, $r_t = r \forall t$. Hence, going from $t-1$ to $t$ means that the integral over $\map{t-1}$, $r_{t-1}$ will be zero everywhere unless $\map{t}=\map{t-1}, r_t = r_{t-1}$. Under these assumptions for $\map{}, r$ in the time domain, we reduce \cref{eqn:slam:bayes:3} a Bayesian inference problem which depends only on the transition probability distribution of the sensing qubit and it directly analogous to the central problem outlined in \cite{thrun2001probabilistic}.\\
  \\
  The full inference problem in \cref{eqn:slam:bayes:3} is generally intractable due to the high dimensionality over all the maps, $\map{t}$, and associated length-scales. We follow a modified version of the iterative maximum likelihood approach in \cite{thrun2001probabilistic}. This means that we substitute $\hat{M}_t^{(k)}$ into the Bayesian inference equation as discussed in detail in the next section. 
  
\section{Iterative Predictive Estimation for SLAM via Born's Rule}  \label{sec:iterativebayes}
While the full classical SLAM problem may be intractable, we wish to incorporate our \textit{a priori} knowledge of the data association function granted via Born's rule. Substitution of the data association function,  $\hat{M}_t^{(k)}$, forces us to consider a recursive inference procedure where the likelihood calculation and state update is iterative for each new measurement (map update) and its associated set of quasi-measurements (length scale update). The rationale for the design and ordering of these iterative calculations are discussed in this section, and the iterative procedure for maximising the likelihood is inspired from \cite{thrun2001probabilistic}.\\
\\
As assumed in \cite{thrun2001probabilistic}, a data association $\hat{M}_t^{(k)}$ function exists to update the values of the environmental map if all other state variables are assumed known. The departure of our approach from \cite{thrun2001probabilistic} is that this data association function is prescribed by Born's rule to extend classical techniques for qubit control. In particular, at each time step $t$,  we perform a likelihood calculation and state update for a physical measurement on a sensing qubit, and use this information to do a likelihood calculation over quasi-measurements generated over a neighbourhood. The first procedure enables a map update due to physical measurements, and the second procedure attempts to discover the physical length-scales over which significant variation of dephasing field is seen.  An iterative maximum likelihood procedure then forces us to change the structure of weight calculation and resampling for a typical particle filtering algorithm, and these changes are specified in the last section.
  
For a measurement at $t$ on the $j_t$ qubit, we  substitute  $ \map{t} \to \hat{M}_t \equiv \hat{M}_t^{(k)}$,  $ \forall k \in \{j_t, \qset{j_t}{t}\}$ into the Bayesian recursive equation. This substitution is allowed as $f_t \equiv M_t$ under Born's rule for the ideal case when our inference procedure allows us to perfectly learn the dephasing field from (infinite) data. Given $x_{t-1}, \pose{t}, \rstate{t}$, the function $\hat{M}_t$ is a deterministic update of the dephasing map based on physical and quasi-measurement data. This means that:
\begin{align}
	\prob{}{ \hat{M}_t| \hat{M}_{t-1}}  \to \delta (M_t - \hat{M}_t| \hat{M}_{t-1}),  \label{eqn:slam:ilm:1}
\end{align} and the integral over the space of maps disappears. Since $\hat{M}_t$ depends only on the previous state, the Markov assumptions of our model are preserved. The substitution of the data association function gives the Bayesian recursion as:
\begin{widetext}
\begin{align}
\prob{}{&\pose{t}, \rstate{t}, \hat{M}_{t}| \mathcal{D}_{t-1}} \nonumber \\
& =  \eta_t\prob{}{z_t | \pose{t}, \rstate{t}, \hat{M}_t| \hat{M}_{t-1}, u_{t-1}}  \int \prob{}{ \pose{t}| \pose{t-1}, u_{t-1}} \int \prob{}{ \rstate{t}|  \rstate{t-1}, \hat{M}_t| \hat{M}_{t-1}} \prob{}{\pose{t-1}, \rstate{t-1}, \hat{M}_{t-1}| \mathcal{D}_{t-1}}  d\pose{t-1} d\rstate{t-1} \label{eqn:slam:ilm:2}
\end{align}
\end{widetext} 
 \subsection{Dynamical Evolution}
 We assume a simple dynamical evolution for the state vector. The transition distribution for length-scales is just the error in modeling a continuously varying field using sigmoids. We assert the distribution of true errors is white, Gaussian with a non-zero mean and variance:
 \begin{align}
 \prob{}{ r_t^{(k)} |  r_{t-1}^{(k)} , \hat{M}_{t-1}}  & \propto    \prob{}{ \hat{M}_{t} |  \hat{M}_{t-1}}  \forall k \in N \label{eqn:slam:dynamics:r:1} \\
 & \propto \prob{}{ f_t - \hat{M}_{t} |  \hat{M}_{t-1}} \label{eqn:slam:dynamics:r:2}\\
 & = \mathcal{N}(\mu_r, \sigma^2_r),\forall k \in N \label{eqn:slam:dynamics:r:3}
 \end{align}\\
 Similarly, the position of the sensing qubit is given by \cref{eqn:physetting:1}. The position state vector deviates from the ideal control only due to the white, Gaussian zero mean jitter arising from imperfect knowledge of the qubit position relative to the field. The model in \cref{eqn:physetting:1}  implies:
 \begin{align}
 \prob{}{ \pose{t}| \pose{t-1}, u_{t-1}} = \prob{}{ w_t} = \mathcal{N}(\mu_w, \sigma^2_w) \forall t \label{eqn:slam:dynamics:process}
 \end{align} 
 This gives the transition probability distribution for the state vector as:
 \begin{align}
 \prob{}{x_{t}| x_{t-1}, u_{t-1}} = \mathcal{N}(\mu_r + \mu_w, \sigma^2_w + \sigma^2_r) \label{eqn:slam:dynamics:full}
 \end{align}

 For $t=0$, the true priors are the $\prob{}{x_{0}}$ as:
  \begin{align}
  	\prob{}{\pose{0}} & \sim \mathcal{N}(\mu_w, \sigma^2_w) \\
  	\prob{}{ f_{0}} & \sim \mathcal{U}(0, \pi) \label{eqn:prior}\\
  	\prob{}{ \rstate{0}} & \sim \mathcal{U}(0, \infty) 
 \end{align}
 In practice, we approximate $\prob{}{ \rstate{0}}$ using physical arguments about spatial resolution granted by the hardware, namely $\prob{}{ \rstate{0}} \leftarrow \mathcal{U}(L_0, L_t)$, where this approximation will be discussed in \cref{sec:particlefilter}.
 
 \textit{TO DO: Dynamical evolution section is a placeholder. To simplify dynamical evolution, we set the transition probability distribution parameters close to zero, such that dynamics are turned off i.e. in code, particles are propagated with identity dynamics. Tuning of the noise parameters here may result in additional noise filtering for experimental data. } \\
 
 
\subsection{Iterative Likelihood Calculations} 
The iterative calculation proceeds as follows. The first part of the calculation is computing a likelihood with respect to the physical measurement only. Upon receiving a physical measurement, $\pmsmt{j_t}{t}$, at $t$, the likelihood computation is given by:
\begin{align}
\prob{}{\pmsmt{j_t}{t}&=d | \pose{t}, \rstate{t}, \hat{M}_t^{(j_t)}| \hat{M}_{t-1}^{(j_t)}, u_{t-1}} \nonumber \\
 = & \frac{\rho_0}{2} \nonumber \\
& + \rho_0 \left(  2\probest{}{\{1\} | \pose{t}, \rstate{t}, x_{t-1}, u_{t-1}, \mathcal{D}_{t-1}} - 1 \right)  \delta(d-1) \nonumber \\
& - \rho_0 \left(  2\probest{}{\{1\} | \pose{t}, \rstate{t}, x_{t-1}, u_{t-1}, \mathcal{D}_{t-1}} - 1 \right) \delta(d), \label{eqn:slam:ilm:3} \\
\rho_0 :=& \erf\left( \frac{2b}{\sqrt{2R}}\right) + \frac{\sqrt{2R}}{2b}\frac{\exp^{-\left( \frac{2b}{\sqrt{2R}}\right)^2}}{\sqrt{\pi}} - \frac{1}{\sqrt{\pi}} \frac{\sqrt{2R}}{2b}, \label{eqn:slam:ilm:4} \\
&d \in {0,1}, \quad b \equiv 1/2.  \label{eqn:slam:ilm:5}
\end{align} The form of the likelihood  in \cref{eqn:slam:ilm:3} is given in \cite{riddhinotes} and essentially represents the distribution for a coin-toss experiment representative of projective measurement outcome on a qubit, when our knowledge of the bias on the coin is uncertain. In certain regimes, this distribution approaches that of a truncated Gaussian distribution - namely, that measurement errors for our application can at most be resolved for one bit flip. This likelihood enables us to score or weight our hypothesis, $\hat{M}_t^{(j_t)}| \hat{M}_{t-1}^{(j_t)}$ for the true dephasing map, $f_t$, based on physical data. \\
\\
The second part of the likelihood calculation proceeds by scoring or weighting hypotheses about the length-scales, $\rstate{t}$, based on quasi-measurements generated over a neighbourhood. For a physical measurement, $\pmsmt{j_t}{t}$, at $t$, we consider all $q$ qubits in a given hypothesis about the true neighbourhood, $\qset{j_t}{t}$, for all $q \in \qset{j_t}{t}$. For each quasi-measurement on the $q$ node, we compute:
\begin{align}
\prob{}{ \qmsmt{q}{t}&|\pose{t}, \rstate{t}, \hat{M}_t^{(j_t)}| \hat{M}_{t-1}^{(j_t)}, u_{t-1 }} \nonumber \\
& = \frac{1}{\sqrt{2\pi \Sigma}} \exp \left( -\frac{( \qbornm{q}{(j_tq)}{t} - \mest{q}{t}|\mest{q}{t-1} - \mu_f )^2}{2 \Sigma }\right) \label{eqn:slam:ilm:6:quasi}  \\
\mathrm{with } & \lim_{\tau_t^{(q)} \to \infty} \qbornm{q}{(j_tq)}{t} - \mest{q}{t}|\mest{q}{t-1}   = \mu_f. \label{eqn:slam:ilm:7}
\end{align} The term $\mest{q}{t}|\mest{q}{t-1} $ merely denotes the most recent map estimate at the neighbour $q$. The map estimate at $q$ is compared with the predicted map information from the smearing action, $\qbornm{q}{(j_tq)}{t}$. The difference between these two terms is generally large if the length scale of the neighbour is incorrect  (noisy $ \qbornm{q}{(j_tq)}{t}$) or if the accumulation of real and quasi measurements at $q$ are insufficient to yield a reasonable estimate of the map at $t-1$  (noisy $ \mest{q}{t}|\mest{q}{t-1} $).  As the number of physical measurements accumulate on neighbouring qubits, the distribution in \cref{eqn:slam:ilm:6:quasi} approaches a Gaussian  distribution with a true mean $\mu_f$ mean a covariance matrix $\Sigma$ that reflects the distribution of true errors from using a sigmoid approximation in modeling a continuously varying dephasing field.\\
\\ The likelihoods over physical and quasi measurements at a single $t$ can be multiplied to yield the overall likelihood:
\begin{align}
	\prob{}{ z_t& | \pose{t}, \rstate{t}, \hat{M}_t^{(j_t)}| \hat{M}_{t-1}^{(j_t)}, u_{t-1 } } \nonumber \\
	&= \prob{}{\pmsmt{j_t}{t}=d | \pose{t}, \rstate{t}, \hat{M}_t^{(j_t)}| \hat{M}_{t-1}^{(j_t)}, u_{t-1}} \quad  \times \nonumber \\
	& \prod_{\forall q \in \qset{j_t}{t}} \prob{}{ \qmsmt{q}{t}|\pose{t}, \rstate{t}, \hat{M}_t^{(j_t)}| \hat{M}_{t-1}^{(j_t)}, u_{t-1 }} \label{eqn:slam:ilm:8}
\end{align} where the product of all measurements reflects that we treat physical and quasi-measurements on  an equal footing. The product invokes the same assumptions as in the derivation of a recursive Bayesian update equation, namely, the independence of the true dephasing field, sensing measurements, and controls. In particular, the physical measurement has no impact on the true dephasing map and the dephasing map and the choice of sensing qubits are independent. These assumptions mean that the true error from a sigmoid approximation of the dephasing field are spatially uncorrelated. Further, the design of the quasi-measurement procedure ensures that in the limit where infinite data is collected on every node, the full inference procedure yields an unbiased estimator of the true dephasing field for an appropriate choice of $\lambda_1, \lambda_2$. \\
\\
The inter-leaving part of the calculation comes from the fact that $ \prob{}{\pmsmt{j_t}{t}=d | \pose{t}, \rstate{t}, \hat{M}_t^{(j_t)}| \hat{M}_{t-1}^{(j_t)}, u_{t-1}}$ must be used to update the map value $\hat{M}_t^{(j_t)}$ at $j_t$  before the smearing action in the neighbourhood is computed and scored via $\prob{}{ \qmsmt{q}{t}|\pose{t}, \rstate{t}, \hat{M}_t^{(j_t)}| \hat{M}_{t-1}^{(j_t)}, u_{t-1 }}$. In particular, $ \qbornm{q}{(j_tq)}{t}$ takes the updated $\mest{j_t}{t}$ as an input and smears this information over the neighbourhood of all $q$ qubits. The product of likelihoods for all $q$ qubits in the neighbourhood gives us the best estimator of the length-scales, namely, in the second step, we update $\rest{j_t}{t} | \rest{j_t}{t-1} $ as an estimator of the true length-scale $\rstate{t}$. \\

This iterative likelihood calculation and inter-leaved state update procedure leads us to consider a particle filter with two different sets of particles - denoted as an $\alpha$ set and a $\beta$ set of particles respectively. Each particle represents a hypothesis about a true state at a given node, and these hypotheses are weighted by likelihood functions for that node. Ideally, a higher weight / likelihood corresponds to a particle closer to the true state $x_t$ at $t$ given a physical measurement on the $j_t$ node and given all past data.  The aggregate result is that $\alpha$ particles combine information about the most likely dephasing field given physical measurements, and $\beta$ particles combine information about the most likely length-scales for approximating true dephasing, based on physical and quasi-measurements. In particular, $\alpha$ particles are weighted by the first likelihood, $\prob{}{\pmsmt{j_t}{t}=d | \pose{t}, \rstate{t}, \hat{M}_t^{(j_t)}| \hat{M}_{t-1}, u_{t-1}}$; whereas $\beta$ particles are weighted by $\prob{}{ \qmsmt{q}{t}|\pose{t}, \rstate{t}, \hat{M}_t^{(j_t)}| \hat{M}_{t-1}, u_{t-1 }}$. Hence, $\beta$ particles represent a distribution of $r_t$ and are constrained only if the associated node is measured physically. The details for ordinary particle filtering and details about the modifications for our iterative likelihood maximisation approach is the topic of the next section.

\section{Numerical Solutions via a Stratified Particle Filter} \label{sec:particlefilter}

A particle filter numerically approximates a posterior distribution using particles, where each particle is a hypothesis of a true state. As measurement information is received, the weight-calculation and resampling steps of a particle filter refer to \textit{a priori} design of how particles are weighted as being likely or not likely (weight calculation); and the way in which the diversity of the particles is maintained throughout the procedure so that state estimation remains robust and unbiased (resampling). Sophisticated weight calculation and re-sampling schemes exist in literature (e.g., see a review in \cite{li2015resampling}); however, a standard starting point is to use an importance sampling framework. In importance sampling, one chooses \textit{a priori} a so-called importance distribution, denoted $q(\cdot)$, and a set of weights, $W(\cdot)$ to approximate the joint posterior as:
\begin{align}
\prob{}{x_{0:t} , z_{0:t} |u_{0:t}} &:= W(x_{0:t} , z_{0:t}) q(x_{0:t} | z_{0:t}, u_{0:t}) \label{eqn:slam:PF:1}
\end{align} The choice of the importance distribution is such that it must cover the posterior. We assume that the important distribution can be written in Markov (recursive) form, with a transition probability, $q^*(x_{0:t} |x_{0:t-1})$. Under these assumptions, one obtains a recursive equation for the weights:
\begin{align}
& W(x_{0:t} , z_{0:t})  \nonumber \\
&= \frac{\prob{}{z_{t} | x_{t}, u_{t-1}}  \prob{}{x_{t}| x_{t-1}, u_{t-1} }}{q^*(x_{0:t} |x_{0:t-1})} W(x_{0:t-1} , z_{0:t-1})  \label{eqn:slam:PF:2} \\
&q^*(x_{0:t} |x_{0:t-1}) := \frac{q(x_{0:t} | z_{0:t}, u_{0:t})}{q(x_{0:t-1} | z_{0:t-1}, u_{0:t-1})} \label{eqn:slam:PF:3}
\end{align} For any particular choice of $q^*(x_{0:t} |x_{0:t-1})$, the approximate representation of the posterior over $P$ samples  (particles) is given by a weighted collection of true states, where the weights for a given realisation of a true state represented by the $p$-th particle are given by  $W(x_{0:t} , z_{0:t})^{(p)}$ at $t$.  In the above, the weights are normalised over all $P$ particles at each $t$ such that $\sum_{p = 1}^{P} W(x_{0:t} , z_{0:t})^{(p)} = 1$. Under the importance sampling framework, for the choice of $q^*(x_{0:t} |x_{0:t-1}) \equiv \prob{}{x_{t}| x_{t-1}, u_{t-1}}$, the recursive weight update of \cref{eqn:slam:PF:1} depends only on the likelihood function. 
\\
\\
We incorporate a stratified approach using two particle sets, namely, the set of $\alpha \in \{1, \hdots ,P_\alpha\}$ particles, and for each $\alpha$ and a node $k$, a set of $\beta_\alpha \in \{1, \hdots ,P_\beta\}$ particles. Here, the distribution of $\{\alpha \}$  represents the distribution over the dephasing map over the entire grid, and the average uncertainty in the qubit node position relative to the grid after a control has been applied. For each $\alpha$, the distribution of $\{ \beta_\alpha \}^{(k)} $ is the best estimate of the length scales at node $k$. For a specific $j_t$, we access the $\beta_\alpha$ particle set, $\{ \beta_\alpha \}^{(k=j_t)} $, once for each hypothesis of the $\alpha$ particle state. \\
\\
Under the importance sampling framework, the weight update only depends on the likelihood. We perform the following normalised weight calculations over both particle sets in order of computation(tilde represents raw weights). First, we compute the $\alpha$ normalised particle weights based on the physical measurement only:
\begin{align}
&\tilde{W}_t^{( \alpha | j_t)} \nonumber \\
& = \prob{}{\pmsmt{j_t}{t}=d | \pose{t}^{(\alpha)}, \rstate{t}^{(\alpha)}, \hat{M}_t^{(j_t, \alpha)}| \hat{M}_{t-1}, u_{t-1 }} W_{t-1}^{( \alpha | j_{t-1})}  \label{eqn:slam:PF:4:alpha:raw}\\
&W_t^{( \alpha | j_t)} := \frac{\tilde{W}_t^{( \alpha | j_t)}}{\eta^{( \alpha | j_t)}}, \quad \eta^{( \alpha | j_t)} := \sum_{\alpha = 1}^{P_{\alpha}} \tilde{W}_t^{( \alpha | j_t)}  \label{eqn:slam:PF:5:alpha:norm}
\end{align} For the measurement at $j_t$, we need access to the second layer - namely the $\beta_\alpha$ set of particles for each $\alpha$.  Accessing this $\beta_\alpha$ particle set at any node $k$ means that the posterior length-scale from the previous time step, $\rest{k}{t-1}$, is used to generate a uniform distribution of  $P_\beta$ particles, over the interval $\mathcal{U}(L_0, L_t)$. Here, $\mathcal{U}(L_0, L_t)$ represents a `prior' for initiating $\beta$ particles, namely, $\mathcal{U}(L_0, L_t)$ acts like a prior over potential length-scales at each node, and for each iteration, $t$. Both $L_0$ and $L_t$ take into account the minimal and the maximal physical separation of any two qubits in the hardware. (We will provide an expression for $L_t$ in due course.) In the limit when the variance of posterior particle weights are high (low), the length-scale prior should enable the size of the neighbourhoods to collapse (expand) rapidly so that the smearing action does not introduce a systematic bias in the filtering procedure. This is enabled by collapsing and expanding the $\beta$ layer using a uniform prior for length-scales, $\mathcal{U}(L_0, L_t)$.\\
\\
Assuming that $\mathcal{U}(L_0, L_t)$ exists and covers the true (unknown) posterior for each iteration $t$, and the mechanism for marginalisation over the second layer enables adaptive inference, we can score each $\beta_\alpha$ particle by computing the likelihood for the smearing action over the neighbourhood:
\begin{align}
	&\mval{j_t, \alpha}{t}  \leftarrow \hat{M}_t^{(j_t, \alpha)}| \hat{M}_{t-1} \label{eqn:slam:PF:6:beta:mapupdate}\\ 
	&\tilde{W}_t^{( \beta_\alpha | j_t, \alpha)} \nonumber \\
	&= \prod_{\forall q \in \qset{j_t, \alpha, \beta_\alpha}{t}} \prob{}{ \qmsmt{q, \alpha, \beta_\alpha}{t}|\pose{t}^{(\alpha)}, \rstate{t}^{(\alpha, \beta_\alpha)}, \hat{M}_t^{(j_t, \alpha)}| \hat{M}_{t-1}, u_{t-1 }} \label{eqn:slam:PF:7:beta:raw} \\
	&W_t^{( \beta_\alpha | j_t, \alpha)}:= \frac{\tilde{W}_t^{( \beta_\alpha | j_t, \alpha)}}{\eta^{( \beta_\alpha | j_t, \alpha)}} \quad \eta^{( \beta_\alpha | j_t, \alpha)}:= \sum_{\beta_\alpha = 1}^{P_\beta}\tilde{W}_t^{( \beta_\alpha | j_t, \alpha)} \label{eqn:slam:PF:8:beta:norm}
\end{align}	Note that we do not maintain or store the second layer of weights from the previous measurement, namely, there is no record of $W_{t-1}^{( \beta_\alpha | j_{t-1}, \alpha)}$. This is because the location of the sensing qubit changes with $t$. Instead of retaining an arbitrarily long history of all possible $\beta$ particles, we expand and collapse according to $ \mathcal{U}(L_0,L_t)$.\\
\\
The combined likelihood over both $\alpha$ and $\beta$ particles for physical and quasi-measurements is simply the product of the two weights calculated before.  With the normalisation factors $\{ \eta^{(\cdot)} \}$ defined previously, the sum of all $W_t^{(\alpha, \beta_\alpha | j_t)}$  for all $P_\alpha \times P_\beta$ particles is normalised. (Alternatively, the normalisation steps can be removed and done once at the end over  $W_t^{(\alpha, \beta_\alpha | j_t)}$).
\begin{align}
W_t^{(\alpha, \beta_\alpha | j_t)} &:= W_t^{( \alpha | j_t)}  W_t^{( \beta_\alpha | j_t, \alpha)} \label{eqn:slam:PF:9:weights:raw} \\
\mathrm{with} \quad & \sum_{\alpha=1}^{P_\alpha} \sum_{\beta_\alpha=1}^{P_\beta}  W_t^{(\alpha, \beta_\alpha | j_t)} = 1. \label{eqn:slam:PF:10:weights:norm}
\end{align} The  weights, $W_t^{(\alpha, \beta_\alpha | j_t)}$, essentially score particles (hypotheses for the true state) for their net likelihood given physical and quasi-measurement information. The structure of the resulting particle filter is given in \cref{algorithm:q-slam-pf}.

\begin{algorithm}[H] % Floats not comptatible with revtek4-1. Need "[H] option. 
	\caption{QSLAM}\label{algorithm:q-slam-pf}
	\begin{algorithmic}[0] 
		\Procedure{qslam}{$ \mathrm{Qubit Grid}$}\Comment{Input $N$ nodes}
		\\
		\If{$ t = 0$}
		\Procedure{Initialize}{$\mu_{w,r}, \sigma^2_{w,r},  R, \Sigma, \gamma_T, P_{\alpha,\beta} $}
		\State Set $d_{\mathrm{grid}}$ \Comment{\cref{eqn:slam:PF:14}}
		\For{$ \alpha \in P_\alpha$}
		\State Sample $x_{0}^{(\alpha)} \sim \prob{}{x_{0}}$ \Comment{\cref{eqn:prior}}
		\State Compute $W_0^{(\alpha)} $ \Comment{\cref{eqn:slam:PF:5:alpha:norm} }
		\EndFor
		\EndProcedure \label{pseudoalgo:qslamr:initialise}
		\EndIf
		\\
		\While{$ 1 \leq t < T$}
		\If{Controller}
		\State Pick $j_t$ from control scheme $\hat{C}_t$ \Comment{\cref{eqn:slam:controller2}}
		\EndIf
		\State $\{j_t, \pmsmt{j_t}{t}\} \gets $\Call{ReceiveMsmt}{$d, u_t$}
		\State $\{x_{t}^{(\alpha)}\} \gets $ \Call{PropagateStates}{Posterior $\{x_{t-1}^{(\alpha)} \}$ }
		\State $\{\{x_t, W_t\}^{(\alpha, \beta_\alpha)}\} \gets $ \Call{ComputeWeights}{$\{x_{t}^{(\alpha)}\}$}
		
		\If{$P_{\mathrm{eff}, t} < \gamma_T$}
		\State $\{\{x_t, W_t\}^{(\alpha, \beta_\alpha)}\} \gets $ \Call{Resample}{${\{x_t, W_t\}^{(\alpha, \beta_\alpha)}}$}
		\EndIf
		\State $\{x_t^{(\alpha)}, W_t^{(\alpha)}\} \gets $ \Call{Collapse$\beta$}{$\{\{x_t, W_t\}^{(\alpha, \beta_\alpha)}\}$}
		\If{$P_{\mathrm{eff}, t} < \gamma_T$}
		\State $\{\{x_t, W_t\}^{(\alpha)}\} \gets $ \Call{Resample}{${\{x_t, W_t\}^{(\alpha)}}$}
		\EndIf
		\State Generate $\qmsmt{q}{t}$ \Comment{\cref{eqn:quantiser:2}}
		\EndWhile \label{pseudoalgo:qslamr:endwhile2}			
		\EndProcedure
	\end{algorithmic}	
\end{algorithm}

If weights are recursively updated for large measurement records, it is often the case that the particles become degenerate quickly. Particle degeneracy means that a few particles with large weights dominate the inference procedure. In these conditions, it is likely that the filter converges too quickly and will produce biased estimates of the true state. A practical technique to ensure diversity in the particle set is to `resample' - namely, we replace existing particles with a new set of particles and associated weights. This re-sampling step is indicated in \cref{algorithm:q-slam-pf}. The purpose of the re-sampling step is to select the most likely joint true state - namely position, dephasing map value and neighbourhood length-scale - at $j_t$ while maintaining a diversity of particles to track the dephasing field without becoming degenerate. \\
\\
We wish to maintain the diversity of particles in our stratified filter while converging into sub-spaces of the true state which are more likely given our measurement record. A review of procedures for grouping particles and re-sampling operations for unbiased state estimation is given in \cite{li2015resampling}. In this manuscript, we employ a simple procedure for resampling particles at every iteration. If the variance of the posterior weights, $\{W_t^{(\alpha, \beta_\alpha | j_t)}\}$ rises above some threshold, we know that the effective number of particles in the filter reduces (a greater density of particles means we expect less uncertainty in resolving a unknown posterior distribution). Hence, if the variance of the particles is very high, then we draw particles from a discrete posterior probability distribution represented by the final weights, $\{W_t^{(\alpha, \beta_\alpha | j_t)}\}$. This new set of particles replaces the original set  and all posterior weights are reset to a uniform value. 
\begin{algorithm}[H] % Floats not comptatible with revtek4-1. Need "[H] option. 
	\caption{Supporting Functions}\label{algorithm:q-slamR:supportf}
	\begin{algorithmic}[0] 
		\Function{ReceiveMsmt}{$d, u_t$}\Comment{ Receive $d \in \{0,1\}$} 
		\State $j_t \gets \poseest{t}$
		\State $\pmsmt{j_t}{t} \gets d$
		\EndFunction
		\\ \dotfill
		\Function{PropagateStates}{Posterior $\{x_{t-1}^{(\alpha)} \}$ }
		\For{$\alpha \in P_\alpha$}
		\State Sample $X \sim \prob{}{x_{t}| x_{t-1}, u_{t-1}}$ \Comment{\cref{eqn:slam:dynamics:full}}
		\State $x_{t}^{(\alpha)} \gets X$
		\EndFor
		\EndFunction
		\\ \dotfill
		\Function{ComputeWeights}{$\{x_{t}^{\alpha}\}$}
		\For{$\alpha \in P_\alpha$}
		\State Compute $\tilde{W}_t^{( \alpha | j_t)} $ \Comment{\cref{eqn:slam:PF:4:alpha:raw} }
		\State $\mest{j_t, \alpha}{t} \gets \hat{M}^{(j_t)}_t$ \Comment{\cref{eqn:physetting:mapdatassoc:2,eqn:physetting:forgettingfactor}}
		\State Generate $\{\beta_\alpha, x_t^{(\alpha, \beta_\alpha)} \} $ by $\rest{j_t}{t} \sim \mathcal{U}(L_0,L_t) $ 
		\For{$\beta_\alpha \in P_\beta$}
		\State Generate $\qmsmt{q \alpha, \beta_\alpha}{t}$ \Comment{\cref{eqn:quantiser:2,eqn:quantiser:3}}
		\State Compute $\tilde{W}_t^{( \beta_\alpha | j_t, \alpha)}$ \Comment{\cref{eqn:slam:PF:7:beta:raw}}
		\EndFor
		\State Normalise $\tilde{W}_t^{( \beta_\alpha | j_t, \alpha)}$ \Comment{\cref{eqn:slam:PF:8:beta:norm}}
		\EndFor
		\State Normalise $\tilde{W}_t^{( \alpha | j_t)}$ \Comment{\cref{eqn:slam:PF:5:alpha:norm}}
		\State Compute $W_t^{(\alpha, \beta_\alpha |j_t)}, \quad \forall \{\alpha, \{\beta_\alpha\} \}$\Comment{\cref{eqn:slam:PF:10:weights:norm}}
		\EndFunction 
		\\ \dotfill
		\Function{Resample}{${\{x_t, W_t\}^{(i)}}$}
		\For{$i$}
		\State Sample $X$  according to $\{W_t^{(i)}\}$
		\State $x_t^{(i)} \gets X$
		\EndFor
		\State Reset $\{W_t^{(i)}\}$ to a uniform distribution
		\EndFunction 
		\\ \dotfill
		\Function{Collapse$\beta$}{$\{\{x_t, W_t\}^{(\alpha, \beta_\alpha)}\}$}
		\State $\{\rest{j_t, \alpha}{t}\} \gets \ex{\rest{j_t, \alpha, \beta_\alpha}{t}}_{\beta_\alpha}$
		\State $\{\hat{c}_t^{(j_t, \alpha)}\} \gets \mathbb{V}[\rest{j_t, \alpha, \beta_\alpha}{t}]_{\beta_\alpha}$ \Comment{\cref{eqn:slam:controller}}
		\State $\{ W_t^{(\alpha)}\}  \gets \sum_{\beta_\alpha =1 }^{P_\beta} W_t^{(\alpha, \beta_\alpha)} $
		\EndFunction 
	\end{algorithmic}
\end{algorithm}


Quantitatively, let $P_{\mathrm{eff},t}$ denote the effective number of particles such that if the variance of the weights is high, then $P_{\mathrm{eff},t}$ approaches zero:
\begin{align}
& P_{\mathrm{eff},t} := \frac{1}{\sum_{\alpha=1}^{P_\alpha} \sum_{\beta_\alpha=1}^{P_\beta} \left( W_t^{(\alpha, \beta_\alpha | j_t)} \right)^2} \label{eqn:slam:PF:11} \\
&\textrm{Resample at $t$ if:  }  P_{\mathrm{eff},t} < \gamma_{T}  \label{eqn:slam:PF:12}
\end{align}Then we choose to resample only when $P_{\mathrm{eff},t}$ falls below some threshold $\gamma_{T}$ such that setting $\gamma_{T} \equiv P_\alpha \times P_\beta$ will result in resampling at every $t$ step \cite{li2015resampling}. We can now revisit the span of the uniform distribution for length-scales that is expanded and collapsed at each $t$, namely:
\begin{align}
L_t & \propto  d_{\mathrm{grid}} P_{\mathrm{eff}, t} \label{eqn:slam:PF:13}\\  
d_{\mathrm{grid}} &= \max v_{k k'}, \quad \forall k, k' \in N. \label{eqn:slam:PF:14}
\end{align}
This means $P_{\mathrm{eff}, t} $ should reduce the  scale of a distribution over length scales, $L_t$, if the uncertainty in our map estimates increases. Further, $L_t$ depends on the physical size of the grid, namely, the maximal physical separation of any two qubits given by $d_{\mathrm{grid}}$. \\
\\
In simulations (presented in \cref{sec:results}), we find that extremely moderate settings enables the algorithm to perform across a broad range of simulated parameter regimes and experimental data without any prior tuning. The settings for this analysis were very mild such resampling occurred at every iteration $(\gamma_T \gg P_\alpha \times P_\beta )$ and we set $L_t = 4 d_{\mathrm{grid}}, \quad \forall t$. \\
\\
Having completed a resampling procedure according to the posterior weights, $ W_t^{(\alpha, \beta_\alpha | j_t)}$, we wish to return to a single layer of the particle filter - namely, a single collection of particles that summarise our knowledge of the extended state vector. To do this, we marginalise over all the $\beta$ particles, resulting in  $W_t^{(\alpha | j_t)} $, consistent with \cref{eqn:slam:PF:9:weights:raw}. The second re-sampling procedure is conducted according to the posterior $\alpha$ particle weights only, $ W_t^{(\alpha| j_t)}$ and this prepares us for receiving the next set of data for another iteration.  \\
\\
Data generation for the next iteration will require two key steps: firstly, we must choose where to measure next. Secondly, we need to generate quasi-measurement data based on the posterior state vector from the particle filter. For the first step, we wish to choose measurement locations on hardware such that the overall map  is constructed efficiently. This is the subject of the next section, i.e. designing an automated controller. For the second step, the quasi-measurements for the $t+1$ iteration simply generated using the posterior map at $j(t)$, for neighbourhood size, and correlation length set parameterised by $\rest{j_t}{t}$. \\
\\
A summary of the key concepts discussed in this section, and all support functions for implementing propagation, weight calculation, and re-sampling  in \cref{algorithm:q-slam-pf} are collectively given in \cref{algorithm:q-slamR:supportf}. \\
\\


\section{Control action} \label{sec:control}

\textit{Placeholder section: TODO: the controller makes a key assumption that state estimate uncertainty distributions contract at a single node as learning progresses. This is not seen empirically for lengthscales distributions so effectively, the controller is sampling uniformly randomly on the grid. } \\
\\
In absence of external reference or control trajectories, the simplest controllers can be designed by tracking and using information from the state estimation procedure itself. For example, in classical recursive inference and machine learning literature, control criterion can be designed on the basis of the rate of change of the state estimates or the estimate variance of the state estimates [REFS]. In our application, the iterative Bayesian solution fixes the map update and attempts to discover length-scales relevant to the system. We link the controller to the level of uncertainty in the distribution of the \textit{posterior} length-scales for a particular node, in a given iteration. Namely, we extract the quantity $\hat{c}_t^{(j_t)}$:

\begin{align}
 \hat{c}_t^{(j_t, \alpha)} & = \mathbb{V}[\rest{j_t, \alpha, \beta_\alpha}{t}]_{\beta_\alpha}	\label{eqn:slam:controller} \\
  \hat{c}_t^{(j_t)} & = \ex{\mathbb{V}[\rest{j_t, \alpha, \beta_\alpha}{t}]_{\beta_\alpha}}_\alpha	\label{eqn:slam:controller2} \\
  \hat{c}_0^{(j_0)} & := 10^7, j_0 \in [1, 2, \hdots, N]
\end{align}

At $t$, we add $\hat{c}_t^{(j_t)}$ to a list of length-scale uncertainties indexed by spatial location. This list is updated at each iteration, and it is denoted as the control list,  $\hat{C}_t$. For example, we have measured qubits at location $n=2$  and $n=3$ until some time, $t-1$.  Suppose we receive a measurement at the node $n=1$ at $t$, and another measurement at $n=2$ at $t+1$. Then, $\hat{C}_{t-1}:= \{\hat{c}_{t-1}^{(2)}, \hat{c}_{t-1}^{(3)}\}$ is updated for a new location $\hat{C}_{t}:= \{\hat{c}_{t}^{(1)}, \hat{c}_{t}^{(2)}, \hat{c}_{t}^{(3)}\}$, where $\hat{c}_{t}^{(2,3)}$ are carried over from the past. Similarly, $\hat{C}_{t+1}:= \{\hat{c}_{t+1}^{(1)}, \hat{c}_{t+1}^{(2)}, \hat{c}_{t+1}^{(3)}\}$ where $\hat{c}_{t+1}^{(2)}$ is new and all other variables are carried over from the past. \\
\\
To make the control decision, we sort the control list,  $\hat{C}_t$, and pick the location, $n$, for which $\hat{c}_{t}^{(n)}$ is maximal. This means that the controller directs physical measurements to the regions where uncertainty in the state-estimates is highest. For multiple nodes with equally high variance, we sample uniformly randomly among the locations. Of course, alternative specifications of the controller can be considered, for example, based on a risk calculation, rate of change of state updates, inverse scaling with residual error and so on [REFS].\\
\\
\section{Bayes Risk} \label{sec:risk}

\textit{Placeholder section: not written up properly} \\

Each risk metric is the expectation of an error metric taken over many repetitions of simulated datasets. We use two error metrics:  an MSE error metric and a SSIM error metric. For engineered true dephasing maps, the MSE error metric can be expressed in the notation on an L2 norm:
\begin{align}
	MSE = \frac{L_{2}(f - \hat{f})}{L_{2}(f)}
\end{align}
The SSIM error metric compares the structural similarity between images and is used to obtain the universal index for comparing the quality of any two images in recent machine learning literature \cite{wang2004image}. To follow the notation of the original paper, let $x$ and $y$ be two images (or maps) in vectorised form. Then for a comparison of the elements of $x$ and $y$ in a small global region (or multiple local regions) yields:
\begin{align}
	SSIM(y,x) = \frac{(2 \mu_x \mu_y + C_1)(2 \sigma_{xy} + C_2)}{(\mu_x^2  + \mu_y^2 + C_1)(\sigma_{x}^2 + \sigma_{y}^2 + C_2)}
\end{align} In the notation above $\mu_{x,y}$ refers to the mean of an input signal, $\sigma_{x,y}$ is the unbiased sample standard deviation of an input signal, and $\sigma_{xy}$ represents the unbiased sample cross correlation between the two signals. We set $C_1 = C_2 = 0.01$ to stablise the index for data with means close to zero, and the choice of these values make the SSIM score comparable to the universal quality factors, as discussed in detail in \cite{wang2004image}. For our application with small qubit grids, we use the SSIM score globally on the posterior dephasing field, $\hat{f}_t$, (a column vector) in a dataset, setting $x := f_t $ and $y := \hat{f}_t$, with the notation emphasising the order of inputs into the SSIM score as the metric is not a symmetric distance type measure. \\


\section{Numerical Results} \label{sec:results}

\iffalse
[clarify approach]\\
\\
We can consider the performance of qslam for arbitrary spatial arrangements in 2D. A key aspect of performance is whether the physical setting and measurement procedure enables a resolution that allows the qslam algorithm to perform meaningful inference. To investigate spatial resolution, we fix the geometry of the spatial arrangement of qubits and we assume true fields are time-invariant. We vary spatial domains of field values over discrete (suddenly discontinuous) domains. We then explore the performance of the qslam algorithm as a function of the height of the field's discontinuous change (phase resolution); size of field domains relative to total measurement information  (spatial resolution); quality of local single qubit measurements, and the quasi-measurement forgetting factors $\lambda_1, \lambda_2$ i.e. where a value of unity for both factors bring the qslam algorithm to essentially mimic brute force measurements. [ADD TO THEORY, CHECK]. \\
\\
In \cref{figs_zsl_2}, we consider a simple geometry of a linear array of 25 equi-distant qubits, where the true field values are depicted to have two discontinuously split regions. In columns [i]-[iii], the true field is split into a small region (covering 20\% of qubits) of low dephasing (blue) and a large region (covering 80\% of qubits) of high dephasing (green). The low and high dephasing regions are flipped in the last column [iv]. We compare naive approaches with qslam via the Bayes Risk metric discussed in the previous section, where the top row of data panels represents a RMS error loss function and the bottom row represents the SSIM. The orange data in (a)-(d) and the red data in (e)-(h) is generated via a naive brute force approach of measuring qubit phase via single shot Ramsey experiments. The blue data and the indigo data in (a)-(d) and (e)-(h) respectively is generated via the qslam algorithm. The $y$ axis represents the Bayes Risk normalised such that a value of zero represents perfect learning, and unity represents maximal possible error in field map reconstructions. The $x$ axis presents a parameter regime where zero represents inference virtually ignores physical measurements (quasi-measurements dominate) and unity represents  that qslam maximally mimics the brute force measurement procedure (quasi-measurements are virtually ignored). The details of the parameter regimes are provided in Appendix [CODE CHECK] and we note that risk values for the naive approach should be independent of the $x$ axis in all cases (flat-lines).  \\
\\
We increase the number of iterations from column [i] to [iv] in \cref{figs_zsl_2}. In this case, each iteration correspond to one single qubit measurement per iteration. The total number of measurement budget is chosen so that every qubit is measured twice in the naive approach in column [ii] and 10 times in column [iii] and [iv]. In qslam, the location of the single qubit measurement at each iteration is given by the controller.  The RMSE based risk metric in (a) - (d) confirms the general trend that as the number of measurements increase, both algorithms show a reduced error score but that performance of qslam does not depend on the choice of the regimes for $\lambda_1, \lambda_2$. In contrast, the SSIM based risk metric shows as the total amount measurement information increases, qslam performance shows a clear dependence on the choice of the regimes for $\lambda_1, \lambda_2$. Further, for the parameter regimes chosen, the SSIM risk is minimised at a value that is not unity. This dependence of performance on $\lambda_{1,2}$ does not depend on the values of the true field if field domain structures are preserved, for example, by flipping the field from column [iii] to [iv]. For each of the shaded configurations, we show a single map reconstruction from naive and qlam on the top of each data column.\\
\\
FIG: remove true field  / number of measurement arrows in \cref{figs_zsl_2}. 
\begin{figure}[H]
	\includegraphics[scale=0.8]{figs_zsl_2}. 
	\caption{\label{figs_zsl_2} Caption here}    	
\end{figure}
Corresponding to the shaded configuration of $\lambda_{1,2}$ in the previous results, we now fix the forgetting factors and the true field configuration, and we change the quality of local measurements from [i] to [iv] in \cref{figs_zsl_1}. Here, each single qubit measurement is repeated at the same location a number of times before the next iteration is commenced. The total measurement budget for qslam and naive approach is conserved for each experiment and simply increases the overall number of measurements in the brute force approach. However for qslam, the increase in the number of qubit measurements per qubit corresponds to increased quality of the local qubit phase estimate before the local state estimate is shared with neighbouring qubits via a quasi-measurement calculation. In panels (a)-(d), the RMS risk metric for both algorithms is plotted against the number of iterations, where measurements per qubit per iteration increase from 1, 2, 8, 50 from left to right. qslam incurs lower RMS risk than naive for  small and medium values of total iterations, and converges in performance with naive approach in the high information regimes.The performance for the naive approach rapidly improves at iteration = 25, where each location in the linear array has been measured at least once, and this drop in the RMS risk metric becomes more pronounced as the quality of local information improves.  The SSIM risk metric mirror these general trends, but depicts a larger performance gap between the the two algorithms.\\
\\
In panels (i)- (l), we plot the amount of information required by qslam to achieve the same SSIM score under a brute force naive approach. Here,  the SSIM scores of (e) - (h) are plotted along the $x$-axis and the $y$-axis quantifies information given to qslam as a \% of the total measurement budget for the naive approach plotted on a log-scale. In all panels, for almost all values of acceptable SSIM scores between $0-0.5$, the information required by qslam is less than naive ($< 100\%$). To achieve SSIM risk arbitrarily close to zero, all information curves rise  corresponding to the high information regime (right extremal data) in panels (e)-(h). The relative information gain is minimised for an SSIM score of $0.1-0.2$ for the parameter regimes show, and rises again for higher scores corresponding to fewer iterations of the qslam algorithm in the low information regime (left extremal data) in panels (e)-(h).\\
\\
In \cref{figs_zsl_1} (m), we test whether the SSIM risk scores for qslam vary if the correlation lengths of the true field change from columns [v] - [viii], where a true field, and a single map reconstruction from qslam and naive are shown for 125 iterations, with a single measurement per iteration. The field split ratios determine the ratio of a low dephasing true field region over the linear array, and this is plotted on the $x$-axis of the two data panels. The $y$-axis depicts the SSIM risk metric on a log-scale, and each data line corresponds to a different number of total iterations. The left (right) plot shows data from qslam (naive ) in different shades of blue (orange). For all algorithms, all lines are relatively flat showing that the variation in SSIM risk metric is low even as the true field changes from left to right. All data lines drop vertically as the number of iterations (information) increase, confirming the intuition that performance improves as more data is made available for inference. However, data markers for qslam are  are substantially lower than the corresponding markers for the naive approach and this shows qslam outperforms naive for all parameter regimes depicted in panel (m).\\
\\
FIG/CODE CHECK: remove anything about linearisation!. 
CODE CHECK: msmt budget is compared properly between qslam and naive as msmts / qubit are increased.
FIG/CODE CHECK: \cref{figs_zsl_1} (m) - make colours same in both plots. or find another way to show this information. \\
\\
\begin{figure}
	\includegraphics[scale=0.8]{figs_zsl_1}. 
	\caption{\label{figs_zsl_1} Caption here}    	
\end{figure}
Preserving the geometry of the physical setting, we introduce measurement error in \cref{fig_nsl_v1}. We explore two different noise classes: salt and pepper noise and dark noise. Physically, salt and pepper noise corresponds to introducing classification error, where randomly chosen measurements are set to extremal values zero or one with probability half. Dark noise represents randomly chosen measurements set to value zero. The noise strength parameter represents the probability that measurements are corrupted by noise - where zero represents the noiseless case (reported previously)  and one represents maximal noise. For example, under dark noise with strength parameter unity, all measurements fed to an algorithm are zero. In \cref{fig_nsl_v1}, we increase the number of iterations from [i]-[iv] fixing one measurement per qubit per iteration. The $y$-axis depicts risk metrics for all data plots and the $x$-axis varies the noise strength parameter for S\&P noise in (a)-(h) and Dark noise in (j)-(q). In virtually all cases, qslam outperforms naive approaches in low and moderate noise regimes, and both algorithms converge the maximal risk value for extremely high noise regimes. The SSIM risk metric breaks down in (q) for Dark noise at unity strength parameter since the SSIM score is not stable when the means of the data are close to zero (in our case, data will be exactly zero). We depict single map reconstructions from the shaded regimes corresponding to XX\% noise level below each data panel, where the naive approach essentially summarises the measurement outcome without any measurement noise filtering. The weight factor for qslam is set to the value of, as before, in \cref{figs_zsl_2}. No noise filtering parameters were tuned in qslam, and the smoothening effect is due to the quasi-measurement mechanism alone. In panels (s)-(v), we consider S\&P noise with improved local quality of measurements by using eight measurements per qubit before sharing globally. With improved local in formation the performance gap between naive and qslam closes rapidly as the number of iterations increase.\\
\\
FIG : delete lengthscale distributions 
NB: can't add a noise corrupt input image unless you take the same positions on the map and wipe them out for 1 mst per qubit.\\
\\
\begin{figure}
	\includegraphics[scale=0.8]{fig_nsl_v1}. 
	\caption{\label{fig_nsl_v1} Caption here}    	
\end{figure}
We conclude the discussion of the linear 1D array of qubits by looking that many runs of the same experiment and plotting the posterior length scale and posterior map value at a single qubit at the boundary between two domains in \cref{fig_correlation_v0_summary}. Each row corressponds to an increase in the number of total iterations, where one measurement occurs per qubit per iteration. A single qslam run is repeated 150 times and the posterior correlation length and map value at qubit 11 (as shown) is a single data point in the left and right columns respectively. As the number of iterations increase, the distribution of posterior map value over many independent runs converges to the true map value (dotted line). In contrast, the distribution of posterior correlation lengths broadens  and appears localised only in the small information regime.\\
\\
FIG: add the position of qubit 11. See if there is a similar effect for increase in noise levels and plot that too. \\
\\
\begin{figure}
	\includegraphics[scale=0.8]{fig_correlation_v0_summary}. 
	\caption{\label{fig_correlation_v0_summary} Caption here}    	
\end{figure}
We extend the qslam analysis to consider a grid arrangement in 2D, with a true field that varies discontinuously between three domains as shown in \cref{figs_zqg_3}. We increase the number of iterations from columns [i]-[iii], and in high information regimes, the SSIM-risk metric is minimised for a weighting ratio that is less than unity. Shaded configurations of the weighting ratios in (a)-(h) correspond to the single run maps plotted in panel (i). In the bottom half of the figure, we increase the field difference at the boundaries of the true field from columns [v]-[vii], where the corresponding risk metrics plotted as a function of the weighting ratio. Column [vii] repeats the results presented in the first half of the figure to link the comparison to earlier results. For an extreme difference between the low and high field domains, as in [viii] qslam breaks at the brute force algorithm outperforms qslam for any choice of $\lambda$ weighting ratio. \\
\\
FIG : remove labels and arrows. Don't plot single run maps from keeping the forgetting factor constant. Pick panel (h) and plot maps for different weighting ratios in panel (h) in (i). OR only retain [v]-[viii] and cover the rest in the next figure. 
QUESTION: for moderate information regimes, if the risk metric is minimised at unity, shouldn't the value of the SSIM-risk be the same as naive approach? namely, explain (f) and (g). In what ways is the forgetting factor at unity not the same for naive approach and qslam? \\
\\
\begin{figure}
	\includegraphics[scale=0.8]{figs_zqg_3}. 
	\caption{\label{figs_zqg_3} Caption here}    	
\end{figure} 
As in the linear regime, we depict performance as total information for both approaches is increased - first, by increasing the number of iterations and second, by improving the local quality of measurement in \cref{figs_zqg_1b}. In (a)-(d), we depict increasing quality of local measurements from left to right. The risk metrics are plotted as a function of the total number of iterations. In high information regimes, both algorithms converge in performance. In low to moderate information regimes, qslam outperforms the naive approach by a considerable margin according to both RMS and SSIM based risk metrics. We take a selection of these parameter regimes (shaded green) and visually inspect single map reconstructions from qslam and naive in (i). From columns [i]-[ii], we increase the total number of iterations, which increases total information and spatial sampling of the 2D grid. From [ii]-[iv], we increase the total information budget by increasing the number of measurements per qubit per iteration before local state estimates are shared globally in qslam. 
\begin{figure}
	\includegraphics[scale=0.8]{figs_zqg_1b}. 
	\caption{\label{figs_zqg_1b} Caption here}    	
\end{figure}

\clearpage
NB: There is no need for \cref{figs_zqg_2}
\begin{figure}
	\includegraphics[scale=0.8]{figs_zqg_2}. 
	\caption{\label{figs_zqg_2} Caption here}    	
\end{figure}




\fi
\section{Discussion} \label{sec:discussion}
\section{Conclusion} \label{sec:conclusion}
%The recursive calculation of the posterior, as well as the normalisation process, is given in the pseudo-code of a typical particle filtering algorithm (see, for example, \cite{candy2016bayesian}). In a typical recursive calculation, each particle represents a hypothesis about the true state, and is denoted as $x_t^{(\alpha)}$. These particles are propagated using a known or chosen  $\prob{}{x_{t}| x_{t-1}, u_{t-1} }$. Given propagated states, one computes the weights of particles. 







\iffalse
	\begin{algorithmic}[0]
		
		\Procedure{Euclid}{$a,b$}\Comment{The g.c.d. of a and b}
		\State $r\gets a\bmod b$
		\While{$r\not=0$}\Comment{We have the answer if r is 0}
		\State $a\gets b$
		\State $b\gets r$
		\State $r\gets a\bmod b$
		\EndWhile\label{euclidendwhile}
		\State \textbf{return} $b$\Comment{The gcd is b}
		\EndProcedure
		
	\end{algorithmic}
	
\fi