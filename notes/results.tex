\section{Results \label{sec:results}}

We demonstrate the favorable performance of QSLAM relative to a brute force approach in a range of simulations using both engineered measurements and  experimentally measured data. For simulations with engineered true fields, we use both 1D and 2D spatial arrangements of qubits where the true field changes slowly or rapidly with respect to the spatial arrangement. At the end of this section, we report results using Ramsey measurements of a stray magnetic field gradient across a linear string of six trapped ions in a Paul trap. While real experimental data is used, we `simulate' physical hardware which can perform single qubit measurements at arbitrary locations in real time by releasing a randomly chosen measurement outcome from the entire record to QSLAM at the location chosen by the controller.

The key comparison being made across all the simulations is that for any given choice of acceptable error, we compare the number of measurements incurred by QSLAM relative to the number of measurements incurred by a naive brute force approach. The error metric is a Structural Similarity Index Metric (SSIM) frequently used to compare images in machine learning analysis and is robust to large single-pixel errors that frequently plague the mean-square error metric for vectorised images \cite{wang2004image,chen2009similarity,wang2009mean} The SSIM score compares the structural similarity between the image of the true field,$F_t$, and the posterior estimate of $F_t$ yielded by an algorithm. A value of $0$ indicates two images are the same with respect to the score, and a value of unity indicates that the two images are maximally dissimilar.  The expectation value of the SSIM score over 50 trials is reported as a `Norm SSIM' score in the subsequent figures, where the word `Norm' is indicative that the score can only take a value between 0 and 1.  

For engineered measurements, we fix the spatial arrangement to consist of $d=25$ qubits, where the line of qubits in 1D becomes a $5\times 5$ grid in 2D. The total measurement budget across both QSLAM and naive approaches are fixed at $N = 5, 10, 15, 20, 25, 50, 75, 100, 125, 250$. The choice of the budget $N\geq25$ means that every qubit is measured $1, 2, 3, 4, 5, 10$ times in the naive approach, and uniformly randomly sampled for $N< 25$. In QSLAM, the controller chooses a measurement location in each iteration. Hence, `measurements' and `iterations' are synonymous for the demonstrations of QSLAM in this paper, although complex demonstrations may involve cases where many measurements may be specified by the controller during each iteration. Both QSLAM and Naive approaches terminate when the total measurement budget is exhausted. 	An SSIM score is then calculated using the estimated map for each algorithm, and this procedure constitutes as a single trial in the SSIM calculation.

An example of a 1D case with engineered measurements  is shown in \cref{SSIM0_collated}. Panel (a) left inset shows the true engineered field, $F_t$ over 25 qubits that any algorithm must re-construct. The right inset shows the expected SSIM score  for each algorithm as a function of the total measurement budget. The ratio of the results for QSLAM (crimson circles) to the Naive approach (black crosses) is inverted so that one can read off the reduction in total measurement budget as a function of acceptable error in the main figure of panel (a). We note that for arbitrarily small SSIM scores (here, at $(0.05, 1)$), the ratio approaches unity. The numerical inversion for this example is unstable for scores $< 0.05$, and these smaller scores to increasingly large measurement budgets projected in the right-inset. For all the crimson curves (inset and main figure), we numerically optimise the value of ($\lambda_0, \lambda_1$) and test this against the hypothesis that sharing is trivial (i.e. $\lambda_0, \lambda_1 \equiv 0$ ). For all choices of measurement budgets, we find optimal $\lambda_0, \lambda_1 \neq 0$ and the resulting curves are plotted as solid lines  in (a) and in the right inset. Since it may not be practical to tune QSLAM for every choice of measurement budget, the dashed crimson line in (a) corresponds to fixing the choice of $\lambda_0, \lambda_1$ to the optimal value for $N=20$, and using this fixed choice for all $N \neq 20$. For a large range of SSIM scores, we see that  Naive approach uses between $2$x to $20$x times the number of measurements required by QSLAM to reconstruct $F_t$. 

\begin{figure}
	\includegraphics[scale=1]{SSIM0_collated}
	\caption{\label{SSIM0_collated} 1D array of 25 qubits in `step' field in left inset of (a).  Right inset plots SSIM score against total measurement budget for QSLAM [crimson circles] and Naive [black crosses] over 50 trials. This data inverted in(a) where ratio of Naive to QSLAM measurements shows x-fold reduction for a range of SSIM scores for optimised $\lambda_0, \lambda_1$ [solid crimson] and fixed choice $\lambda_0 = 0.93, \lambda_1 = 0.77$ [dashed crimson]. (b)-(e) Columns show single run maps using $N=10, 75$ measurements plotted for QSLAM [top] and Naive [bottom]. $(\lambda_0, \lambda_1)= (0.72, 0.58)$ and $ (0.95, 0.65)$ for (b),(c) respectively.}    	
\end{figure} 

In \cref{SSIM0_collated}(b)-(e), we extract the actual map images from the algorithms for the analysis reported in (a). For each map, the horizontal axis depicts the spatial index for 25 qubits, whereas the color of the image corresponds to the value of the estimated field, to be compared with the left inset in (a). The measurement budget for the left and right columns of maps is $N=10, 75$ respectively. QSLAM maps correspond to (b) and (c), where the path chosen by the controller is vertically expanded and plotted as a black line in (b). Comparing with the map reconstruction from the naive approach in (d) and (e), we see that the naive approach is yet to reconstruct the map after 75 measurements. The map values in (e) are either close to extreme values ($0, \pi$) or near half way mark $0.5\pi$, as what can be expected if one is to estimate the true field using empirical probabilities generated with just $3$ measurements in each location. In contrast, an `average' map value is attained by QSLAM in just 10 iterations, and this map is refined subsequently such that the QSLAM reconstruction is much closer to the values of the true field after 75 measurements. 

We extend the analysis from 1D to 2D in \cref{SSIM1_collated} and \cref{SSIM2_collated}. The structure of the data presented in both these figures is the same as \cref{SSIM0_collated} and the differences in data arise by moving from 1D to 2D and due to the change in the engineered true field. The comparison of results between \cref{SSIM1_collated} and \cref{SSIM2_collated} is that the true field is uniform for large regions in  the left inset of \cref{SSIM1_collated}(a), but the true field varies by every `pixel' for the left inset of \cref{SSIM2_collated}(a). The choice of the so-called `Square' and `Gaussian' field respectively in both of the figures tests robustness of QSLAM if spatial variation of the true field is significant relative to inter-qubit spacing. 
\begin{figure}
	\includegraphics[scale=1]{SSIM1_collated}
	\caption{\label{SSIM1_collated} 2D array of 25 qubits in $5 \times 5$  `square' field in left inset of (a).  Right inset plots SSIM score against total measurement budget for QSLAM [crimson circles] and Naive [black crosses] over 50 trials. Ratio of Naive to QSLAM measurements shows x-fold reduction for a range of SSIM scores for optimised $\lambda_0, \lambda_1$ [solid crimson] and fixed choice $\lambda_0 = 0.95, \lambda_1 = 0.65$ [dashed crimson]. (b)-(e) Columns show single run maps using $N=10, 75$ measurements plotted for QSLAM [top] and Naive [bottom]. $(\lambda_0, \lambda_1)= (0.92, 0.44)$ and $ (0.93, 0.77)$ for (b),(c) respectively.}    	
\end{figure} 

\begin{figure}
	\includegraphics[scale=1]{SSIM2_collated}
	\caption{\label{SSIM2_collated} 2D array of 25 qubits in $5 \times 5$  `Gaussian' field in left inset of (a).  Right inset plots SSIM score against total measurement budget for QSLAM [crimson circles] and Naive [black crosses] over 50 trials. Ratio of Naive to QSLAM measurements shows x-fold reduction for a range of SSIM scores for optimised $\lambda_0, \lambda_1$ [solid crimson] and fixed choice $\lambda_0 = 0.85, \lambda_1 = 0.71$ [dashed crimson]. (b)-(e) Columns show single run maps using $N=10, 75$ measurements plotted for QSLAM [top] and Naive [bottom]. $(\lambda_0, \lambda_1)= (0.85, 0.71)$ and $ (0.92, 0.44)$ for (b),(c) respectively.}    	
\end{figure} 

For all the results presented for 2D in  \cref{SSIM1_collated} and \cref{SSIM2_collated}, we find that numerically optimized choice of $\lambda_0, \lambda_1 \gg 0$ confirming that the sharing of information in QSLAM is non-trivial for any choice of the total measurement budget. .
The solid crimson lines in panel (a) of both figures correspond to optimised $\lambda_0, \lambda_1 $  for each $N$ as before,  and the dashed crimson line fixes these hyper-parameters to the optimal $\lambda_0, \lambda_1 $ at $N = 20$. We see that the optimal curve (solid crimson)  and the fixed choice curve (dashed crimson) do not change the overall observation that Naive approaches take $2$x - $18$x ($2$x - $15$x) more measurements than QSLAM for the same SSIM error threshold in the case where the field is Square (Gaussian). Since the Gaussian true field corresponds to a fast spatial variation relative to inter-qubit spacing, the result indicate that QSLAM performance drops as expected.

Ensemble-averaged performance results for 2D are also examined using single-run maps generated by the algorithms in panels (b)-(e) of both figures. By $75$ measurements in \cref{SSIM1_collated}, QSLAM reconstructs the average shape  of the field  in (b) and (c) whereas maps obtained from the naive approach are dominated by extreme values in (d) and (e), as in the 1D case. The control path in (b) (black line) confirms that not all qubits were visited, but only those for which state estimation was most uncertain. Qualitatively, more locations along the diagonal and the first off-diagonals are measured than the extreme off-diagonals, corresponding to locations where the true field is discontinuous. However, in \cref{SSIM2_collated} we compare (c) and (e) to see that the naive map is dominated by extreme field values and retains the structure of the true field, whereas the QSLAM map converges to approximate field values but loses the shape of the fast moving true field. With large measurement budgets, both QSLAM and Naive approaches converge to the true field. However,  for any give measurement budget, QSLAM requires more measurements relative to Naive than, for example, in the simpler case of \cref{SSIM1_collated}.

We return to the 1D regime to consider a linear string of trapped 171Yb+ ions subject to an unknown, stray magnetic field (noise). In the level structure of 171Yb+, the levels $S_{1/2}, F=0$  and one of the states $S_{1/2}, F=1$ form the hyperfine qubit. A projective measurement of an ion induces state dependent photon emission, where the ion fluoresces if its in the `bright' ($F=1$) state, else the ion is deemed to be in the dark ($F=0$) state. The transition from $S_{1/2}, F=1$ to $P_{1/2}, F=0$ is used for florescence detection. One source of `measurement noise' is that a hyperfine qubit can change its state from the bright to dark (and vice versa) during the measurement procedure. These state changes are due to off-resonant excitations and spontaneous decay processes. Dark (bright) state lifetime is of order milliseconds (tens of $\mu$s). Camera images are classified using classification algorithm that outputs a set of labels `0' or `1' for each ion in the camera picture accommodating, in the language of classifiers, class-dependent `label' (measurement) noise due to off-resonant excitations. An example of all ions in the bright state captured by a camera image is shown in \cref{SSIM_expt_0_collated_reweighted}(a) using a resolution of $0.861 \mu$m per pixel and exposure of $50 \mu$s. Bright-state decay during the camera detection is the dominant measurement noise contribution.  

In experiment, we perform a total of $25,500$ Ramsey measurements of all six ions with a Ramsey wait time of $40$ ms. Here, each value of $F_t^{(j)}$ corresponds to the relative phase between the bright and dark states of the $j$-th ion induced by the static, stray magnetic field. The full set of $6 \times 25500$ measurements constitute a data bank for both algorithms, QSLAM and Naive. The rest of the analysis proceeds as before with the following two changes. Firstly, instead of engineering measurement data, we randomly choose a measurement outcome from the data bank for a desired qubit location at each iteration. Secondly, in absence of a true engineered field, the SSIM scores are calculated with respect to the measured field, i.e. the expected value of $F_t$ under $25,500$ measurements. This measured field is shown as the right inset of \cref{SSIM_expt_0_collated_reweighted}(b). Under these conditions, the Naive approach smoothly approaches zero error as the measurement budget increases in the left inset of \cref{SSIM_expt_0_collated_reweighted}(b). 
\begin{figure}
	\includegraphics[scale=1]{SSIM_expt_0_collated_reweighted}
	\caption{\label{SSIM_expt_0_collated_reweighted} (a) Image of six trapped 171Yb+ ions in bright (F=1) state subject to a stray magnetic field gradient in right inset of (b) measured using 25,000 Ramsey measurements, with wait time $40$ms, resolution $0.861 \mu$m per pixel and exposure $50 \mu$s. Images subject to bright state decay with mean lifetime order $\sim 10\mu$s. (b) right inset depicts SSIM error as a function of total measurement budget for QSLAM (crimson circles) and Naive (black crosses) inverted to give crimson solid in main figure; dashed crimson optimal for fixed  $\lambda_0, \lambda_1) = (0.95, 0.97) $ at $N=20$. (b)-(f) Columns for single run maps using $N=10, 75$ plotted for QSLAM [top] and Naive [bottom]. $(\lambda_0, \lambda_1)= (0.99, 0.92)$ for both (c),(d).}    	
\end{figure}
The main figure of \cref{SSIM_expt_0_collated_reweighted}(b) shows QSLAM performance for numerically optimised $\lambda_0, \lambda_1 \gg 0$ (solid crimson). The analysis assumes that the ions are approximately equidistant. A $2$x-$4$x reduction in measurements are achieved with respect to using the Naive approach. This is confirmed in the left inset of \cref{SSIM_expt_0_collated_reweighted}(b) where QSLAM data (crimson circles) sits below Naive (black crosses) for a wide range of measurement budgets. The solid crimson curve in (b) corresponds to the numerically optimized $\lambda_0, \lambda_1$ and fixed choice (dashed crimson) refers to the optimal hyper-parameters at $N=20$. Near zero SSIM scores, the optimal curves saturate with ratio of QSLAM and Naive going to unity. For all other cases, the curves are strictly above unity and correspond to measurement reductions in using QSLAM over Naive for a range of SSIM scores. 

A single map is plotted for total measurement $=10, 75$ in panels (c)-(f) in \cref{SSIM_expt_0_collated_reweighted}, where both algorithms reconstruct the value of the measured field in 75 measurements with similar SSIM scores, and as depicted visually, 2-3 pixel errors are incurred via the Naive approach. We note that the Naive approach must converge to the measured field in the limit of large data, and hence the comparison of algorithmic performance in  \cref{SSIM_expt_0_collated_reweighted} is  different in principle to the comparisons made in \cref{SSIM0_collated,SSIM1_collated,SSIM2_collated}.

\section{Discussion \label{sec:discussion}}
The potential space of all possible maps for an unknown field and their associated neighborhoods is tractable in this manuscript only via the use of the iterative maximum likelihood (ML) procedure. The iterative ML procedure is enabled by having access to the quantum mechanical Born rule. In the language of classical SLAM, the Born rule enables us to obtain a deterministic data association function to theoretically link incoming data to the map $F_t$. Classically, a deterministic function for data association typically does not exist, and data association is yet another probabilistic inference procedure. i.e. one must infer from data whether there exist features that should be added to or re-observed on the estimated map. In QSLAM, the Born rule enables unambiguous updates to the estimated map at each qubit location. 

We have chosen to implement an iterative ML procedure by using a two layer particle filtering mechanism. The design of the two-layered particle filter is not typical, but arises very naturally from a tree-based or branching approach to assigning probabilities to a candidate pair of a map and a length-scale at a particular location. The assignment of these probabilities is motivated by physical arguments. Moreover, the branching process of particles in QSLAM satisfies properties of standard non-linear particle filters, despite the two-layered approach. Both QSLAM and standard particle filtering algorithms satisfy multinational sampling of parent `particles' to yield `off-spring' particles after each iteration. In a forthcoming manuscript, we link QSLAM to existing literature about the convergence of particle filtering approximations in non-linear stochastic filtering.

Under the conditions outline above, we have described the behaviour of QSLAM for static fields in the results presented in the previous section. Across these many diverse parameter regimes and spatial configurations, the QSLAM framework is tuned via its two key hyper-parameters - $\lambda_0, \lambda_1$ - which collectively represent the strength of the sharing mechanism in small neighborhoods. Based on our results thus far, the key numerical evidence for the correctness of QSLAM comes from the observation that the numerically optimized values of $\lambda_0, \lambda_1 \gg 0 $ for all of the parameter regimes tested to date. A non zero value for both  $\lambda_0, \lambda_1$  confirms that non-trivial information sharing occurs, particularly in the sparse measurement regime with small measurement budgets. Indeed this sharing of information in the sparse measurement regime is responsible for QSLAM obtaining an approximate value for the unknown field that is closer than the Naive approach. One can see this, as  an example, for the maps depicted in (b) and (d) of \cref{SSIM0_collated,SSIM1_collated,SSIM2_collated} for a total budget of 10 measurements. Using these numerically optimised values for  $\lambda_0, \lambda_1$ with engineered true fields, QSLAM converges to the true field and approaches low SSIM scores $< 0.1$ in the large data limit for all results in the paper. During this convergence, the measurement ratio of QSLAM to the Naive approach is upper bounded by unity ($\leq 1$), suggesting that the sharing mechanism enables QSLAM to learn the true field correctly and with fewer measurements than the Naive approach. 

It is possible to construct scenarios to break QSLAM. For spatial fields that are varying quickly with respect to the spatial resolution afforded by the qubit grid, we expect QSLAM to shrink the size of neighborhoods until there are no neighbours around any qubit and information sharing is no-longer permissible. Under fast vary spatial fields, we expect QSLAM to provide no performance gain over the Naive approach. This effect is seen in \cref{SSIM2_collated}, where the performance gain of QSLAM over Naive begins to diminish for a Gaussian field. Indeed, the smallest inter-qubit spatial separation sets a fundamental limit on our knowledge of the true field. In physical hardware, such as a trapped ion apparatus, it is envisioned that one can shuttle or bus a spatial arrangement of ions to finely sample regions in space. In this scenario, one expects an improvement in spatial resolution relative to that of a fixed spatial arrangement of qubits.

\section{Conclusion \label{sec:conclusion}}

We presented QSLAM - an autonomous learning framework which reconstructs an unknown spatial field (noise)  by choosing the location of the next measurement in large quantum  computing architectures in real time. Simulations using engineered data and offline processing of experimental data confirm that a naive brute force measurement approach uses between $2$x-$20$x more measurements  than QSLAM to reconstruct a slowly varying field within an acceptable error threshold. The framework uses an iterative maximum likelihood procedure via a novel two-layer particle filter to conduct non-linear noise filtering and state estimation. Subsequently, state estimation information is shared in small `neighborhood' to enable rapid learning. This framework is flexible and can be extended in future work to accommodate temporal dynamics in both the true field or changes in the availability of qubits on hardware.


\iffalse
\begin{itemize}
	\item simulations above provide insight into the QSLAM algorithm
	\item key observation is that for all data presented, lambdas are numerical optimised and these optimal values lie far from zero. this is numerical evidence that sharing mechanism of qslam is non-trivial. Further, for non-linear stochastic filtering with continuous variables, it is difficult to contruct a proof of correctness for the field values obtained by QSLAM in regions of overlapping neighborhoods where no physical measurement in taken. For example, the control path for ten iterations does not visit every qubit in any of the cases presented. hence non rigorous testing for the theoretical correctness of QSLAM is attempted by tuning the strength of the sharing mechanism via $\lambda$
	\item iterative ML approach is enabled by having access to the quantum mechnical Born rule. This deterministic data association  function links incomign data streams theoretically to the map $F_t$. classically, such a function is yet another probabilistic inference procedure where incoming data stream are converted to add new features or re-observe existing features of the map. 
	\item the likelihood functions represent assumptions of the noise models. (ref old notes). In particular, both noise models represent the distribution of errors when the sensing mechanism is quantised, namely, that errors are only allowed to take certain values. The parameters of these noise models have been set close to zero,  but one could optimise noise parameters or add experimentally relevant prior knowledge about measurement noise to tune the algorithm to specific applications with a greater level of detail than presented here.
	\item we have chosen a two layer (branched) particle filering mechanism. a key observation is that the the branching process in QSLAM satisfies properties of typical non-linear particle filters and theoretically allows us to link to existing literature about the convergence of particle filtering approximations to the true $X_t$ in non-linear stochastic filtering. Then the true error between the true posterior and the particle filtering approximation scales as $1 / n_\alpha$ where $n_\alpha$ represents the number of $\alpha$ particles. 
\end{itemize}
\fi