Decoherence acting on quantum computing hardware is a key barrier to realising at-scale, high fidelity quantum computations that outstrip modern capabilities offered via networks of classical computers. At the level of hardware, the no-cloning requirement on the information stored by quantum bits (qubits) prevents a naive application of error correction from classical computing; instead, a  `0' or a `1' logical bit is encoded into a composite quantum state spread over many physical qubits. Within such hardware,  controlling a single physical qubit is difficult in the presence of decoherence and an accumulation of errors over many single qubit operations reduces the overall fidelity of a computation. In realistic operating environments, noise sources are often correlated in space or time and these noise drifts typically violate basic assumptions in quantum computation, error correction and single qubit control;  thereby correlating qubit operations and qubit measurements that would otherwise be statistically independent in any computational procedure.  Hence, any physical realisation of a meaningful quantum computational capability will require online procedures for noise characterisation and adaptive control techniques to address a range of practical problems, such as tracking drift in noise sources (in time or space) and enabling feed-forward control techniques for single qubit operations. \\
\\
The availability of free or idling single qubits on hardware while quantum computations are being carried out represents an opportunity to use these qubits for sensing measurements.
Leading experimental platforms for quantum computing are often envisaged as spatial arrangement of qubits on a `chip', where operations required for quantum computation and quantum error correction are described using  a corresponding `quantum circuit' - a prescription of qubit control operations that act on sub-sets of qubits on the chip. A typical feature of many quantum circuits is that they often involve a \textit{projective measurement on a single qubit } within a large composite system (e.g., for error correction or final read-out). Once a projective measurement is made, and when this measurement outcome is known, these single qubits collapse into a classical `0' or a `1' state and thus, these qubits can be appropriated and reset for use, as one example, in sensing applications.  The inter-weaving of sensing measurements with quantum computations as any given qubit is absorbed or released from quantum circuits loaded on the hardware enables us to gather information about the underlying noise field affecting hardware in realistic operating environments.\\
\\
The optimal reconstruction of an arbitrary unknown spatio-temporal environment using local sensor measurements is well considered classically, but with limited extensions to the quantum computing applications, such as the one described above. A temporally drifting noise field in quantum computing hardware can be sensed and tracked using classical inference procedures and data generated from single shot qubit measurements [REFS]. For the case of dephasing noise, the success of these procedures is contingent on learning true noise correlations as the data set grows, for example, by approximating a random realisation of a process in the spectral representation or autoregressive representation, and estimating the coefficients of relevant parameterising using single qubit data [REF]. Under certain physical and experimental conditions, the analysis of spatially correlated data can be re-cast into time series analysis such that tools for characterising temporally correlated random processes can be re-applied to understand correlations over measurements spanning a spatial arrangement of qubits. However, the  change in the availability of any given qubit for sensing corresponds to additional `dynamics'; from which we seek a reconstruction of spatio-temporally correlated noise field over the entire physical platform. Further, we seek to relax the assumption that the spatial arrangement of qubits is perfectly known. The focus of this manuscript, then, is the classical inference problem of learning spatio-temporal dephasing noise correlations from a set of binary measurement data accruing from projectively measuring single qubits on a spatial grid. We resolve this inference problem by borrowing classical procedures for Bayesian inference, in particular, procedures involved in  online, recursive solutions to the simultaneous localisation and mapping (SLAM) problem in the field of autonomous environmental exploration and sensing. In seeking the optimal reconstruction of the dephasing noise map using local sensor measurements, the observation of the environment from a different spatial locations are expected to reduce errors in both the reconstruction of the map and our knowledge of the qubit location relative to the map. Hence, the temporal inter-leaving of environmental sensing measurements at different spatial locations justifies the SLAM framework - as opposed to considering two independent inference problems for localisation and mapping separately.\\
\\
While these analogies pertaining to a `map' and `local sensors' are useful as a high level overview of the key concepts in the inference problem at hand, we depart from the classical SLAM case in several non-trivial ways. Of these, the key challenge is that there is no quantum mechanical equivalent of scanning (measuring) an unknown environment at a new location. In our application, the dephasing field in the neighbourhood of a sensing qubit cannot be measured  without inducing state collapse of neighbouring qubits. This would be counter-productive as the neighbours of our sensing qubit are involved in computations, thus unavailable for sensing. Another challenge is the data-association formalism associated with the original SLAM framework is not directly transferable. In classical SLAM, data association means that one unambiguously  extracts `map features' from a physical scan measurements. In classical applications, map features are sharply discontinuous from the background (e.g. local features, such as a table, or edge features, such as a wall) and often the simplest information retained about the map is the binary data of whether or not a region on a map is occupied. In contrast to classical SLAM, we obtain binary data at one location pertaining to the qubit (no scan); from which we wish to infer a non binary (floating point) value of qubit state under dephasing (a qubit phase). Further, no physical sources can produce a discontinuous features in dephasing field - namely, our map is  continually varying everywhere. Hence, the environmental scan and data association framework of the SLAM problem must be reworked significantly and these changes are the core of what is justified numerically in the SLAM methodologies presented in this manuscript. \\
\\
The rest of the document is structured as follows. In \cref{sec:physicalsetting} we describe the physical set-up and measurement model, in particular we introduce the so-called Born probability for measurement outcomes for a single qubit under dephasing. We formally define the sensing qubit, and the classical dephasing field. \cref{sec:fieldreconstruct}, we introduce the key departures from the classical SLAM framework in two ways: firstly, by changing the data-associate step which links the update of an environmental map based on sensor data and secondly, by introducing a mechanism to share dephasing information in a small spatial neighborhood around a sensing qubit without inducing state collapse on neighbouring (unavailable) qubits. To formalise the second idea, we introduce a concept called `quasi-measurements' - namely, binary `0' or `1' data that has been generated from a mathematical procedure within this neighbourhood as opposed to a real physical single qubit measurement. With the above modifications, we introduce Quantum SLAM (QSLAM) as an approximate theoretical solution to the full Bayesian SLAM problem in \cref{sec:qslam} and \cref{sec:iterativebayes}. Here, we follow classical approaches to building an online, recursive Bayesian formulation of the SLAM problem, pioneered in \cite{thrun2001probabilistic} for online, probabilistic approach to building maps of a time-invariant physical landscape  using autonomously mobile robots. The recursive Bayesian formulation of the SLAM problem in \cite{thrun2001probabilistic} is algorithmically implemented using a particle filter. We present a similar approach in \cref{sec:particlefilter}, where we use a two-tiered particle filter to implement QSLAM. We establish a controller for QSLAM in \cref{sec:control} to help automate the process of choosing the location of the next measurement in an iterative procedure. Performance evaluation risk metrics are discussed in \cref{sec:risk}. Our theoretical framework and algorithmic performance is tested in numerical simulations in \cref{sec:results}; the results are discussed in \cref{sec:discussion} and we conclude in \cref{sec:conclusion}.
\\