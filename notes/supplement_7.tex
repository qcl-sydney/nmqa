\section{Supplementary Information}

In the Supplements, we provide details and results of additional simulations to accompany the main findings of the paper. We provide an overview of the simulation methodology as well as the numerical optimisation procedure for QSLAM hyper-parameters. We define the risk metric, the Structural Similarity Index (SSIM), and subsequently report two additional simulations for QSLAM using engineered data in 1D and 2D. 

For all simulations, we use a risk metric called the Structural Similarity Index (SSIM) score to conduct optimisation of QSLAM hyper-parameters and assess performance relative to the Naive measurement strategy. This metric is defined for two vectorized images $x$ and $y$ as:
\begin{align}
	SSIM(x, y) = \frac{(2 \mu_x\mu_y +C_1)(2\sigma_{xy} + C_2)}{(\mu_x^2  + \mu_y^2 + C_1 )(\sigma_x^2 + \sigma_y^2 + C_2)} \\
	\text{Norm } SSIM(x, y):= |1 - SSIM(x,y)|
\end{align} In the formula above, $\mu_i, \sigma_i^2, i = x,y$ represent the sample estimates of the means and variances of the respective vectorised images, and $\sigma_{xy}$ captures correlation between images. The term $SSIM(x,y)$ is exactly as reported in \cite{wang2004image}, with arbitrary constants $C_1=C_2=0.01$ set to stabilize the metric for images with means or variances close to zero.  The term $SSIM(x, y)$ is defined such that the score is unity if and only if $x=y$. We report the absolute value of the deviations from the ideal score of unity, where we do not care about the direction of the deviation and only the sign, as given by $\text{Norm } SSIM(x, y)$. For our application, this is metric lies between $[0, 1]$ (negative values of $SSIM(x, y)$ are not seen in our numerical demonstrations). For the analysis presented in the main paper and the Supplement, we report the expected value of the $\text{Norm } SSIM(x, y)$ taken as an average over 50 trials. 

\begin{figure}
	\includegraphics[scale=1]{SSIM_optimisation}
	\caption{\label{SSIM_optimisation} }  	
\end{figure}  
We plot the Norm SSIM score for 50 randomly sampled $(\lambda_1, \lambda_2)$ pairs and we compare this with the expected value of the Norm SSIM score for $\lambda_1, \lambda_2 = 0$ in \cref{SSIM_optimisation}. The rows of \cref{SSIM_optimisation} correspond to three different simulations of the true field: a 1D step field of \cref{SSIM0_collated}, a 2D square field reported in the main text as \cref{SSIM1_collated}, and a 2D Gaussian field corresponding to \cref{SSIM2_collated}. In each panel, a black dot represents a randomly selected $(\lambda_1, \lambda_2)$ pair. The shaded circle selects pair with an SSIM score better than $\lambda_1, \lambda_2 = 0$ by at least 0.1 in arbitrary units (10\% of maximal expected deviation). The color of the shaded circle refers to the actual value of the Norm SSIM. The crimson star indicates the lowest score achieved over all pairs, and defines the numerically optimised $(\lambda_1, \lambda_2)$ values used for analysis. This procedure is repeated for all $N$ in the solid lines of \cref{SSIM1_collated,SSIM0_collated,SSIM2_collated}. The fixed choice case refers to finding optimal $(\lambda_1, \lambda_2)$  pair for $N=20$  and using this pair for all $N \neq 20$. To aid visual comparisons, we use the same set of random pairs $(\lambda_1, \lambda_2)$ across all experiments, but results hold if candidate hyper-parameters are randomised across experiments. For the numerical demonstrations in the main text and the Supplement, it is seen that numerically optimised $\lambda_1, \lambda_2 \neq 0$ and QSLAM sharing is non-trivially contributing to the overall inference procedure. 

\begin{figure}
	\includegraphics[scale=1]{SSIM0_collated}
	\caption{\label{SSIM0_collated} 1D array of 25 qubits in `step' field in left inset of (a).  Right inset plots SSIM score against total measurement budget for QSLAM [crimson circles] and Naive [black crosses] over 50 trials. This data inverted in(a) where ratio of Naive to QSLAM measurements shows x-fold reduction for a range of SSIM scores for optimised $\lambda_1, \lambda_2$ [solid crimson] and fixed choice $\lambda_1 = 0.93, \lambda_2 = 0.77$ [dashed crimson]. (b)-(e) Columns show single run maps using $N=10, 75$ measurements plotted for QSLAM [top] and Naive [bottom]. $(\lambda_1, \lambda_2)= (0.72, 0.58)$ and $ (0.95, 0.65)$ for (b),(c) respectively. Single map SSIM for QSLAM (b) 0.26; (c) 0.08; for Naive (d) 0.70;  (e) 0.54.}    	
\end{figure} 

Of the experiments reported in \cref{SSIM_optimisation}, one observes that performance improves from bottom left to top right corners of all panels, indicating a performance improvement away from zero values for both hyper-parameters.   With $\lambda_1, \lambda_2 \equiv 1$, shared information in QSLAM is treated on an equal footing with information obtained from physical data.  For $\lambda_1, \lambda_2 \equiv 0$, QSLAM effectively reduces to a brute force measurement strategy with randomly sampled locations for performing measurements. Since performance appears to improve as two hyper-parameters, $\lambda_1, \lambda_2$ move away from zero and this trend is unchanged even as the total amount of measurement data increases (left column to right), we interpret this as numerical evidence that the sharing mechanism in QSLAM framework is both non-trivial and correct in the large data limit. 

\cref{SSIM0_collated} provides a simulation of the performance of QSLAM when neighbours are constrained to a line in 1D. We can re-stack 25 qubits on a grid in 2D while keeping the ratio of qubits subject to low v. high field the same (25:75) in \cref{SSIM1_collated} in the main text.The performance of QSLAM is of course conditioned on the  spatial variation in the true field relative to the inter-qubit spacing on the grid.

\begin{figure}
	\includegraphics[scale=1]{SSIM2_collated}
	\caption{\label{SSIM2_collated} 2D array of 25 qubits in $5 \times 5$  `Gaussian' field in left inset of (a).  Right inset plots SSIM score against total measurement budget for QSLAM [crimson circles] and Naive [black crosses] over 50 trials. Ratio of Naive to QSLAM measurements shows x-fold reduction for a range of SSIM scores for optimised $\lambda_1, \lambda_2$ [solid crimson] and fixed choice $\lambda_1 = 0.85, \lambda_2 = 0.71$ [dashed crimson]. (b)-(e) Columns show single run maps using $N=10, 75$ measurements plotted for QSLAM [top] and Naive [bottom]. $(\lambda_1, \lambda_2)= (0.85, 0.71)$ and $ (0.92, 0.44)$ for (b),(c) respectively. Single map SSIM for QSLAM (b) 0.24; (c) 0.16; for Naive (d) 0.74;  (e) 0.38.}    	
\end{figure} 
\FloatBarrier
In \cref{SSIM2_collated}, we provide an example of a true field where the value of the field changes by almost every `pixel' that is, spatial variation is `fast' compared to results in \cref{SSIM1_collated}. In these additional simulations, QSLAM still outperforms Naive by 2-15x for a large range of error scores and the ratio between QSLAM and Naive measurements approaches 1 in the large data limit. 

\section{QSLAM Algorithm}

\begin{algorithm}[H] % Floats not comptatible with revtek4-1. Need "[H] option. 
	\caption{QSLAM}\label{algorithm:q-slam-pf}
	\begin{algorithmic}[0] 
		\Procedure{QSLAM}{$d$ qubit locations, $\lambda_1, \lambda_2$}
		\\
		\If{$ t = 0$}
		\Procedure{Initialize}{$X_0$} 
		\For{$ \alpha \in \{1, 2, \hdots, n_\alpha \}$}
		\State Initially sample $x_{0}^{(\alpha)} \sim \pi_0$ 
		\State Initially compute $W_0^{(\alpha)} = \frac{1}{n_\alpha}$
		\EndFor
		\EndProcedure 
		\EndIf
		\\
		\While{$ 1 \leq t < T$}
		\If{Controller}
		\State $j_t, Y_t^{(j_t)} \gets$ \Call{Controller}{$X_{t-1}$}
		\EndIf
		\For{$ \alpha \in \{1, 2, \hdots, n_\alpha \}$}
		\State Update $F_t^{(\cdot), (\alpha)}$ via $ \{Y_t^{(j_t)},  \{\hat{Y}_t^{(q_t)}\}, \lambda_1\} $
		\State $\{x_{t}^{(\alpha)}\} \gets $ \Call{PropagateStates}{$\{x_{t-1}^{(\alpha)} \}$ }
		\State $\{\{x_t, W_t\}^{(\alpha, \beta_\alpha)}\} \gets $ \Call{ComputeWeights}{$\{x_{t}^{(\alpha)}\}$}
		\State $\{x_t^{(\alpha, \beta_\alpha)}, \frac{1}{n_\alpha n_\beta}\} \gets $ \Call{Resample}{$\{\{x_t, W_t\}^{(\alpha, \beta_\alpha)}\}$}
		\State Update $R_t^{(j_t), (\alpha)}$
		\State $\{\{x_t, W_t\}^{(\alpha)}\} \gets$ \Call{Collapse$\beta$}{$\{x_t^{(\alpha, \beta_\alpha)}, \frac{1}{n_\alpha n_\beta}\}$}
		\State $\{x_t^{(\alpha)}, \frac{1}{n_\alpha}\} \gets $ \Call{Resample}{$\{\{x_t, W_t\}^{(\alpha)}\}$}
		\EndFor
		\State $\{\hat{Y}_{t+1}^{(q)}\}_{q\in Q_{t+1}} \gets $ \Call{Generate$\hat{Y}$}{Posterior $X_t$}
		\EndWhile \label{pseudoalgo:qslamr:endwhile2}			
		\EndProcedure
		\\ \dotfill
		\Function{ComputeWeights}{$\{x_{t}^{\alpha}\}$}
		\For{$ \alpha \in \{1, 2, \hdots, n_\alpha \}$}
		\State Compute $\tilde{W}_t^{( \alpha )} = g_1(\lambda_1, Y_t^{(j)}) $ 
		\State $\{x_t^{(\alpha, \beta_\alpha)}\} \gets $  Generate $\beta$-layer by sampling $R_0$ 
		\For{$\beta_\alpha \in \{1, 2, \hdots, n_\beta \} $}
		\State Compute $\tilde{W}_t^{( \beta_\alpha | \alpha)} = g_2(\lambda_2, Q_t) $
		\EndFor
		\State Normalise $\tilde{W}_t^{( \beta_\alpha | \alpha)}$
		\EndFor
		\State Normalise $\tilde{W}_t^{( \alpha)}$
		\State Compute $W_t^{(\alpha, \beta_\alpha)} =  \tilde{W}_t^{( \beta_\alpha | \alpha)} \tilde{W}_t^{( \alpha)} \quad \forall \{\alpha, \{\beta_\alpha\} \}$
		\State Return $ n_\alpha n_\beta $ particles and weights $\{\{x_t, W_t\}^{(\alpha, \beta_\alpha)}\}$		
		\EndFunction 
		\\ \dotfill
		\Function{Resample}{$\{\{x_t, W_t\}^{(i)}\}$}
		\For{$i$}
		\State Sample $X$  according to $\{W_t^{(i)}\}$
		\State $x_t^{(i)} \gets X$
		\EndFor
		\State Reset $\{W_t^{(i)}\}$ to a uniform distribution
		\EndFunction 
	\end{algorithmic}	
\end{algorithm}

In the pseudocode of \cref{algorithm:q-slam-pf}, $X_t$ is the extended state vector and the initialisation procedure ensures all particles are sampled from the prior for $X_0$ and are equally weighted. For $t>0$, we note that the update rules for $F_t$ and $R_t$ occur at different times for each $t$, corresponding to an iterative maximum likelihood approach over the state of all maps and length-scales, $X_t$. The function \texttt{PropagateStates} represents the transition probability distribution for Markov $X_t$ i.e. it represents identity dynamics and is a placeholder for future extensions to model dynamical $F_t$. Weights are computed according to noise density functions  $g_1(\lambda_1, Y_t^{(j)})$ and $g_2(\lambda_2, Q_t)$, as described earlier, and each re-sampling step is a so-called multinomial branching random process. We marginalise out the $\beta$ distribution of particles in the function \texttt{Collapse$\beta$}. This marginalisation step  means that the total number of particles is then constant at the begining and end of each time-step $t$, ensuring that the entire branching process in QSLAM remains a multinomial branching process. The function \texttt{Generate$\hat{Y}$} spreads the state information generated by the measurement at $j$ to all qubits in the posterior neighborhood about $j$, where the shared state information is converted into a 0 or 1 output using Born's rule. The resulting output is the discrete binary set labeled  $\{\hat{Y}_{t+1}^{(q)} \}$.