\section{QSLAM Framework}
Our framework applies to a physical setting that consists of a spatial arrangement of $d$ qubits coupled to a common, classical field. We assume that the spatial arrangement of qubits is known and determined by a particular choice of hardware. A classical field couples all qubits in this arrangement - this field is unknown and corresponds to stray-field effects or the presence of noise fields in realistic operating architectures.  Our objective is to build an algorithm that uses iterative updates to characterize the values of the unknown noise field, while using the lowest number of single-qubit measurements. The desired output of our algorithm at any given iteration, is a map of the noise field, denoted $F_t$ at time $t$, inferred from the measurement record. 

Inspired by classical algorithms for simultaneous localisation and mapping problems, QSLAM reconstructs a map by not only filtering single qubit measurements, but by sharing information between qubits in small spatial regions called `neighborhoods'. We consider both 1D and 2D spatial arrangements where qubits are subject to a time-invariant $F_t$. In making a measurement, we assume that a single qubit is addressed and measured once during each iteration, and these measurements are single qubit Ramsey measurements. An example in 1D of such a physical setting are stray magnetic field gradients which couple a line of trapped ions along the axial trap axis. The algorithm is structured to incorporate additional complexity as we introduce realistic operating environments for quantum computing hardware, for example, dynamics for $F_t$.

The key operating principle of QSLAM is an iterative maximum likelihood procedure that locally updates state variable, $F_t^{(j)}$, before globally sharing the state information at qubit $j$ with the neighboring qubits in the vicinity of $j$. This neighborhood of $j$ is  denoted $Q_t$ and examples of neighborhoods are depicted as shaded circular regions in the leftmost panel of \cref{fig_intro_highlevel_v2}. Each neighborhood is parameterised by a so-called length-scale value, $R_t^{(j)}$.   The collection of map values and length-scales at every qubit, $X_t:=\{F_t^{(j)}, R_t^{(j)}\}$ are depicted as the extended state vector in the leftmost panel \cref{fig_intro_highlevel_v2}. 

Obtaining a posterior state over the space of all maps and length scales, $X_t$, is defined but formidable. Indeed, this is often a roadblock in classical SLAM problems [refs]. We follow \cite{thrun2001probabilistic} in performing a numerical approximation to solve the QSLAM inference problem. A numerical technique called a particle-filter is used to iteratively obtain the most likely map and neighborhood at $j$ and this is called the posterior state vector at $t$. As part of this technique, QSLAM scores candidate neighborhoods and maps based on the likelihood of seeing such neighborhoods and maps  given data observed. However, to make this step tractable, an iterative maximum likelihood update procedure is used: first, we update $F_t$ assuming $X_{t-1}$ is known, and second, we update $R_t$ given $F_t$ is known. This allows us to obtain a posterior $X_t$ after the particle filtering  step in \cref{fig_intro_highlevel_v2}.
\begin{figure*}
	\includegraphics[scale=0.85]{fig_intro_highlevel_v2}
	\caption{\label{fig_intro_highlevel_v2}}    	
\end{figure*} 

In the middle panel of \cref{fig_intro_highlevel_v2}, the controller chooses where to physically measure next by choosing the region where posterior state variance is maximally uncertain. This is schematically shown as the location $k$ with the measurement outcome denoted $Y_{t+1}^{(k)}$ in the top-middle panel of the figure. Meanwhile, the posterior state information at $t$ is shared with other qubits in the vicinity of the qubit at $j$, where shared information is denoted by the set $\{\hat{Y}_{t+1}^{(q)} \}$ and $q$ labels the neighboring qubits of $j$ in the posterior neighborhood, $Q_{t+1}$. This new physical measurement, $Y_{t+1}^{(k)}$,  and the set of shared information between qubits in a posterior neighborhood, $\{\hat{Y}_{t+1}^{(q)}, q \in Q_{t+1}\}$,  collectively form the inputs for the next time-step, $t+1$. The full schematic in \cref{fig_intro_highlevel_v2} summarises the QSLAM framework.

A key feature is that the particle filter in QSLAM is implemented as modified particle filter with 2 types of particles - one type carries the full state vector, $X_t$, and the other type discovers optimal neighborhood size, $R_t^{(j)}$, for sharing information at any qubit location, $j$.  A standard particle represents the posterior distribution over the state variables $X_t$ as a discrete collection of weighted particles. In this approximation, each particle has two properties - a position and a weight - where the position of the particle carries information about state variable and the weight specifies the importance of the particle. After each iteration, the particles are `re-sampled' i.e. the original set of particles at $t$ are replaced by an off-spring set of particles, where the probability that a parent is chosen to represent itself again (with replacement) is directly proportional to its weight. Over many iterations, only the highest weighted particles survive. Hence QSLAM naturally requires two types of particles in each time step $t$ to implement an iterative maximum likelihood procedure, the first type for  updating $F_t$ and the second type to discover an optimal update for $R_t$ , given $F_t$.

With these approximations, and assuming time invariant maps (identity dynamics), the QSLAM filtering problem requires only two specifications: a prior or initial distribution for the state vector at $t=0$, and a so-called likelihood function that scores or weights each particle in every iteration. Assuming a uniform prior, we need only to define the global likelihood function incorporating both particle-types. We will denote the first set of particles as $\alpha$-particles indexed by the set of numbers $\{1, 2, \hdots, n_\alpha\}$. For each $\alpha$ we associated a layer of $\beta^{(\alpha)}$-particles labeled from $\{1, 2, \hdots, n_\beta \}$. Each  $\alpha$ particle is weighted by $ g_1(\lambda_1, Y_t^{(j_t)})$, where $\lambda_1$ is a hyper-parameter discussed subsequently. A single $\beta^{(\alpha)}$-particle inherits the state from its $\alpha$-parent; but additionally, acquires a single uniformly distributed sample for $R_t^{(j)}$ from the length-scale prior distribution. These properties make the $\beta$-layer uniquely a particle layer to discover local neighborhood length-scales at $j$, and as such are scored by a separate likelihood function, $g_2(\lambda_2, Q_t)$, where $\lambda_2$ is another hyperparameter of the QSLAM model. Then the total likelihood for an $(\alpha, \beta^{(\alpha)})$ pair is given by the product of the $\alpha$ and $\beta$ particle weights. %Here,the notation  ${}^{z_t}$ signifies that the likelihood function depends on the input $Z_t = (Y_t^{(j)}, \{\hat{Y}_t^{(q)}\}_{q \in Q_t}) \in \{0,1\}$ due to a single physical measurement at location $j$ and time $t$,  and  neighbourhood updates within $Q_t$ about $j$. 

In general, the functions $g_1(\lambda_1, Y_t^{(j)})$ and $g_2(\lambda_2, Q_t)$ are derived as the noise probability density functions that capture the main injections of uncertainty in our model for the physical system. The function $g_1(\lambda_1, Y_t^{(j)})$ describes measurement noise on a local binary qubit measurement. We use Born's rule to specify the probability of seeing a zero or a one based on the value of the dephasing field, $F_t^{(j)}$. The residual uncertainty introduced by a projective quantum measurement can be described as a truncated or a quantised Gaussian error model, as described in \cite{riddhinotes}. Meanwhile, the function $g_2(\lambda_2, Q_t)$ is based on noise density of unknown true errors arising from approximating a continuously varying unknown spatial field with a series of overlapping neighborhoods. We assume that this true error density has a non-zero mean, variance and it is Gaussian distributed. As examples, if the true field is uniform everywhere, large neighborhood sizes are likely to be favorably weighted by  $g_2(\lambda_2, Q_t)$ and survive over many iterations. If the true field is fluctuating too fast  with respect to the smallest separation distances between qubits, then QSLAM is limited by this minimum spatial resolution. %One can configure hyper-parameters of QSLAM such that in the infinite data limit where all qubits have been measured repeatedly, the true error distribution corresponding to $g_2(\lambda_2, Q_t)$ shrinks to zero, as detailed blow. % by increasing the importance of physical measurements with respect to the sharing mechanisms in QSLAM. %The function $g_2(\lambda_2, Q_t)$ weights or scores candidate neighborhoods $Q_t$ at $j$, with a candidate $R_t^{(j)}$, and assuming the map, $F_t$ is given. 

%As an illustrative example, the first measurement at $j$ locally updates the field, $F_t^{(j)}$, In the absence of any further information, there are only two possible choices for the field update, either $F_t^{(j)}$ is zero or $\pi$. Given $F_t$, and that no other qubit has been measured, the QSLAM picks a uniformly distributed posterior $R_t^{(j)}$. The posterior $R_t^{(j)}$ sets the posterior neighborhood, $Q_{t+1}$, and generates the posterior neighborhood updates $\{\hat{Y}_{t+1}^{(q)}\}_{q \in Q_{t+1}}$. Another physical measurement arrives at $k$ at $t+1$, and the same process above repeats. If there is no overlap between the neighborhoods of $j$ and $k$, then the inference procedure at each location is independent and posterior neighborhoods are chosen uniformly randomly. Suppose a candidate neighborhood at $k$ exists and overlaps with the posterior neighbourhood at $j$, and suppose only  one qubit exists at $q$ inside the overlapping region, with $q  \neq j, k $.  With some probability and acceptable true error, there are two possibilities: the map values at $j, q, k,$ are well explained by physical measurements at $j,k$ and overlapping neighbourhoods; or that our model fails to explain values  at $q$ (mismatch at $q$ exceeds acceptable error). In the first case, the candidate neighborhood at $k$ receives a high score, and in the second case, the candidate neighbouhood at $k$ receives a low score by $g_2(\lambda_2, Q_t)$. The affect of the resampling step in the particle filter is to deny low-scoring  particles, namely, posterior neighbourhod at $k$ shrinks until $q$ is excluded. Future physical measurements at any location will induce revisions of neighborhood estimates about any location $j, q, k$. If the true field is uniform everywhere, large neighborhood sizes are likely to be highly weighted and survive re-sampling over many iterations. If the true field is fluctuating too fast  with respect to the smallest separation distances between qubits, then QSLAM is limited by this minimum spatial resolution.

Sharing information between qubits at different locations in QSLAM is a tunable procedure and this allows us to test whether or not the sharing mechanism is trivial in simulations. This sharing mechanism is mediated by the two hyper-parameters of the QSLAM model, $\lambda_0, \lambda_1 \in [0, 1]$. A value of $\lambda_0, \lambda_1 \equiv 0$  means that QSLAM only performs physical measurement filtering locally, and no shared information via $\{\hat{Y}_{t+1}^{(\cdot)} \}$ is incorporated into the inference procedure. This equates to randomly sampling measurements on the qubit hardware and using Born's rule to invert the empirical probability obtained by the histogram of physical measurements. For zero valued $\lambda_0, \lambda_1$, QSLAM effectively reduces to the brute force technique, except that instead of measuring every qubit, QSLAM is uniformly randomly sampling qubits.  In contrast, a value of $\lambda_0, \lambda_1 \equiv 1$ means that qlam gives equal importance to $\{\hat{Y}_{t+1}^{(\cdot)} \}$ and  physical measurement $Y_{t+1}^{(\cdot)} $ for all $t$ i.e. $\hat{Y}_{t+1}^{(\cdot)}$ is treated as if it was the outcome of a real physical measurement. For intermediary values of $\lambda_0, \lambda_1$, QSLAM is an unbiased estimator in the large $t$ limit. In simulated experiments, one can optimise the value of  $\lambda_0, \lambda_1$, where an optimal non-zero value for $\lambda_0, \lambda_1$ suggests that sharing is non-trivial in QSLAM compared to brute force approaches.

The full QSLAM algorithm will be detailed in a forthcoming paper, and as such, we only provide details sufficient to qualitatively illustrate the operating features and structure. In the pseudocode of \cref{algorithm:q-slam-pf}, $X_t$ is the extended state vector and the initialisation procedure ensures all particles are sampled from the prior for $X_0$ and are equally weighted. For $t>0$, we note that the update rules for $F_t$ and $R_t$ occur at different times for each $t$, corresponding to an iterative maximum likelihood approach over the state of all maps and length-scales, $X_t$.
\begin{algorithm}[H] % Floats not comptatible with revtek4-1. Need "[H] option. 
	\caption{QSLAM}\label{algorithm:q-slam-pf}
	\begin{algorithmic}[0] 
		\Procedure{QSLAM}{$ \mathrm{Qubit Arrangement}, \lambda_0, \lambda_1$}\Comment{ $d$ qubits}
		\\
		\If{$ t = 0$}
		\Procedure{Initialize}{$X_0$} 
		\For{$ \alpha \in \{1, 2, \hdots, n_\alpha \}$}
		\State Initially sample $x_{0}^{(\alpha)} \sim \pi_0$ 
		\State Initially compute $W_0^{(\alpha)} = \frac{1}{n_\alpha}$
		\EndFor
		\EndProcedure 
		\EndIf
		\\
		\While{$ 1 \leq t < T$}
		\If{Controller}
		\State $j_t, Y_t^{(j_t)} \gets$ \Call{Controller}{$X_{t-1}$}
		\EndIf
		\For{$ \alpha \in \{1, 2, \hdots, n_\alpha \}$}
		\State Update $F_t^{(\cdot), (\alpha)}$ via $ \{Y_t^{(j_t)},  \{\hat{Y}_t^{(q_t)}\}, \lambda_1\} $
		\State $\{x_{t}^{(\alpha)}\} \gets $ \Call{PropagateStates}{$\{x_{t-1}^{(\alpha)} \}$ }
		\State $\{\{x_t, W_t\}^{(\alpha, \beta_\alpha)}\} \gets $ \Call{ComputeWeights}{$\{x_{t}^{(\alpha)}\}$}
		\State $\{x_t^{(\alpha, \beta_\alpha)}, \frac{1}{n_\alpha n_\beta}\} \gets $ \Call{Resample}{$\{\{x_t, W_t\}^{(\alpha, \beta_\alpha)}\}$}
		\State Update $R_t^{(j_t), (\alpha)}$ via $\{F_t^{(\cdot), (\alpha)}, \lambda_0\} $
		\State $\{\{x_t, W_t\}^{(\alpha)}\} \gets$ \Call{Collapse$\beta$}{$\{x_t^{(\alpha, \beta_\alpha)}, \frac{1}{n_\alpha n_\beta}\}$}
		\State $\{x_t^{(\alpha)}, \frac{1}{n_\alpha}\} \gets $ \Call{Resample}{$\{\{x_t, W_t\}^{(\alpha)}\}$}
		\EndFor
		\State $\{\hat{Y}_{t+1}^{(q)}\}_{q\in Q_{t+1}} \gets $ \Call{Generate$\hat{Y}$}{Posterior $X_t$}
		\EndWhile \label{pseudoalgo:qslamr:endwhile2}			
		\EndProcedure
		\\ \dotfill
		\Function{ComputeWeights}{$\{x_{t}^{\alpha}\}$}
		\For{$ \alpha \in \{1, 2, \hdots, n_\alpha \}$}
		\State Compute $\tilde{W}_t^{( \alpha )} = g_1(\lambda_1, Y_t^{(j)}) $ 
		\State $\{x_t^{(\alpha, \beta_\alpha)}\} \gets $  Generate $\beta$-layer by sampling $R_0$ 
		\For{$\beta_\alpha \in \{1, 2, \hdots, n_\beta \} $}
		\State Compute $\tilde{W}_t^{( \beta_\alpha | \alpha)} = g_2(\lambda_2, Q_t) $
		\EndFor
		\State Normalise $\tilde{W}_t^{( \beta_\alpha | \alpha)}$
		\EndFor
		\State Normalise $\tilde{W}_t^{( \alpha)}$
		\State Compute $W_t^{(\alpha, \beta_\alpha)} =  \tilde{W}_t^{( \beta_\alpha | \alpha)} \tilde{W}_t^{( \alpha)} \quad \forall \{\alpha, \{\beta_\alpha\} \}$
		\State Return $ n_\alpha n_\beta $ particles and weights $\{\{x_t, W_t\}^{(\alpha, \beta_\alpha)}\}$		
		\EndFunction 
		\\ \dotfill
		\Function{Resample}{$\{\{x_t, W_t\}^{(i)}\}$}
		\For{$i$}
		\State Sample $X$  according to $\{W_t^{(i)}\}$
		\State $x_t^{(i)} \gets X$
		\EndFor
		\State Reset $\{W_t^{(i)}\}$ to a uniform distribution
		\EndFunction 
	\end{algorithmic}	
\end{algorithm}
The function \texttt{PropagateStates} represents the transition probability distribution for Markov $X_t$ i.e. it represents identity dynamics and is a placeholder for future extensions to model dynamical $F_t$. Weights are computed according to noise density functions  $g_1(\lambda_1, Y_t^{(j)})$ and $g_2(\lambda_2, Q_t)$, as described earlier, and each re-sampling step is a so-called multinomial branching random process. We marginalise out the $\beta$ distribution of particles in the function \texttt{Collapse$\beta$}. This marginalisation step  means that the total number of particles is then constant at the begining and end of each time-step $t$, ensuring that the entire branching process in QSLAM remains a multinomial branching process. In a forthcoming paper, the branching properties of QSLAM are used to link to existing literature on general convergence properties for particle filters (cf. \cite{bain2009}).  The function \texttt{Generate$\hat{Y}$} is information generated by the measurement at $j$ that is shared in the posterior neighborhood about $j$. The full details of these mechanisms require a technical introduction that is beyond the scope of this manuscript and reserved for a forthcoming paper. 

In next section, we describe numerical simulations that test the performance of QSLAM with respect to the brute force approach. 


