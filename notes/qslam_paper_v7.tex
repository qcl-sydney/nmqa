\documentclass[reprint,longbibliography]{revtex4-1} %reprint
%############################################################################################################################
% PREAMBLE  ############################################################################################################################
\usepackage{amssymb,amsmath}
\usepackage{graphicx,import}
\usepackage{placeins}
\usepackage{hyperref}
\usepackage{color}
\usepackage{longtable}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algpseudocode} 
\usepackage[nameinlink,poorman]{cleveref}

\setlength{\parskip}{20em} 

% cleverref options
\crefname{equation}{Eq.}{Eqs.}
\crefname{figure}{Fig.}{Figs.}
\crefname{table}{Table}{Tables} 
\crefname{section}{Section}{Sections}
\crefname{chapter}{Chapter}{Chapters}
\crefname{appendix}{Appendix}{Appendices}
\crefname{algorithm}{Algorithm}{Algorithms}

\graphicspath{{../paperfigs/}} 

% ############################################################################################################################
% FRONT MATTER  
% ############################################################################################################################

\begin{document}


\title{Adaptive scheduling of noise characterization in quantum computers}

\author{Riddhi Swaroop Gupta} 
\email{riddhi.sw@gmail.com}
\affiliation{ARC Centre of Excellence for Engineered Quantum Systems, School of Physics, The University of Sydney, New South Wales 2006, Australia}

\author{Alistair R. Milne} 
\affiliation{ARC Centre of Excellence for Engineered Quantum Systems, School of Physics, The University of Sydney, New South Wales 2006, Australia}

\author{Claire L. Edmunds} 
\affiliation{ARC Centre of Excellence for Engineered Quantum Systems, School of Physics, The University of Sydney, New South Wales 2006, Australia}

\author{Cornelius Hempel} 
\affiliation{ARC Centre of Excellence for Engineered Quantum Systems, School of Physics, The University of Sydney, New South Wales 2006, Australia}

\author{Michael J. Biercuk}
\affiliation{ARC Centre of Excellence for Engineered Quantum Systems, School of Physics, The University of Sydney, New South Wales 2006, Australia}

\begin{abstract}
Common spatial correlations between processing and spectator qubits is information which may be exploited for developing improved control techniques for large scale quantum computers. One challenge arises when attempting learn noise correlations using single-qubit measurements as each measurement is a costly resource. This cost is incurred as reduced availability of processing qubits on hardware and in ensuring the reliability of a noise-sensing measurement. We present an algorithmic framework, QSLAM, for reconstructing a map of unknown spatial correlations in quantum computing architectures using imperfect, sparse measurements. We implement QSLAM via a novel two-layer particle filter that shares information between neighbouring qubits while discovering neighbourhood sizes relevant to the system. An adaptive controller autonomously schedules future measurements to address regions where map estimation is weakest. QSLAM outperforms a brute force approach for estimating spatially varying magnetic fields by $3$x and upto $18$x in simulations using measurements (1D) and engineered data (2D) respectively.
\end{abstract}



\maketitle

% ############################################################################################################################
% MAIN TEXT
% ############################################################################################################################

\section{Introduction}

Decoherence remains a key challenge in the development of large scale quantum computing architectures. Contemporary physical hardware for quantum computing typically involves multi-qubit arrays embedded in chip- or wafer-like architectures \cite{yao2012scalable,monroe2014large,veldhorst2017silicon,jones2012layered,kielpinski2002architecture,franke2019rent}. Characterisation and control of qubits in hardware is impeded by spatially dependent decoherence mechanisms, cross-talk, and unknown, classical noise fields,  \cite{postler2018experimental} manifesting as unwanted correlations between single-qubit operations. Understanding spatial noise correlations through single-qubit measurement data is often prohibitively costly, as each measurement reduces time-availability of computational qubits on hardware and measurement fidelity is often limited by noise. As a first step in developing efficient noise suppressing protocols for multi-qubit physical settings, pioneering techniques designed for large-scale quantum computers now involve in-situ detection methods, where integrated measurement scheduling and compilation protocols are critical components to optimising overall hardware performance without human supervision \cite{venturelli2018compiling,murali2019noise,shi2019optimized,venturelli2018optimization,tannu2018case}.


Inspired by classical simultaneous mapping and localisation (SLAM) protocols \cite{cadena2016past,bergman1999recursive,stachniss2014particle,durrant2006simultaneous,bailey2006simultaneous,murphy2000bayesian,howard2006multi,thrun2005probabilistic,thrun1998probabilistic}in autonomous navigation and robotic exploration, we bring new insights for efficiently mapping spatial noise in unknown physical settings. In SLAM, the total error in spatially characterising an unknown environment is significantly reduced by combining two separate inference problems - mapping and localisation - into one. The first problem concerns building a map of an unknown terrain using incoming sensor data, such as a LIDAR scan implemented by a robot. The second problem is to place a robot on the map, and for the robot to choose where to go and measure next. The total error is reduced if features on the estimated map are correctly re-observed by the robot from many different locations. While SLAM can be implemented using a wide range of algorithms (e.g. extended Kalman filters, graphical models), a widely used inference technique called particle filtering is used to implement SLAM when the underlying sensor measurement models and/or robot dynamics are unavoidably non-linear (cf. standard \cite{doucet2001introduction}, or SLAM implementations \cite{beevers2007fixed,grisettiyz2005improving,poterjoy2016localized}).

In this letter, we introduce a new framework for autonomous learning, denoted Quantum SLAM (QSLAM), to reconstruct a map of unknown spatial fields in a large scale quantum computer. The QSLAM algorithm discovers spatial `neighborhoods' around spectator qubits in which information may be shared, representing the presence of spatial correlations in the underlying noise source. We implement the QSLAM framework via a novel two-layer particle filter, followed by an autonomous real-time controller. The algorithm iteratively builds a map of the noise field  in real time by maximising the information utility obtained from each physical measurement, enabling the algorithm to adaptively build the map and determine the most useful subsequent measurement to perform in the following step. In numeric simulations, a Naive approach would be to measure this field in a spatially uniform way, as in panel (a) of \cref{fig_paper_overview}. This is contrasted with the QSLAM framework in \cref{fig_paper_overview}(b), which reconstructs a map by sharing information between qubits within the same neighborhood and adaptively determining the location of the next most relevant measurement to perform. Our results show QSLAM outperforms brute-force measurement strategies by a reduction of up to $18$x in the number of measurements required to estimate a noise map with a target fidelity.  These results hold for both 1D and 2D qubit arrays subject to noise fields with varying spatial configurations.  Finally, we demonstrate the QSLAM algorithm using real experimental measurements on an array of trapped ions and achieve a $3$x gain over brute force measurement in offline simulations.

\section{The QSLAM Framework}

We consider a spatial arrangement of $d$ qubits as determined by a particular choice of hardware. An unknown, classical field exhibiting spatial correlations extends over all qubits on our device, and corresponds to stray noise fields in realistic operating architectures.  Our objective is to build a map of the underlying spatial variation of the noise field with the fewest possible single-qubit measurements. We conceive that each measurement is  a single-shot Ramsey-like experiment in which the unknown field manifests as a precession of the qubit's state with a measurable phase at the end of a fixed interrogation period. This phase is not observed directly, but inferred from data, as it parameterises the Born probability of seeing a 0 or 1 outcome of a projective measurement on the qubit. Our algorithms take as input data these discrete binary `clicks'. The desired output of our algorithm at any given iteration, is a map of the noise field, denoted as a set of unknown qubit phases $F_t$ at time $t$, inferred from the binary measurement record.  

\begin{figure}
	\includegraphics[scale=1]{paper_overview}
	\caption{\label{fig_paper_overview} Schematic depicting differences between naive brute force and QSLAM strategy to reconstruct true field via single qubit measurements. Arbitrary spatial arrangement of qubits (red open circles); a true unknown field with finite correlations (colored regions). (a) naive strategy uniformly measures field everywhere (red dots). (b) QSLAM strategy iteratively chooses which qubit to measure next (black arrows); shares state estimation information in local neighborhoods (white shaded circles).}  	
\end{figure} 

QSLAM is a non-linear filtering problem inspired by, but radically distinct from, classical SLAM techniques. The framework incorporates an information sharing mechanism that is constructed using the quantum mechanical Born rule. The Born rule is theoretical function linking incoming single qubit measurement data to estimates of the true unknown map. By exploiting access to Born's rule, the QSLAM algorithm reconstructs a map by not only filtering measurement noise from local measurements, but by sharing information between qubits in small spatial regions called `neighborhoods'.  In contrast, classical map-building in SLAM does not have access to a theoretical analogue to the Born's rule: features of the environment are inferred from raw data, and each feature is labeled as new- or a re- observation based on statistical inference. By exploiting access to Born's rule, QSLAM departs radically from classical SLAM problems. 

We approximate the true theoretical non-linear filtering problem embodied by QSLAM via particle filtering techniques. Particle filtering techniques are used to propagate, update, and filter classical probability distributions over true state variables where these distributions are expected to undergo highly non-linear transformations. By choosing particle filters, we can accommodate  projective single-qubit measurements as binary, discrete outputs of a highly non-linear measurement model.  As part of the particle filtering technique, the posterior distribution over the state variable, $X_t$, contains information about the true map and it is approximately represented as a discrete collection of weighted `particles' at each iteration $t$. Each particle has two properties - a position and a weight - where the position of the particle carries information about $X_t$ and the weight specifies the likelihood or the importance of the particle.  After receiving measurement data at each time-step, the particles are `re-sampled' i.e. the original set of particles at $t$ are replaced by an off-spring set of particles, where the probability that a parent is chosen to represent itself again (with replacement) is directly proportional to its weight. Over many iterations, only the highest weighted particles survive and these surviving particles form, in our case, the maximum likelihood estimate of the true $X_t$.

\begin{figure*}
	\includegraphics[scale=0.58]{paper_particle_filter}
	\caption{\label{fig_intro_highlevel_v2} Schematic overview of QSLAM for an incoming qubit measurement, $Y^{(j)}_t$ at qubit $j$. A particle filter estimates the map, $F_t$, and discovers neighborhoods [circular shaded] parameterised by $R_t^{(j)}$ for sharing state information about $j$. Posterior state estimates from the particle filter are used (a) by the controller, to choose $k$ as the location of the next measurement, $Y^{(k)}_{t+1}$ based on regions of highest estimated uncertainty [middle top]; (b) to share information at $j$ within the posterior neighborhood, $Q$ via $\hat{Y}^{(q)}_{t+1}$, for  all $q \in Q$ [middle bottom] before commencing the next iteration.}  	
\end{figure*} 

The key operating principle of QSLAM is that we locally estimate the map value, $F_t^{(j)}$, before globally sharing the map information at qubit $j$ with the neighboring qubits in the vicinity of $j$. The algorithm is responsible for determining the size of the neighborhood at $j$ and examples of neighborhoods are depicted as shaded circular regions in the leftmost panel of \cref{fig_intro_highlevel_v2}. The radial size of each neighborhood, and the spreading of information within this neighborhood, are both parameterised by a so-called length-scale value, $R_t^{(j)}$.   The collection of map values and length-scales at every qubit, $X_t:=\{F_t^{(j)}, R_t^{(j)}\}$ is depicted as the extended state vector in the leftmost panel \cref{fig_intro_highlevel_v2}.  $F_t$ represents the qubit phase and takes the values between $[0, \pi]$ radians. We are ignorant, apriori, of any knowledge of $R_t$. Estimates of $R_t$ take values between $R_{min}$, defined approximately as the inter-qubit spacing, and $R_{max}$, defined using the size of the qubit array, in units of distance. The set of qubits that fall inside a neighborhood are denoted by $Q_t$. % We consider both 1D and 2D spatial arrangements where qubits are subject to a time-invariant true field. 

Obtaining a posterior state over the space of all maps and length scales, $X_t$, is formidably difficult problem as the state-space is very large. Indeed, this is a general roadblock in producing analytic solutions for classical SLAM instances \cite{thrun2005probabilistic,thrun2001probabilistic}. We therefore rely on an additional approximation - an iterative maximum likelihood procedure within each time step of the particle filter - to solve the QSLAM inference problem. In each time-step, the iterative maximum likelihood procedure means that, first, we update $F_t$ assuming $X_{t-1}$ is known, and second, we update $R_t$ assuming $F_t$ is known. The structure of this iterative likelihood maximization approach means that the QSLAM particle filter is atypical, and the filter has two different types of particles, labeled $\alpha$ and $\beta$ particles respectively.  The $\alpha$ particles carry information about the full state vector, $X_t$. The $\beta$ particles discover optimal neighborhood size, $R_t^{(j)}$, for sharing information at any qubit location, $j$.  The two different types of particle sets are then used to manipulate the joint probability distribution defined over $F_t, R_t$ such that we numerically implement an iterative maximum likelihood procedure. The end result of the particle filtering step in \cref{fig_intro_highlevel_v2} is to obtain a posterior $X_t$ that represents our best knowledge about $X_t$ given measurement data.

Our best knowledge of $X_t$ is passed onto an autonomous measurement scheduler, the QSLAM controller, which maximises the information utility from each measurement. The QSLAM controller selects the location of the next physical measurement by choosing the region where posterior state variance of $X_t$ is maximally uncertain (\cref{fig_intro_highlevel_v2}, top-middle panel). The autonomous selection next measurement location is schematically shown as the location $k$ with the measurement outcome denoted $Y_{t+1}^{(k)}$. Meanwhile, the posterior state information at $t$ is shared with other qubits in the vicinity of the qubit at $j$ (\cref{fig_intro_highlevel_v2}, bottom-middle panel).  The shared information is denoted by the set $\{\hat{Y}_{t+1}^{(q)} \}$ and $q$ labels the neighboring qubits of $j$ in the posterior neighborhood, $Q_{t+1}$. The information in the set $\{\hat{Y}_{t+1}^{(q)} \}$  is an effective state estimate that is taken as an input to the algorithm in a manner similar to a physical measurement. The new physical measurement, $Y_{t+1}^{(k)}$,  and the set of shared information between qubits in a posterior neighborhood, $\{\hat{Y}_{t+1}^{(q)}, q \in Q_{t+1}\}$, form the inputs for the next time-step, $t+1$. 

Under these conditions, the QSLAM filtering problem requires only two specifications: a prior or initial distribution for the state vector, $X_0$, at $t=0$, and a likelihood function that assigns each particle with an appropriate weight based on measurement data. Assuming a uniform prior, we need only to define the global likelihood function incorporating both particle-types. We will label $\alpha$-particles by the set of numbers $\{1, 2, \hdots, n_\alpha\}$. For each $\alpha$ particle, we also associate a set of $\beta$ particles labeled $\beta^{(\alpha)}$, with label values from $\{1, 2, \hdots, n_\beta \}$.  Each  $\alpha$ particle is weighted by $ g_1(\lambda_1, Y_t^{(j_t)})$, where $\lambda_1$ is a parameter of the QSLAM model. A single $\beta^{(\alpha)}$-particle inherits the state from its $\alpha$-parent; but additionally, acquires a single uniformly distributed sample for $R_t^{(j)}$ from the length-scale prior distribution. As such, the $\beta$-layer is uniquely employed to discover local neighborhood length-scales $R_t^{(j)}$. The $\beta$-particles are scored by a separate likelihood function, $g_2(\lambda_2, Q_t)$, where $\lambda_2$ is another parameter of the QSLAM model. Then the total likelihood for an $(\alpha, \beta^{(\alpha)})$ pair is given by the product of the $\alpha$ and $\beta$ particle weights.

The functions $g_1(\lambda_1, Y_t^{(j)})$ and $g_2(\lambda_2, Q_t)$ are likelihood functions used to score particles inside the QSLAM particle filter. These functions are  derived by representing the noise affecting the physical system via probability density functions. The function $g_1(\lambda_1, Y_t^{(j)})$ describes measurement noise on a local binary qubit measurement. The uncertainty introduced by a projective quantum measurement can be described as a truncated or a quantised Gaussian error model and specifies the form of the noise density function. Meanwhile, the function $g_2(\lambda_2, Q_t)$ is based on noise density of unknown true errors arising from approximating a continuously varying unknown spatial field with a series of overlapping neighborhoods, where the point value of the field at the center is blurred over the neighborhood using a Gaussian kernel. We assume that this true error density has a non-zero mean, variance and it is Gaussian distributed.  

Two free parameters, $\lambda_1, \lambda_2 \in [0,1]$, are used to numerically tune the performance of the QSLAM particle filter. Physically, the free parameter, $\lambda_1$ mediates how shared information is aggregated when estimating the value of the map locally; $\lambda_2$ mediates how neighborhoods expand or contract in size. The numerically optimised values of $\lambda_1, \lambda_2 \in [0,1]$ provide an important test of whether or not the QSLAM sharing mechanism is trivial in a given application. A numerically optimised non-zero value for $\lambda_1, \lambda_2$ suggests that sharing mechanism in QSLAM is non-trivial i.e. true spatial correlations exist such that sharing information spatially improves performance more than just locally filtering observations for measurement noise. In contrast, value of $\lambda_1, \lambda_2 \equiv 0$  means that QSLAM only performs measurement noise filtering locally, and no shared information between qubits is incorporated into the inference procedure. This equates to randomly sampling measurements on the qubit hardware i.e. QSLAM effectively reduces to the brute force technique. A maximal value of $\lambda_1, \lambda_2 \equiv 1$ means that QSLAM gives equal importance to shared information and the physical measurement for all $t$. This means that the shared data between qubits, the set $\{\hat{Y}_{t+1}^{(q)} \}$, is treated as if they were the outcomes of real physical measurements. For intermediary values of $\lambda_1, \lambda_2$, QSLAM is an unbiased estimator in the large $t$ limit. 

\section{Results \label{sec:results}}

Application of the QSLAM algorithm using both numerical simulations and real experimental data demonstrate the capabilities of this routine, for arbitrary spatial arrangements of $d$ qubits in 1D or 2D. 

In order to evaluate the performance of QSLAM, we adopt a common metric for assessing the quality of the constructed map. This error metric is a Structural SIMilarity Index (SSIM) frequently used to compare images in machine learning analysis. The SSIM metric compares the structural similarity between two images. It is found to be mathematically sensitive to improvements in the quality of images in the presence of non-standard error sources including large single-pixel errors that frequently plague the mean-square error or other norm-based metrics for vectorised images \cite{wang2004image,chen2009similarity,wang2009mean}  We compare the true map and its algorithmic re-construction; a score of zero corresponds to ideal performance, and implies that the true map and its reconstruction are identical. 

Each iteration $t$ corresponds to one physical measurement and total number of physical measurements given to any algorithm is given by $T$, so that $t = 1, 2, \hdots T$. For simplicity, we pick values of $T\geq d$ as multiples of $d$ so that every qubit is measured $1, 2, 3...$ times in the Naive approach.  For $T<d$,  the Naive approach is given $T$ uniformly randomly sampled locations to measure. In QSLAM, the above considerations do not apply as the controller chooses a measurement location in each iteration. Both QSLAM and Naive approaches terminate when the total number of measurements is reached. A single-run SSIM score is then calculated using the estimated map for each algorithm, and the average value of the score over 50 trials is reported in figures.

We start with a challenging example, in which the 25 qubits are arranged in a 2D grid. The true engineered field is partitioned into square regions with low and high values (\cref{SSIM1_collated}(a), left inset). We plot the average SSIM score against the total number of measurements, $T$, provided to each algorithm (QSLAM, Naive in \cref{SSIM1_collated}(a), upper right inset). For any choice of $T$, QSLAM reconstructs the true field with a lower error score than the Naive approach. 

We input this data into a numerical inverse function that takes as input average SSIM scores, and reports the ratio of measurements of Naive to QSLAM (\cref{SSIM1_collated}(a), main panel). This ratio enables one to read off the reduction in total measurements required to achieve the same error using QSLAM rather than a brute force measurement strategy. The shape of the figure in \cref{SSIM1_collated}(a) main panel is linked to the total amount for information fed to both algorithms. The high data regime exists near the origin and low data regime extends right-wards along the $x$-axis. At high SSIM scores on the far-right of the figure, both Naive and QSLAM algorithms receive very few measurements and map-reconstructions are dominated by errors. Near the origin, an extremely large number of measurements are required to achieve low SSIM scores and the ratio of measurements between Naive and QSLAM tends to unity. The numerical inversion  of raw data in the inset to the main panel of \cref{SSIM1_collated}(a) introduces artifacts and it is particularly unstable for smaller scores corresponding to increasingly large $T$. 

The key region of interest is the intermediary data regime, where QSLAM outperforms brute force measurement by several orders of magnitude in achieving moderate levels of map reconstruction fidelity, resulting in a broad peak. The broad peak indicates that for a large range of subjectively acceptable SSIM scores, e.g. $< 0.3$, the Naive approach uses between $2$x to $18$x times the number of measurements required by QSLAM to reconstruct $F_t$.  In the Supplementary Materials, we show similar performance for different qubit array geometries and fields with different spatial characteristics. These supporting results using simulated data exhibit the broad peak structure characteristic of QSLAM outperforming Naive in an intermediate data regime.

All of our results rely on the appropriate tuning of the QSLAM particle filter. The full numerical optimisation of  ($\lambda_1, \lambda_2$) involves tuning these parameters for each $T$, and this data is represented as solid crimson curves (\cref{SSIM1_collated}(a) upper right inset and main panel).  Since it may not be practical to tune QSLAM for every choice of measurement budget, the dashed crimson line in (a) corresponds to fixing the choice of $\lambda_1, \lambda_2$ to the optimal value for $T=20$, and using this fixed choice for all $T \neq 20$. The path in (b) shows how  a total of ten measurements were selected by QSLAM during a single run.  QSLAM reconstruction in (c) after 75 measurements captures the spatial structure of the true field. In contrast, Naive maps in (d) and (e) are dominated by extreme values ($0$ or $\pi$) characteristic of simple reconstructions using sparse measurements. 
\begin{figure}
	\includegraphics[scale=1]{SSIM1_collated}
	\caption{\label{SSIM1_collated} 2D array of 25 qubits in $5 \times 5$  `square' field in left inset of (a).  Right inset plots SSIM score against total measurement budget for QSLAM [crimson circles] and Naive [black crosses] over 50 trials. Ratio of Naive to QSLAM measurements shows x-fold reduction for a range of SSIM scores for optimised $\lambda_1, \lambda_2$ [solid crimson] and fixed choice $\lambda_1 = 0.95, \lambda_2 = 0.65$ [dashed crimson]. (b)-(e) Columns show single run maps using $T=10, 75$ measurements plotted for QSLAM [top] and Naive [bottom]. $(\lambda_1, \lambda_2)= (0.92, 0.44)$ and $ (0.93, 0.77)$ for (b),(c) respectively. Single map SSIM for QSLAM (b) 0.64, with control path [black solid]; (c) 0.13; for Naive (d) 0.80;  (e) 0.47. }    	
\end{figure} 

In \cref{SSIM_expt_0_collated_normal}, we conduct an offline simulation of both algorithms using real experimental data. Data is collected using a system of $^{171}\text{Yb}^{+}$ ions confined in a linear Paul trap (similar to \cite{Guggemos:2017}) with center-of-mass (COM) trap frequencies \mbox{$\omega_{x,y,z}/2\pi \approx (1.6,1.5,0.5)~\text{MHz}$}. Qubits are encoded in the $^2\mathrm{S}_{1/2}$ ground-state manifold where we associate the hyperfine states $|F=0, m_F =0\rangle$ and $|F=1, m_F =0\rangle$, split by 12.6~GHz, with the $|0\rangle$ and $|1\rangle$ state, respectively. State initialization to $|0\rangle$ via optical pumping and state detection are performed using a laser resonant with the $^2\mathrm{S}_{1/2} - {^2\mathrm{P}_{1/2}}$ transition near 369.5~nm. A total of $25500$ Ramsey measurements are performed on all six ions with a wait time of $40$ ms. Measurement is performed using a spatially-resolving EMCCD camera to detect the fluorescence of individual ions. An example of all ions in the bright state captured by a camera image is shown in \cref{SSIM_expt_0_collated_normal}(a) using a resolution of $0.861 \mu$m per pixel and exposure of $50 \mu$s. A classification algorithm assigns a `0' (dark) or `1' (bright) to each ion in the camera picture. The full set of $6 \times 25500$ measurements constitute a data-bank for both algorithms, QSLAM and Naive. 

The data-bank of real measurements is used for an offline simulation for both algorithms. At each iteration, an algorithm randomly draws a measurement outcome from the data-bank at the desired single-ion location.  The rest of the analysis proceeds as before with the following changes. In absence of a true engineered field, the SSIM scores are calculated with respect to the measured field, i.e. the expected value of $F_t$ under $25500$ measurements. This measured field is shown as the right inset of \cref{SSIM_expt_0_collated_normal}(b), where each pixel represents the field value measured at one of the six approximately equidistant qubits. This means that the Naive approach must smoothly go to zero error as the measurement budget increases (\cref{SSIM_expt_0_collated_normal}(b), left inset), and hence the comparison of algorithmic performance in  \cref{SSIM_expt_0_collated_normal} is  different in principle to the comparisons made in \cref{SSIM1_collated}.
\begin{figure}
	\includegraphics[scale=1]{SSIM_expt_0_collated_normal}
	\caption{\label{SSIM_expt_0_collated_normal} (a) Image of six trapped 171Yb+ ions in bright (F=1) state subject to a stray magnetic field gradient in right inset of (b) measured using 25,000 Ramsey measurements, with wait time $40$ms, resolution $0.861 \mu$m per pixel and exposure $50 \mu$s. Images subject to bright state decay with mean lifetime order $\sim 10\mu$s. (b) right inset depicts SSIM error as a function of total measurement budget for QSLAM (crimson circles) and Naive (black crosses) inverted to give crimson solid in main figure; dashed crimson optimal for fixed  $\lambda_1, \lambda_2) = (0.95, 0.97) $ at $T=20$. (c)-(f) Columns for single run maps using $T=10, 75$ plotted for QSLAM [top] and Naive [bottom]. $(\lambda_1, \lambda_2)= (0.99, 0.92)$ for both (c),(d). Single map SSIM for QSLAM (c) 0.46, with control path [black solid]; (d) 0.05; for Naive (e) 0.90; (f) 0.34.}    	
\end{figure}

The main figure of \cref{SSIM_expt_0_collated_normal}(b) shows QSLAM performance for numerically optimised $\lambda_1, \lambda_2 \gg 0$ (solid crimson). A $2$x-$3$x reduction in measurements are achieved with respect to using the Naive approach. Near zero SSIM scores, the optimal curves saturate with ratio of QSLAM and Naive going to unity. For all other cases, the curves are strictly above unity and correspond to approximately $2$x - $3$x measurement reductions in using QSLAM over Naive for a range of low SSIM scores. 

Based on our results thus far, the key numerical evidence for the correctness of QSLAM is that the numerically optimized values of $\lambda_1, \lambda_2 \gg 0 $ for all results reported in the main text and the Supplementary Materials. A non-zero optimal value for both  $\lambda_1, \lambda_2$  confirms that non-trivial information sharing occurs and QSLAM achieves low SSIM scores than Naive for any fixed data budget in \cref{SSIM1_collated,SSIM_expt_0_collated_normal}. Since QSLAM is designed to be an unbiased estimator, it is not surprising that the ratio of measurements for Naive to QSLAM  should go unity in the large data limit, if the $\lambda_1, \lambda_2$ parameters are correctly tuned. However, we additionally see that the ratio is lower-bounded by unity in \cref{SSIM1_collated,SSIM_expt_0_collated_normal} as we move from intermediate to high data regimes. This provides numerical evidence that QSLAM learns the true field both correctly and with fewer measurements than Naive, for both engineered data and in experiment. 

\FloatBarrier
\section{Methods}

We summarise the pseudocode of the QSLAM algorithm in \cref{algorithm:q-slam-pf}. The first part of the algorithm consists of an initialisation procedure which ensures all particles are sampled from the prior for extended state vector at $t=0$, giving $X_0$. All particles are equally weighted before measurement data is received. 

\begin{algorithm}[H] % Floats not comptatible with revtek4-1. Need "[H] option. 
	\caption{QSLAM}\label{algorithm:q-slam-pf}
	\begin{algorithmic}[0] 
		\Procedure{QSLAM}{$d$ qubit locations, $\lambda_1, \lambda_2$}
		\\
		\If{$ t = 0$}
		\Procedure{Initialize}{$X_0$} 
		\For{$ \alpha \in \{1, 2, \hdots, n_\alpha \}$}
		\State Initially sample $x_{0}^{(\alpha)} \sim \pi_0$ 
		\State Initially compute $W_0^{(\alpha)} = \frac{1}{n_\alpha}$
		\EndFor
		\EndProcedure 
		\EndIf
		\\
		\While{$ 1 \leq t < T$}
		\If{Controller}
		\State $j_t, Y_t^{(j_t)} \gets$ \Call{Controller}{$X_{t-1}$}
		\EndIf
		\For{$ \alpha \in \{1, 2, \hdots, n_\alpha \}$}
		\State $\{x_{t}^{(\alpha)}\} \gets $ \Call{PropagateStates}{$\{x_{t-1}^{(\alpha)} \}$ }
		\State Update $F_t^{(\cdot), (\alpha)}$ via $ \{Y_t^{(j_t)},  \{\hat{Y}_t^{(q_t)}\}, \lambda_1\} $
		\State $\{\{x_t, W_t\}^{(\alpha, \beta_\alpha)}\} \gets $ \Call{ComputeWeights}{$\{x_{t}^{(\alpha)}\}$}
		\State $\{x_t^{(\alpha, \beta_\alpha)}, \frac{1}{n_\alpha n_\beta}\} \gets $ \Call{Resample}{$\{\{x_t, W_t\}^{(\alpha, \beta_\alpha)}\}$}
		\State Update $R_t^{(j_t), (\alpha)}$
		\State $\{\{x_t, W_t\}^{(\alpha)}\} \gets$ \Call{Collapse$\beta$}{$\{x_t^{(\alpha, \beta_\alpha)}, \frac{1}{n_\alpha n_\beta}\}$}
		\State $\{x_t^{(\alpha)}, \frac{1}{n_\alpha}\} \gets $ \Call{Resample}{$\{\{x_t, W_t\}^{(\alpha)}\}$}
		\EndFor
		\State $\{\hat{Y}_{t+1}^{(q)}\}_{q\in Q_{t+1}} \gets $ \Call{Generate$\hat{Y}$}{Posterior $X_t$}
		\EndWhile \label{pseudoalgo:qslamr:endwhile2}			
		\EndProcedure
		\\ \dotfill
		\Function{ComputeWeights}{$\{x_{t}^{\alpha}\}$}
		\For{$ \alpha \in \{1, 2, \hdots, n_\alpha \}$}
		\State Compute $\tilde{W}_t^{( \alpha )} = g_1(\lambda_1, Y_t^{(j)}) $ 
		\State $\{x_t^{(\alpha, \beta_\alpha)}\} \gets $  Generate $\beta$-layer by sampling $R_0$ 
		\For{$\beta_\alpha \in \{1, 2, \hdots, n_\beta \} $}
		\State Compute $\tilde{W}_t^{( \beta_\alpha | \alpha)} = g_2(\lambda_2, Q_t) $
		\EndFor
		\State Normalise $\tilde{W}_t^{( \beta_\alpha | \alpha)}$
		\EndFor
		\State Normalise $\tilde{W}_t^{( \alpha)}$
		\State Compute $W_t^{(\alpha, \beta_\alpha)} =  \tilde{W}_t^{( \beta_\alpha | \alpha)} \tilde{W}_t^{( \alpha)} \quad \forall \{\alpha, \{\beta_\alpha\} \}$
		\State Return $ n_\alpha n_\beta $ particles and weights $\{\{x_t, W_t\}^{(\alpha, \beta_\alpha)}\}$		
		\EndFunction 
		\\ \dotfill
		\Function{Resample}{$\{\{x_t, W_t\}^{(i)}\}$}
		\For{$i$}
		\State Sample $X$  according to $\{W_t^{(i)}\}$
		\State $x_t^{(i)} \gets X$
		\EndFor
		\State Reset $\{W_t^{(i)}\}$ to a uniform distribution
		\EndFunction 
	\end{algorithmic}	
\end{algorithm}

For $t>0$, the function \texttt{PropagateStates} represents the transition probability distribution for Markov $X_t$ i.e. it represents identity dynamics and is a placeholder for future extensions to model dynamical $F_t$. 

In each $t$, a single physical measurement is received. This triggers a set of update rules for $F_t$ and $R_t$. We note that the state variables, $F_t$ and $R_t$, are updated in a specific order within each time-step $t$. The order of these computations correspond to the iterative maximum likelihood approximation for the QSLAM framework.

The form of the update rules reflect standard calculations associated with particle filtering techniques. For each type of particles, the weight or the importance of the particle is  computed according to noise density (likelihood) functions  $g_1(\lambda_1, Y_t^{(j)})$ and $g_2(\lambda_2, Q_t)$. Weighted particles are re-sampled according to a so-called multinomial branching random process. The atypical step in the QSLAM framework is that we marginalise out the $\beta$ distribution of particles in the function \texttt{Collapse$\beta$} while still extracting some information about the posterior $R_t$. Despite this atypical marginalisation step, the total number of particles is constant at the beginning and end of each time-step $t$ and the particle branching mechanism fo QSLAM remains a multinomial branching process. 

Once a posterior map and neighbourhood has been discovered, the function \texttt{Generate$\hat{Y}$} spreads the state information generated by the measurement at $j$ to all qubits in the posterior neighborhood about $j$. This shared state information is converted into a 0 or 1 output using Born's rule. The resulting output is the discrete binary set labeled  $\{\hat{Y}_{t+1}^{(q)} \}$. 

\section{Conclusion \label{sec:conclusion}}


We presented QSLAM - a framework for autonomous learning where we reconstruct an unknown spatial field (noise) by efficiently scheduling measurements on 2D multi-qubit devices.  QSLAM uses an iterative maximum likelihood procedure via a novel two-layer particle filter to share state estimation information  between qubits within small spatial `neighborhoods'. An autonomous controller schedules the next future measurement in order to reduce the estimated uncertainty of the map reconstruction.  

Simulations using engineered data and offline processing of experimental data confirm that QSLAM uses between $2$x-$18$x less measurements than a naive brute force measurement approach to reconstruct a map of spatial noise correlations. These performance improvements apply for a range acceptable map reconstruction fidelity scores and gains are attributed to the algorithm's information sharing mechanism. Indeed, an experimenter may use standard numerical optimisation techniques to test whether QSLAM's information sharing is non-trivial for a given physical application, by tuning parameters $\lambda_1, \lambda_2$ and observing a non-zero optimal value. For example, in scenarios where a noise field varies rapidly in space relative to the resolution afforded by the qubit grid, meaningful information sharing between qubits may not be permissible and QSLAM reduces to brute-force measurement.

Our proposed framework is flexible and future work will extend QSLAM to time-varying operating conditions, such as, incorporating noise drift and/or changes in the time-availability of spectator qubits.  In specific cases where multi-qubit architectures are not rigidly fixed during fabrication, like trapped-ions, one may achieve sub-lattice resolution by dynamically moving qubits in space. 

\section{Acknowledgments}
Authors thank Virginia Frey for proposing methods for single-ion state detection using camera images; and Andrew Doherty for useful discussions. Work supported by [add info].

\section{Data Availability}
Simulated data and experimental measurements to reproduce all figures are stored as .npz and .txt files respectively. These databases can be accessed via links in the Supplementary Information without restrictions.

\section{Code Availability}
Minimally reproducing equations for QSLAM and links to the code-base are provided in the Supplementary Information without restrictions.

\section{Author Contributions}
The QSLAM theoretical framework and numerical implementations were devised by R. Gupta based on research directions set by M.J. Biercuk. R. Gupta and M.J. Biercuk co-wrote the paper. A. Milne, C. Edmunds and C. Hempel led all experimental efforts.

\section{Competing Interests}
R. Gupta declares no competing financial interests. \\

\section{Materials and Correspondence} 
Correspondence to R. Gupta.

% ############################################################################################################################
% REFERENCES  
% ############################################################################################################################

% \clearpage
\bibliography{./references_7}  

\end{document}
